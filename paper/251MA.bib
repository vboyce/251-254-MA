@article{altmejd2019,
  title = {Predicting the Replicability of Social Science Lab Experiments},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  date = {2019-12-05},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {12},
  pages = {e0225826},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0225826},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225826},
  urldate = {2023-03-17},
  abstract = {We measure how accurately replication of experimental results can be predicted by black-box statistical models. With data from four large-scale replication projects in experimental psychology and economics, and techniques from machine learning, we train predictive models and study which variables drive predictable replication. The models predicts binary replication with a cross-validated accuracy rate of 70\% (AUC of 0.77) and estimates of relative effect sizes with a Spearman ρ of 0.38. The accuracy level is similar to market-aggregated beliefs of peer scientists [1, 2]. The predictive power is validated in a pre-registered out of sample test of the outcome of [3], where 71\% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to ρ = 0.25. Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two-variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing the process of evaluation of new findings and guiding resources to those direct replications that are likely to be most informative.},
  langid = {english},
  keywords = {Experimental economics,Experimental psychology,Forecasting,Machine learning,Machine learning algorithms,read,Replication studies,Scientists,Trees},
  file = {/home/vboyce/Zotero/storage/98PWUT3J/Altmejd et al. - 2019 - Predicting the replicability of social science lab.pdf}
}

@article{anderson2016,
  title = {Response to {{Comment}} on “{{Estimating}} the Reproducibility of Psychological Science”},
  author = {Anderson, Christopher J. and Bahník, Štěpán and Barnett-Cowan, Michael and Bosco, Frank A. and Chandler, Jesse and Chartier, Christopher R. and Cheung, Felix and Christopherson, Cody D. and Cordes, Andreas and Cremata, Edward J. and Della Penna, Nicolas and Estel, Vivien and Fedor, Anna and Fitneva, Stanka A. and Frank, Michael C. and Grange, James A. and Hartshorne, Joshua K. and Hasselman, Fred and Henninger, Felix and van der Hulst, Marije and Jonas, Kai J. and Lai, Calvin K. and Levitan, Carmel A. and Miller, Jeremy K. and Moore, Katherine S. and Meixner, Johannes M. and Munafò, Marcus R. and Neijenhuijs, Koen I. and Nilsonne, Gustav and Nosek, Brian A. and Plessow, Franziska and Prenoveau, Jason M. and Ricker, Ashley A. and Schmidt, Kathleen and Spies, Jeffrey R. and Stieger, Stefan and Strohminger, Nina and Sullivan, Gavin B. and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and Vanpaemel, Wolf and Vianello, Michelangelo and Voracek, Martin and Zuni, Kellylynn},
  options = {useprefix=true},
  date = {2016-03-04},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aad9163},
  url = {https://www.science.org/doi/10.1126/science.aad9163},
  urldate = {2023-04-04},
  abstract = {Gilbert               et al               . conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/SZGMIEPM/Anderson et al. - 2016 - Response to Comment on “Estimating the reproducibi.pdf}
}

@online{bak-coleman2022,
  title = {Replication Does Not Reliably Measure Scientific Productivity},
  author = {Bak-Coleman, Joseph and Mann, Richard P. and West, Jevin and Bergstrom, Carl T.},
  date = {2022-04-27T16:11:25},
  eprinttype = {arxiv},
  doi = {10.31235/osf.io/rkyf7},
  url = {https://osf.io/preprints/socarxiv/rkyf7/},
  urldate = {2023-03-09},
  abstract = {Replication surveys are becoming a common tool for assessing the knowledge production of scientific disciplines. In psychology, economics, and preclinical cancer biology, replication rates near 50\% have been argued as evidence the disciplines are not reliably producing knowledge, are rife with questionable research practices, and warrant reform. Concerns over failed replications have eroded faith in science, with claims that the vast majority of published research is false. However, these claims are often made under the assumption that effect sizes are fixed and point null hypotheses can be true in practice. Here we derive a theoretical model of the publication process that instead accounts for variation in observed effect sizes. We show that replication rates provide little insight into whether a scientific discipline is reliably and efficiently producing knowledge. In applying our model to data from a large-scale replication survey, we reveal that concerns over the reliability of scientific research may be overstated. Finally, we highlight how proposed reforms may be ineffective at improving replicability and can be detrimental to orthogonal measures of scientific productivity.},
  langid = {american},
  pubstate = {preprint},
  keywords = {read,Social and Behavioral Sciences,Social Statistics},
  file = {/home/vboyce/Zotero/storage/HHJMQHCD/Bak-Coleman et al. - 2022 - Replication does not reliably measure scientific p.pdf}
}

@article{camerer2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  date = {2016-03-25},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0918},
  url = {https://www.science.org/doi/10.1126/science.aaf0918},
  urldate = {2023-03-17},
  abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/EH39UNZJ/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf}
}

@article{camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  date = {2018-08-27},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {2},
  number = {9},
  pages = {637--644},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  url = {https://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2023-02-11},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/DBNTT4GA/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{crump2013a,
  ids = {crump2013},
  title = {Evaluating {{Amazon}}'s {{Mechanical Turk}} as a {{Tool}} for {{Experimental Behavioral Research}}},
  author = {Crump, Matthew J. C. and McDonnell, John V. and Gureckis, Todd M.},
  date = {2013-03-13},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  number = {3},
  pages = {e57410},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0057410},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410},
  urldate = {2020-09-30},
  abstract = {Amazon Mechanical Turk (AMT) is an online crowdsourcing service where anonymous online workers complete web-based tasks for small sums of money. The service has attracted attention from experimental psychologists interested in gathering human subject data more efficiently. However, relative to traditional laboratory studies, many aspects of the testing environment are not under the experimenter's control. In this paper, we attempt to empirically evaluate the fidelity of the AMT system for use in cognitive behavioral experiments. These types of experiment differ from simple surveys in that they require multiple trials, sustained attention from participants, comprehension of complex instructions, and millisecond accuracy for response recording and stimulus presentation. We replicate a diverse body of tasks from experimental psychology including the Stroop, Switching, Flanker, Simon, Posner Cuing, attentional blink, subliminal priming, and category learning tasks using participants recruited using AMT. While most of replications were qualitatively successful and validated the approach of collecting data anonymously online using a web-browser, others revealed disparity between laboratory results and online results. A number of important lessons were encountered in the process of conducting these replications that should be of value to other researchers.},
  langid = {english},
  keywords = {Attention,Experimental psychology,Human learning,Internet,Learning,Learning curves,Payment,Reaction time,read},
  file = {/home/vboyce/Zotero/storage/ZEC2UBQV/Crump et al. - 2013 - Evaluating Amazon's Mechanical Turk as a Tool for .pdf;/home/vboyce/Zotero/storage/PQIINGVU/article.html}
}

@article{dreber2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  date = {2015-12-15},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci.},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1516179112},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1516179112},
  urldate = {2023-03-17},
  abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants’ individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a “statistically significant” finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/HIYRRZKP/Dreber et al. - 2015 - Using prediction markets to estimate the reproduci.pdf}
}

@article{ebersole2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and van Allen, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2016-11-01},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
  urldate = {2023-02-10},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  keywords = {Cognitive psychology,Individual differences,Participant pool,read,Replication,Sampling effects,Situational effects,Social psychology},
  file = {/home/vboyce/Zotero/storage/DRUUKDVH/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf;/home/vboyce/Zotero/storage/44D4H5C4/S0022103115300123.html}
}

@article{ebersole2020,
  title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and Bart-Plange, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarević, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Baník, Gabriel and Baskin, Ernest and Belopavlović, Radomir and Bernstein, Michael H. and Białek, Michał and Bloxsom, Nicholas G. and Bodroža, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Brühlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and Čolić, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falcão, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, Máire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Hałasa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and Hüffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and Jiménez-Leal, William and Johannesson, Magnus and Joy-Gaba, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Kołodziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and de Lima, Tiago Jessé Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, Łukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafał and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orlić, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedović, Ivana and Pękala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrović, Boban and Pfeiffer, Thomas and Pieńkosz, Damian and Preti, Emanuele and Purić, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemysław and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and Schulz-Hardt, Stefan and Schütz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, Rúben and Sioma, Barbara and Skorb, Lauren and de Souza, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilović, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Szöke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and Žeželj, Iris and Zrubka, Mark and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2020-09-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Adv. Methods Pract. Psychol. Sci.},
  volume = {3},
  number = {3},
  pages = {309--331},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920958687},
  url = {https://doi.org/10.1177/2515245920958687},
  urldate = {2023-02-10},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3?9; median total sample = 1,279.5, range = 276?3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (?r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00?.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19?.50).},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/3UTAEQGU/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf}
}

@article{errington2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  date = {2021-12-07},
  journaltitle = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  url = {https://doi.org/10.7554/eLife.71601},
  urldate = {2022-11-16},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,read,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/home/vboyce/Zotero/storage/S78PG95J/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{etz2016,
  title = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}: {{Psychology}}},
  shorttitle = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  date = {2016-02-26},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {11},
  number = {2},
  pages = {e0149794},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0149794},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149794},
  urldate = {2023-03-17},
  abstract = {We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75\% of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor {$<$} 10). The majority of the studies (64\%) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15\%), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10\%), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.},
  langid = {english},
  keywords = {Analysts,Experimental psychology,Psychology,Publication ethics,read,Replication studies,Reproducibility,Statistical data,Statistical distributions},
  file = {/home/vboyce/Zotero/storage/8L573IDM/Etz and Vandekerckhove - 2016 - A Bayesian Perspective on the Reproducibility Proj.pdf}
}

@article{forsell2019,
  title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
  author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
  date = {2019-12-01},
  journaltitle = {Journal of Economic Psychology},
  shortjournal = {Journal of Economic Psychology},
  series = {Replications in {{Economic Psychology}} and {{Behavioral Economics}}},
  volume = {75},
  pages = {102117},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2018.10.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0167487018303283},
  urldate = {2023-03-17},
  abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p\,{$<$}\,0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
  langid = {english},
  keywords = {Beliefs,Prediction markets,read,Replications,Reproducibility},
  file = {/home/vboyce/Zotero/storage/EQMDAYS5/Forsell et al. - 2019 - Predicting replication outcomes in the Many Labs 2.pdf;/home/vboyce/Zotero/storage/CYUI5JQH/S0167487018303283.html}
}

@article{frank2012,
  ids = {frankTeachingReplication2012a},
  title = {Teaching {{Replication}}:},
  shorttitle = {Teaching {{Replication}}},
  author = {Frank, Michael C. and Saxe, Rebecca},
  date = {2012-11-07},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect. Psychol. Sci.},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/1745691612460686},
  url = {https://journals.sagepub.com/doi/10.1177/1745691612460686},
  urldate = {2020-09-14},
  abstract = {Replication is held as the gold standard for ensuring the reliability of published scientific literature. But conducting direct replications is expensive, time-...},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/MYPG288Y/Frank and Saxe - 2012 - Teaching Replication.pdf;/home/vboyce/Zotero/storage/R5PGKLUR/1745691612460686.html}
}

@article{gelman2018,
  title = {Don't Characterize Replications as Successes or Failures},
  author = {Gelman, Andrew},
  year = {2018/ed},
  journaltitle = {Behavioral and Brain Sciences},
  shortjournal = {Behav. Brain Sci.},
  volume = {41},
  pages = {e128},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X18000638},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/dont-characterize-replications-as-successes-or-failures/1AB661F1E7A08E9C5870258878DBA0EA},
  urldate = {2023-04-04},
  abstract = {No replication is truly direct, and I recommend moving away from the classification of replications as “direct” or “conceptual” to a framework in which we accept that treatment effects vary across conditions. Relatedly, we should stop labeling replications as successes or failures and instead use continuous measures to compare different studies, again using meta-analysis of raw data where possible.},
  langid = {english},
  keywords = {read}
}

@article{gilbert2016,
  title = {Comment on “{{Estimating}} the Reproducibility of Psychological Science”},
  author = {Gilbert, Daniel T. and King, Gary and Pettigrew, Stephen and Wilson, Timothy D.},
  date = {2016-03-04},
  journaltitle = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aad7243},
  url = {https://www.science.org/doi/10.1126/science.aad7243},
  urldate = {2023-03-17},
  abstract = {A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.},
  file = {/home/vboyce/Zotero/storage/232VAWWQ/Gilbert et al. - 2016 - Comment on “Estimating the reproducibility of psyc.pdf}
}

@article{hawkins,
  title = {Improving the {{Replicability}} of {{Psychological Science Through Pedagogy}}},
  author = {Hawkins, Robert X D and Smith, Eric N and Au, Carolyn and Arias, Juan Miguel and Hermann, Eric and Keil, Martin and Lampinen, Andrew and Raposo, Sarah and Salehi, Shima and Salloum, Justin and Tan, Jed and Frank, Michael C},
  pages = {41},
  abstract = {Replications are important to science, but who will do them? One proposal is that students can conduct replications as part of their training. As a proof-of-concept for this idea, here we report a series of 11 pre-registered replications of findings from the 2015 volume of Psychological Science, all conducted as part of a graduate-level course. Congruent with larger, more systematic efforts, replications typically yielded smaller effects than originals: The modal outcome was partial support for the original claim. This work documents the challenges facing motivated students as they attempt to replicate previously published results on a first attempt. We describe the workflow and pedagogical methods that were used in the class and discuss implications both for the adoption of this pedagogical model and for replication research more broadly.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/R9HV5BKA/Hawkins et al. - Improving the Replicability of Psychological Scien.pdf}
}

@report{hoogeveen2019,
  type = {preprint},
  title = {Laypeople {{Can Predict Which Social Science Studies Replicate}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  date = {2019-09-25},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/egw9d},
  url = {https://osf.io/egw9d},
  urldate = {2019-09-30},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social science literature could not be replicated successfully.  Here we assess the extent to which a finding’s replication success relates to its intuitive plausibility. Each of 27 high-profile social science findings was evaluated by 233 people without a PhD in psychology. Results showed that these laypeople predicted replication success with above-chance performance (i.e., 58\%). In addition, when laypeople were informed about the strength of evidence from the original studies, this boosted their prediction performance to 67\%. We discuss the prediction patterns and apply signal detection theory to disentangle detection ability from response bias. Our study suggests that laypeople’s predictions contain useful information for assessing the probability that a given finding will replicate successfully.},
  keywords = {read}
}


@article{hagger2016,
	title = {A Multilab Preregistered Replication of the Ego-Depletion Effect},
	volume = {11},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691616652873},
	doi = {10.1177/1745691616652873},
	abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals ({CIs}) that encompassed zero (d = 0.04, 95\% {CI} [?0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
	pages = {546--573},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Schütz, A. and Stamos, A. and Tinghög, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
	urldate = {2023-04-27},
	date = {2016-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/home/vboyce/Zotero/storage/SYM3U87Q/Hagger et al. - 2016 - A Multilab Preregistered Replication of the Ego-De.pdf:application/pdf},
}

@article{jekel2020,
  title = {How to {{Teach Open Science Principles}} in the {{Undergraduate Curriculum}}—{{The Hagen Cumulative Science Project}}},
  author = {Jekel, Marc and Fiedler, Susann and Allstadt Torras, Ramona and Mischkowski, Dorothee and Dorrough, Angela Rachael and Glöckner, Andreas},
  date = {2020-03-01},
  journaltitle = {Psychology Learning \& Teaching},
  shortjournal = {Psychol. Learn. Teach.},
  volume = {19},
  number = {1},
  pages = {91--106},
  publisher = {{SAGE Publications}},
  issn = {1475-7257},
  doi = {10.1177/1475725719868149},
  url = {https://doi.org/10.1177/1475725719868149},
  urldate = {2023-04-04},
  abstract = {The Hagen Cumulative Science Project is a large-scale replication project based on students? thesis work. In the project, we aim to (a) teach students to conduct the entire research process for conducting a replication according to open science standards and (b) contribute to cumulative science by increasing the number of direct replications. We describe the procedural steps of the project from choosing suitable replication studies to guiding students through the process of conducting a replication, and processing results in a meta-analysis. Based on the experience of more than 80 replications, we summarize how such a project can be implemented. We present practical solutions that have been shown to be successful as well as discuss typical obstacles and how they can be solved. We argue that replication projects are beneficial for all groups involved: Students benefit by being guided through a highly structured protocol and making actual contributions to science. Instructors benefit by using time resources effectively for cumulative science and fulfilling teaching obligations in a meaningful way. The scientific community benefits from the resulting greater number of replications and teaching state-of-the-art methodology. We encourage the use of student thesis-based replication projects for thesis work in academic bachelor and master curricula.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/LIXYF3KI/Jekel et al. - 2020 - How to Teach Open Science Principles in the Underg.pdf}
}

@article{jern2018,
  title = {A Preliminary Study of the Educational Benefits of Conducting Replications in the Classroom},
  author = {Jern, Alan},
  date = {2018},
  journaltitle = {Scholarship of Teaching and Learning in Psychology},
  shortjournal = {Scholarsh. Teach. Learn. Psychol.},
  volume = {4},
  pages = {64--68},
  publisher = {{Educational Publishing Foundation}},
  location = {{US}},
  issn = {2332-211X},
  doi = {10.1037/stl0000104},
  abstract = {Some have argued that having students conduct rigorous replications of published studies would provide benefits to both psychological science and the students themselves. However, while it seems clear that replications are beneficial to psychological science, there is little empirical evidence that having students conduct replications provides benefits to students. In this study, I conducted a preliminary test (N = 37) of one purported benefit to students of conducting a classroom replication: the development of scientific critical thinking skills. Students completed a 1-term research methods course centered on a class replication project. I assessed students’ critical thinking development after completing the course. The results were largely inconclusive, showing no significant change in performance between a pretest (M = 11.00, SD = 3.44) and a posttest (M = 10.30, SD = 2.62), t(36) = −1.21, p = .23. This study highlights the need for additional research on the question of whether having students conduct replications provides educational benefits beyond those offered by other pedagogical methods. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Classrooms,Critical Thinking,Experimental Replication,Methodology,Psychology Education,Undergraduate Education},
  file = {/home/vboyce/Zotero/storage/BL62BTSS/Jern - 2018 - A preliminary study of the educational benefits of.pdf;/home/vboyce/Zotero/storage/JT5ET6TR/2018-11647-005.html}
}

@article{klein2014,
  title = {Investigating {{Variation}} in {{Replicability}}: {{A}} “{{Many Labs}}” {{Replication Project}}},
  shorttitle = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‘t Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2014-05-01},
  journaltitle = {Social Psychology},
  shortjournal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000178},
  url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
  urldate = {2023-02-10},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/5N98UHLQ/Klein et al. - 2014 - Investigating Variation in Replicability A “Many .pdf}
}

@article{klein2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Lee Nichols, Austin and Ocampo, Aaron and O’Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ’t Veer, Anna Elisabeth and Vásquez- Echeverría, Alejandro and Ann Vaughn, Leigh and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2018-12-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Adv. Methods Pract. Psychol. Sci.},
  volume = {1},
  number = {4},
  pages = {443--490},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  url = {https://doi.org/10.1177/2515245918810225},
  urldate = {2023-02-10},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen?s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/RI74ZVZ8/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@article{klein2022,
  title = {Many {{Labs}} 4: {{Failure}} to {{Replicate Mortality Salience Effect With}} and {{Without Original Author Involvement}}},
  shorttitle = {Many {{Labs}} 4},
  author = {Klein, Richard A and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Hilgard, Joseph and Ahn, Paul Hangsan and Brady, Abbie J. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret T. and Cromar, Ryan and Gardiner, Gwendolyn and Gosnell, Courtney L. and Grahe, Jon and Hall, Calvin and Howard, Irene and Joy-Gaba, Jennifer A. and Kolb, Miranda and Legg, Angela M. and Levitan, Carmel A. and Mancini, Anthony D. and Manfredi, Dylan and Miller, Jason and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine L. M. and Storage, Daniel and Swanson, Trevor and Van Swol, Lyn M. and Vaughn, Leigh Ann and Vidamuerte, Devere and Wiggins, Brady and Ratliff, Kate A.},
  date = {2022-04-29},
  journaltitle = {Collabra: Psychology},
  shortjournal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {35271},
  issn = {2474-7394},
  doi = {10.1525/collabra.35271},
  url = {https://doi.org/10.1525/collabra.35271},
  urldate = {2023-02-10},
  abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 17 Labs and N = 1,550 participants, after exclusions) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not fully understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/SVP6S3SB/Klein et al. - 2022 - Many Labs 4 Failure to Replicate Mortality Salien.pdf;/home/vboyce/Zotero/storage/7KS2AIAS/168050.html}
}

@article{lewandowsky2020,
  title = {Low Replicability Can Support Robust and Efficient Science},
  author = {Lewandowsky, Stephan and Oberauer, Klaus},
  date = {2020-01-17},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {1--12},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-14203-0},
  url = {https://www.nature.com/articles/s41467-019-14203-0},
  urldate = {2020-01-21},
  abstract = {There has been much concern about the “replication crisis” in psychology and other disciplines. Here the authors show that an efficient solution to the crisis would not insist on replication before publication, and would instead encourage publication before replication, with the findings marked as preliminary.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/GVMLLYIE/Lewandowsky and Oberauer - 2020 - Low replicability can support robust and efficient.pdf;/home/vboyce/Zotero/storage/LWXTQE3H/s41467-019-14203-0.html}
}

@article{mathur2020,
  title = {New Statistical Metrics for Multisite Replication Projects},
  author = {Mathur, Maya B. and VanderWeele, Tyler J.},
  date = {2020},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  shortjournal = {J. R. Stat. Soc. Ser. A Stat. Soc.},
  volume = {183},
  number = {3},
  pages = {1145--1166},
  issn = {1467-985X},
  doi = {10.1111/rssa.12572},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12572},
  urldate = {2023-04-04},
  abstract = {Increasingly, researchers are attempting to replicate published original studies by using large, multisite replication projects, at least 134 of which have been completed or are on going. These designs are promising to assess whether the original study is statistically consistent with the replications and to reassess the strength of evidence for the scientific effect of interest. However, existing analyses generally focus on single replications; when applied to multisite designs, they provide an incomplete view of aggregate evidence and can lead to misleading conclusions about replication success. We propose new statistical metrics representing firstly the probability that the original study's point estimate would be at least as extreme as it actually was, if in fact the original study were statistically consistent with the replications, and secondly the estimated proportion of population effects agreeing in direction with the original study. Generalized versions of the second metric enable consideration of only meaningfully strong population effects that agree in direction, or alternatively that disagree in direction, with the original study. These metrics apply when there are at least 10 replications (unless the heterogeneity estimate τ\^=0, in which case the metrics apply regardless of the number of replications). The first metric assumes normal population effects but appears robust to violations in simulations; the second is distribution free. We provide R packages (Replicate and MetaUtility).},
  langid = {english},
  keywords = {Effect sizes,Heterogeneity,read,Replication,Reproducibility},
  file = {/home/vboyce/Zotero/storage/Q4YLCSED/Mathur and VanderWeele - 2020 - New statistical metrics for multisite replication .pdf;/home/vboyce/Zotero/storage/Q583USBP/rssa.html}
}

@article{mcshane2019,
  title = {Large-{{Scale Replication Projects}} in {{Contemporary Psychological Research}}},
  author = {McShane, Blakeley B. and Tackett, Jennifer L. and Böckenholt, Ulf and Gelman, Andrew},
  date = {2019-03-29},
  journaltitle = {The American Statistician},
  shortjournal = {Am. Stat.},
  volume = {73},
  pages = {99--105},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1505655},
  url = {https://doi.org/10.1080/00031305.2018.1505655},
  urldate = {2023-02-10},
  abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next—an issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching large-scale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous—even if the sign of the effect is consistent—the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
  issue = {sup1},
  keywords = {Between-study variation,Heterogeneity,Hierarchical,Meta-Analysis,Multilevel,Null hypothesis significance testing,p-value,Psychology,read,Replication},
  file = {/home/vboyce/Zotero/storage/T9294332/McShane et al. - 2019 - Large-Scale Replication Projects in Contemporary P.pdf}
}

@article{nosek2020,
  title = {The Best Time to Argue about What a Replication Means? {{Before}} You Do It},
  shorttitle = {The Best Time to Argue about What a Replication Means?},
  author = {Nosek, Brian A. and Errington, Timothy M.},
  date = {2020-07},
  journaltitle = {Nature},
  volume = {583},
  number = {7817},
  pages = {518--520},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-02142-6},
  url = {https://www.nature.com/articles/d41586-020-02142-6},
  urldate = {2023-03-17},
  abstract = {To avoid stalemates and provide lessons, replicators and original researchers must reach agreement on a study design and set out expectations ahead of time.},
  issue = {7817},
  langid = {english},
  keywords = {Publishing,read,Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Publishing, Research data, Research management},
  file = {/home/vboyce/Zotero/storage/2WGSH9TU/Nosek and Errington - 2020 - The best time to argue about what a replication me.pdf;/home/vboyce/Zotero/storage/YQF3YWQH/d41586-020-02142-6.html}
}

@article{nosek2022,
  ids = {nosek2022a},
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aurélien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Michèle B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Schönbrodt, Felix D. and Vazire, Simine},
  date = {2022-01-04},
  journaltitle = {Annual Review of Psychology},
  shortjournal = {Annu. Rev. Psychol.},
  volume = {73},
  number = {1},
  eprint = {34665669},
  eprinttype = {pmid},
  pages = {719--748},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-020821-114157},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157},
  urldate = {2023-02-10},
  abstract = {Replication—an important, uncommon, and misunderstood practice—is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
  langid = {english},
  keywords = {generalizability,metascience,read,replication,reproducibility,research methods,robustness,statistical inference,theory,validity},
  file = {/home/vboyce/Zotero/storage/MCGCYCM7/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf;/home/vboyce/Zotero/storage/V6P5NF42/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}

@article{odonnell2021,
  title = {Empirical Audit and Review and an Assessment of Evidentiary Value in Research on the Psychological Consequences of Scarcity},
  author = {O’Donnell, Michael and Dev, Amelia S. and Antonoplis, Stephen and Baum, Stephen M. and Benedetti, Arianna H. and Brown, N. Derek and Carrillo, Belinda and Choi, Andrew L. and Connor, Paul and Donnelly, Kristin and Ellwood-Lowe, Monica E. and Foushee, Ruthe and Jansen, Rachel and Jarvis, Shoshana N. and Lundell-Creagh, Ryan and Ocampo, Joseph M. and Okafor, Gold N. and Azad, Zahra Rahmani and Rosenblum, Michael and Schatz, Derek and Stein, Daniel H. and Wang, Yilu and Moore, Don A. and Nelson, Leif D.},
  date = {2021-11-02},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {118},
  number = {44},
  pages = {e2103313118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2103313118},
  url = {https://pnas.org/doi/10.1073/pnas.2103313118},
  urldate = {2023-04-04},
  abstract = {Empirical audit and review is an approach to assessing the evidentiary value of a research area. It involves identifying a topic and selecting a cross-section of studies for replication. We apply the method to research on the psychological consequences of scarcity. Starting with the papers citing a seminal publication in the field, we conducted replications of 20 studies that evaluate the role of scarcity priming in pain sensitivity, resource allocation, materialism, and many other domains. There was considerable variability in the replicability, with some strong successes and other undeniable failures. Empirical audit and review does not attempt to assign an overall replication rate for a heterogeneous field, but rather facilitates researchers seeking to incorporate strength of evidence as they refine theories and plan new investigations in the research area. This method allows for an integration of qualitative and quantitative approaches to review and enables the growth of a cumulative science.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/UBHIV97L/O’Donnell et al. - 2021 - Empirical audit and review and an assessment of ev.pdf}
}

@article{openscienceconsortium2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Consortium}},
  date = {2015},
  journaltitle = {Science},
  url = {https://www.science.org/doi/full/10.1126/science.aac4716?casa_token=IJ35TwwlcjsAAAAA%3AqiP68QbVAHleIg9zD3WugKWuV6Oa5rswS0VQnDsCq5I14ME4WIQabNGVD_T6SBSuAt6voVHNnWc0sw},
  urldate = {2023-03-17},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\textbackslash\% of original effect sizes were in the 95\textbackslash\% confidence interval of the replication effect size; 39\textbackslash\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\textbackslash\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/372DFPZ2/RPP_SCIENCE_2015.pdf;/home/vboyce/Zotero/storage/8JGRI4V2/science.html}
}


@article{crump2013,
	title = {Evaluating Amazon's Mechanical Turk as a Tool for Experimental Behavioral Research},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410},
	doi = {10.1371/journal.pone.0057410},
	abstract = {Amazon Mechanical Turk ({AMT}) is an online crowdsourcing service where anonymous online workers complete web-based tasks for small sums of money. The service has attracted attention from experimental psychologists interested in gathering human subject data more efficiently. However, relative to traditional laboratory studies, many aspects of the testing environment are not under the experimenter's control. In this paper, we attempt to empirically evaluate the fidelity of the {AMT} system for use in cognitive behavioral experiments. These types of experiment differ from simple surveys in that they require multiple trials, sustained attention from participants, comprehension of complex instructions, and millisecond accuracy for response recording and stimulus presentation. We replicate a diverse body of tasks from experimental psychology including the Stroop, Switching, Flanker, Simon, Posner Cuing, attentional blink, subliminal priming, and category learning tasks using participants recruited using {AMT}. While most of replications were qualitatively successful and validated the approach of collecting data anonymously online using a web-browser, others revealed disparity between laboratory results and online results. A number of important lessons were encountered in the process of conducting these replications that should be of value to other researchers.},
	pages = {e57410},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Crump, Matthew J. C. and {McDonnell}, John V. and Gureckis, Todd M.},
	urldate = {2020-09-30},
	date = {2013-03-13},
	langid = {english},
	note = {publisher: Public Library of Science},
	keywords = {read, Internet, Attention, Experimental psychology, Human learning, Learning, Learning curves, Payment, Reaction time},
	file = {Full Text PDF:/home/vboyce/Zotero/storage/ZEC2UBQV/Crump et al. - 2013 - Evaluating Amazon's Mechanical Turk as a Tool for .pdf:application/pdf;Snapshot:/home/vboyce/Zotero/storage/PQIINGVU/article.html:text/html},
}

@article{patil2016,
  title = {What {{Should Researchers Expect When They Replicate Studies}}? {{A Statistical View}} of {{Replicability}} in {{Psychological Science}}},
  shorttitle = {What {{Should Researchers Expect When They Replicate Studies}}?},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  date = {2016-07},
  journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {11},
  number = {4},
  eprint = {27474140},
  eprinttype = {pmid},
  pages = {539--544},
  issn = {1745-6924},
  doi = {10.1177/1745691616646366},
  abstract = {A recent study of the replicability of key psychological findings is a major contribution toward understanding the human side of the scientific process. Despite the careful and nuanced analysis reported, the simple narrative disseminated by the mass, social, and scientific media was that in only 36\% of the studies were the original results replicated. In the current study, however, we showed that 77\% of the replication effect sizes reported were within a 95\% prediction interval calculated using the original effect size. Our analysis suggests two critical issues in understanding replication of psychological studies. First, researchers' intuitive expectations for what a replication should show do not always match with statistical estimates of replication. Second, when the results of original studies are very imprecise, they create wide prediction intervals-and a broad range of replication effects that are consistent with the original estimates. This may lead to effects that replicate successfully, in that replication results are consistent with statistical expectations, but do not provide much information about the size (or existence) of the true effect. In this light, the results of the Reproducibility Project: Psychology can be viewed as statistically consistent with what one might expect when performing a large-scale replication experiment.},
  langid = {english},
  pmcid = {PMC4968573},
  keywords = {Behavioral Research,Data Interpretation; Statistical,Humans,p values,prediction intervals,Psychology,read,replication,reproducibility,Reproducibility of Results,Reproducibility Project: Psychology},
  file = {/home/vboyce/Zotero/storage/4I4NTQ52/Patil et al. - 2016 - What Should Researchers Expect When They Replicate.pdf}
}

@article{pawel2020,
  title = {Probabilistic Forecasting of Replication Studies},
  author = {Pawel, Samuel and Held, Leonhard},
  date = {2020-04-22},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231416},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231416},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231416},
  urldate = {2023-03-17},
  abstract = {Throughout the last decade, the so-called replication crisis has stimulated many researchers to conduct large-scale replication projects. With data from four of these projects, we computed probabilistic forecasts of the replication outcomes, which we then evaluated regarding discrimination, calibration and sharpness. A novel model, which can take into account both inflation and heterogeneity of effects, was used and predicted the effect estimate of the replication study with good performance in two of the four data sets. In the other two data sets, predictive performance was still substantially improved compared to the naive model which does not consider inflation and heterogeneity of effects. The results suggest that many of the estimates from the original studies were inflated, possibly caused by publication bias or questionable research practices, and also that some degree of heterogeneity between original and replication effects should be expected. Moreover, the results indicate that the use of statistical significance as the only criterion for replication success may be questionable, since from a predictive viewpoint, non-significant replication results are often compatible with significant results from the original study. The developed statistical methods as well as the data sets are available in the R package ReplicationSuccess.},
  langid = {english},
  keywords = {Economics,Experimental economics,Experimental psychology,Forecasting,read,Replication studies,Social psychology,Social sciences,Test statistics},
  file = {/home/vboyce/Zotero/storage/P6HK7WJE/Pawel and Held - 2020 - Probabilistic forecasting of replication studies.pdf}
}

@article{pownall2021,
  title = {Embedding Open and Reproducible Science into Teaching: {{A}} Bank of Lesson Plans and Resources},
  shorttitle = {Embedding Open and Reproducible Science into Teaching},
  author = {Pownall, Madeleine and Azevedo, Flavio and Aldoh, Alaa and Elsherif, Mahmoud and Vasilev, Martin and Pennington, Charlotte R. and Robertson, Olly and Tromp, Myrthe Vel and Liu, Meng and Makel, Matthew C. and Tonge, Natasha and Moreau, David and Horry, Ruth and Shaw, John and Tzavella, Loukia and McGarrigle, Ronan and Talbot, Catherine and Parsons, Sam},
  date = {2021},
  journaltitle = {Scholarship of Teaching and Learning in Psychology},
  shortjournal = {Scholarsh. Teach. Learn. Psychol.},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{Educational Publishing Foundation}},
  location = {{US}},
  issn = {2332-211X},
  doi = {10.1037/stl0000307},
  abstract = {Recently, there has been a growing emphasis on embedding open and reproducible approaches into research. One essential step in accomplishing this larger goal is to embed such practices into undergraduate and postgraduate research training. However, this often requires substantial time and resources to implement. Also, while many pedagogical resources are regularly developed for this purpose, they are not often openly and actively shared with the wider community. The creation and public sharing of open educational resources is useful for educators who wish to embed open scholarship and reproducibility into their teaching and learning. In this article, we describe and openly share a bank of teaching resources and lesson plans on the broad topics of open scholarship, open science, replication, and reproducibility that can be integrated into taught courses to support educators and instructors. These resources were created as part of the Society for the Improvement of Psychological Science (SIPS) hackathon at the 2021 Annual Conference, and we detail this collaborative process in the article. By sharing these open pedagogical resources, we aim to reduce the labor required to develop and implement open scholarship content to further the open scholarship and open educational materials movement. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {College Students,Lesson Plans,Nontraditional Education,Open Data,Open Science,read,School Learning,Sciences,Teachers,Teaching,Teaching Methods,Training},
  file = {/home/vboyce/Zotero/storage/2TMKCJZA/Pownall et al. - 2021 - Embedding open and reproducible science into teach.pdf;/home/vboyce/Zotero/storage/3AGXGDH7/2022-15361-001.html}
}

@report{protzko2020,
  type = {preprint},
  title = {High {{Replicability}} of {{Newly-Discovered Social-behavioral Findings}} Is {{Achievable}}},
  author = {Protzko, John and Krosnick, Jon and Nelson, Leif D. and Nosek, Brian A. and Axt, Jordan and Berent, Matthew and Buttrick, Nick and DeBell, Matthew and Ebersole, Charles R. and Lundmark, Sebastian and MacInnis, Bo and O'Donnell, Michael and Perfecto, Hannah and Pustejovsky, James E and Roeder, Scott S. and Walleczek, Jan and Schooler, Jonathan},
  date = {2020-09-10},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/n2a9x},
  url = {https://osf.io/n2a9x},
  urldate = {2023-04-05},
  abstract = {Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigor-enhancing practices: confirmatory tests, large sample sizes, preregistration, and methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50\%, replication attempts here produced the expected effects with significance testing (p\&lt;.05) in 86\% of attempts, slightly exceeding maximum expected replicability based on observed effect sizes and sample sizes. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97\% that of the original study. This high replication rate justifies confidence in rigor enhancing methods to increase the replicability of new discoveries.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/HZ2FV597/Protzko et al. - 2020 - High Replicability of Newly-Discovered Social-beha.pdf}
}

@article{quintana2021,
  title = {Replication Studies for Undergraduate Theses to Improve Science and Education},
  author = {Quintana, Daniel S.},
  date = {2021-09},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {5},
  number = {9},
  pages = {1117--1118},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01192-8},
  url = {https://www.nature.com/articles/s41562-021-01192-8},
  urldate = {2023-04-04},
  abstract = {Requiring undergraduate students to perform what is termed original research for their thesis, an investigation that cannot constitute a replication of an existing study, is a failed opportunity for science and education, argues Daniel Quintana.},
  issue = {9},
  langid = {english},
  keywords = {Behavioral Sciences,Experimental Psychology,general,Life Sciences,Microeconomics,Neurosciences,Personality and Social Psychology},
  file = {/home/vboyce/Zotero/storage/AIIH4N6M/Quintana - 2021 - Replication studies for undergraduate theses to im.pdf}
}

@article{ramscar,
  title = {Why Many Priming Results Don’t (and Won’t) Replicate: {{A}} Quantitative Analysis},
  author = {Ramscar, Michael and Shaoul, Cyrus and Baayen, R Harald},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/VAL7VCFH/Ramscar et al. - Why many priming results don’t (and won’t) replica.pdf}
}

@article{schmidt2009,
  title = {Shall We {{Really}} Do It {{Again}}? {{The Powerful Concept}} of {{Replication}} Is {{Neglected}} in the {{Social Sciences}}},
  shorttitle = {Shall We {{Really}} Do It {{Again}}?},
  author = {Schmidt, Stefan},
  date = {2009-06-01},
  journaltitle = {Review of General Psychology},
  shortjournal = {Rev. Gen. Psychol.},
  volume = {13},
  number = {2},
  pages = {90--100},
  publisher = {{SAGE Publications Inc}},
  issn = {1089-2680},
  doi = {10.1037/a0015108},
  url = {https://doi.org/10.1037/a0015108},
  urldate = {2023-04-04},
  abstract = {Replication is one of the most important tools for the verification of facts within the empirical sciences. A detailed examination of the notion of replication reveals that there are many different meanings to this concept and the relevant procedures, but hardly any systematic literature. This paper analyzes the concept of replication from a theoretical point of view. It demonstrates that the theoretical demands are scarcely met in everyday work within the social sciences. Some demands are just not feasible, whereas others are constricted by restrictions relating to publication. A new classification scheme based on a functional approach that distinguishes between different types of replication is proposed. Next, it will be argued that replication addresses the important connection between existing and new knowledge. To do so it has to be applied explicitly and systematically. The paper ends with a description of procedures how this could be done and a set of recommendations how to handle the concept of replication in the future to exploit its potential to the full.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/26KGZ44C/Schmidt - 2009 - Shall we Really do it Again The Powerful Concept .pdf}
}

@article{simonsohn2015,
  title = {Small {{Telescopes}}: {{Detectability}} and the {{Evaluation}} of {{Replication Results}}},
  shorttitle = {Small {{Telescopes}}},
  author = {Simonsohn, Uri},
  date = {2015-05-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {26},
  number = {5},
  pages = {559--569},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797614567341},
  url = {https://doi.org/10.1177/0956797614567341},
  urldate = {2023-04-04},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ?unsuccessful? replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ?protecting? true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/ARIPXHJ6/Simonsohn - 2015 - Small Telescopes Detectability and the Evaluation.pdf}
}

@article{vanbavel2016,
  title = {Contextual Sensitivity in Scientific Reproducibility},
  author = {Van Bavel, Jay J. and Mende-Siedlecki, Peter and Brady, William J. and Reinero, Diego A.},
  date = {2016-06-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci.},
  volume = {113},
  number = {23},
  pages = {6454--6459},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1521897113},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1521897113},
  urldate = {2023-04-05},
  abstract = {In recent years, scientists have paid increasing attention to reproducibility. For example, the Reproducibility Project, a large-scale replication attempt of 100 studies published in top psychology journals found that only 39\% could be unambiguously reproduced. There is a growing consensus among scientists that the lack of reproducibility in psychology and other fields stems from various methodological factors, including low statistical power, researcher’s degrees of freedom, and an emphasis on publishing surprising positive results. However, there is a contentious debate about the extent to which failures to reproduce certain results might also reflect contextual differences (often termed “hidden moderators”) between the original research and the replication attempt. Although psychologists have found extensive evidence that contextual factors alter behavior, some have argued that context is unlikely to influence the results of direct replications precisely because these studies use the same methods as those used in the original research. To help resolve this debate, we recoded the 100 original studies from the Reproducibility Project on the extent to which the research topic of each study was contextually sensitive. Results suggested that the contextual sensitivity of the research topic was associated with replication success, even after statistically adjusting for several methodological characteristics (e.g., statistical power, effect size). The association between contextual sensitivity and replication success did not differ across psychological subdisciplines. These results suggest that researchers, replicators, and consumers should be mindful of contextual factors that might influence a psychological process. We offer several guidelines for dealing with contextual sensitivity in reproducibility.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/3UHVUHMG/Van Bavel et al. - 2016 - Contextual sensitivity in scientific reproducibili.pdf}
}

@article{wagge2019,
  title = {Publishing {{Research With Undergraduate Students}} via {{Replication Work}}: {{The Collaborative Replications}} and {{Education Project}}},
  shorttitle = {Publishing {{Research With Undergraduate Students}} via {{Replication Work}}},
  author = {Wagge, Jordan R. and Brandt, Mark J. and Lazarevic, Ljiljana B. and Legate, Nicole and Christopherson, Cody and Wiggins, Brady and Grahe, Jon E.},
  date = {2019},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {10},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00247},
  urldate = {2023-04-04},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/K5DMQS64/Wagge et al. - 2019 - Publishing Research With Undergraduate Students vi.pdf}
}

@article{wilson2020,
  title = {Science Is Not a Signal Detection Problem},
  author = {Wilson, Brent M. and Harris, Christine R. and Wixted, John T.},
  date = {2020-03-17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {117},
  number = {11},
  pages = {5559--5567},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1914237117},
  url = {https://pnas.org/doi/full/10.1073/pnas.1914237117},
  urldate = {2023-04-05},
  abstract = {The perceived replication crisis and the reforms designed to address it are grounded in the notion that science is a binary signal detection problem. However, contrary to null hypothesis significance testing (NHST) logic, the magnitude of the underlying effect size for a given experiment is best conceptualized as a random draw from a continuous distribution, not as a random draw from a dichotomous distribution (null vs. alternative). Moreover, because continuously distributed effects selected using a               P               {$<$} 0.05 filter must be inflated, the fact that they are smaller when replicated (reflecting regression to the mean) is no reason to sound the alarm. Considered from this perspective, recent replication efforts suggest that most published               P               {$<$} 0.05 scientific findings are “true” (i.e., in the correct direction), with observed effect sizes that are inflated to varying degrees. We propose that original science is a screening process, one that adopts NHST logic as a useful fiction for selecting true effects that are potentially large enough to be of interest to other scientists. Unlike original science, replication science seeks to precisely measure the underlying effect size associated with an experimental protocol via large-               N               direct replication, without regard for statistical significance. Registered reports are well suited to (often resource-intensive) direct replications, which should focus on influential findings and be published regardless of outcome. Conceptual replications play an important but separate role in validating theories. However, because they are part of NHST-based original science, conceptual replications cannot serve as the field’s self-correction mechanism. Only direct replications can do that.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/J47QEVYF/Wilson et al. - 2020 - Science is not a signal detection problem.pdf}
}
