%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Mike Frank at 2023-07-08 18:03:09 +0200 


%% Saved with string encoding Unicode (UTF-8) 

@article{lebel2018,
  title = {A {{Unified Framework}} to {{Quantify}} the {{Credibility}} of {{Scientific Findings}}},
  author = {LeBel, Etienne P and McCarthy, Randy J and Earp, Brian D and Elson, Malte and Vanpaemel, Wolf},
  langid = {english},
}

@article{greenwald1976,
  title={Within-subjects designs: To use or not to use?},
  author={Greenwald, Anthony G},
  journal={Psychological Bulletin},
  volume={83},
  number={2},
  pages={314},
  year={1976},
  publisher={American Psychological Association}
}

@misc{osfdata,
  title={Meta Analysis of class replication projects from PSYCH251},
  url={osf.io/xwn9m},
  publisher={OSF},
  doi={10.17605/OSF.IO/XWN9M},
  author={Boyce, Veronica},
  year={2023},
  month={Oct}
}

@article{hawkins2018,
	abstract = {Replications are important to science, but who will do them? One
               proposal is that students can conduct replications as part of
               their training. As a proof of concept for this idea, here we
               report a series of 11 preregistered replications of findings
               from the 2015 volume of Psychological Science, all conducted as
               part of a graduate-level course. As was expected given larger,
               more systematic prior efforts, the replications typically
               yielded effects that were smaller than the original ones: The
               modal outcome was partial support for the original claim. This
               work documents the challenges facing motivated students as they
               attempt to replicate previously published results on a first
               attempt. We describe the workflow and pedagogical methods that
               were used in the class and discuss implications both for the
               adoption of this pedagogical model and for replication research
               more broadly.},
	author = {Hawkins, Robert D and Smith, Eric N and Au, Carolyn and Arias, Juan Miguel and Catapano, Rhia and Hermann, Eric and Keil, Martin and Lampinen, Andrew and Raposo, Sarah and Reynolds, Jesse and Salehi, Shima and Salloum, Justin and Tan, Jed and Frank, Michael C},
	date-added = {2023-07-08 17:51:25 +0200},
	date-modified = {2023-07-08 17:51:25 +0200},
	journal = {Advances in Methods and Practices in Psychological Science},
	month = mar,
	number = 1,
	pages = {7--18},
	publisher = {SAGE Publications Inc},
	title = {Improving the Replicability of Psychological Science Through Pedagogy},
	volume = 1,
	year = 2018}

@misc{frank2023,
	author = {Frank, Michael C and Braginsky, Mika and Cachia, Julie and Coles, Nicholas and Hardwicke, Tom and Hawkins, Robert and Mathur, Maya B and Williams, Rondeline},
	date-added = {2023-07-08 17:11:41 +0200},
	date-modified = {2023-07-08 17:11:44 +0200},
	publisher = {Boston, MA: MIT Press},
	title = {Experimentology: an open science approach to experimental psychology methods},
	year = {2023}}

@book{rosenthal2008,
	author = {Rosenthal, Robert and Rosnow, Ralph L},
	date-added = {2023-07-08 16:59:25 +0200},
	date-modified = {2023-07-08 16:59:27 +0200},
	title = {Essentials of behavioral research: Methods and data analysis},
	year = {2008}}

@article{greenwald1976,
	author = {Greenwald, Anthony G},
	date-added = {2023-07-08 16:55:38 +0200},
	date-modified = {2023-07-08 16:55:40 +0200},
	journal = {Psychological Bulletin},
	number = {2},
	pages = {314},
	publisher = {American Psychological Association},
	title = {Within-subjects designs: To use or not to use?},
	volume = {83},
	year = {1976}}

@article{flake2020,
	author = {Flake, Jessica Kay and Fried, Eiko I},
	date-added = {2023-07-08 16:53:38 +0200},
	date-modified = {2023-07-08 16:53:40 +0200},
	journal = {Advances in Methods and Practices in Psychological Science},
	number = {4},
	pages = {456--465},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {Measurement schmeasurement: Questionable measurement practices and how to avoid them},
	volume = {3},
	year = {2020}}

@article{simmons2011,
	author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
	date-added = {2023-07-08 16:20:09 +0200},
	date-modified = {2023-07-08 16:20:12 +0200},
	journal = {Psychological science},
	number = {11},
	pages = {1359--1366},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
	volume = {22},
	year = {2011}}

@article{smaldino2016,
	author = {Paul E. Smaldino and Richard McElreath},
	date-added = {2023-07-08 16:19:58 +0200},
	date-modified = {2023-07-08 16:20:00 +0200},
	doi = {10.1098/rsos.160384},
	journal = {Royal Society Open Science},
	month = {sep},
	number = {9},
	pages = {160384},
	publisher = {The Royal Society},
	title = {The natural selection of bad science},
	url = {https://doi.org/10.1098%2Frsos.160384},
	volume = {3},
	year = 2016,
	bdsk-url-1 = {https://doi.org/10.1098%2Frsos.160384},
	bdsk-url-2 = {https://doi.org/10.1098/rsos.160384}}

@article{altmejd2019,
	abstract = {We measure how accurately replication of experimental results can be predicted by black-box statistical models. With data from four large-scale replication projects in experimental psychology and economics, and techniques from machine learning, we train predictive models and study which variables drive predictable replication. The models predicts binary replication with a cross-validated accuracy rate of 70\% (AUC of 0.77) and estimates of relative effect sizes with a Spearman ρ of 0.38. The accuracy level is similar to market-aggregated beliefs of peer scientists [1, 2]. The predictive power is validated in a pre-registered out of sample test of the outcome of [3], where 71\% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to ρ = 0.25. Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two-variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing the process of evaluation of new findings and guiding resources to those direct replications that are likely to be most informative.},
	author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
	date = {2019-12-05},
	doi = {10.1371/journal.pone.0225826},
	file = {/home/vboyce/Zotero/storage/UJDHJVZU/Altmejd et al. - 2019 - Predicting the replicability of social science lab.pdf},
	issn = {1932-6203},
	journaltitle = {PLOS ONE},
	keywords = {Experimental economics,Experimental psychology,Forecasting,Machine learning,Machine learning algorithms,read,Replication studies,Scientists,Trees},
	langid = {english},
	number = {12},
	pages = {e0225826},
	publisher = {{Public Library of Science}},
	shortjournal = {PLOS ONE},
	title = {Predicting the Replicability of Social Science Lab Experiments},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225826},
	urldate = {2023-03-17},
	volume = {14},
	bdsk-url-1 = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225826},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0225826}}

@article{anderson2016,
	abstract = {Gilbert               et al               . conclude that evidence from the Open Science Collaboration's Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.},
	author = {Anderson, Christopher J. and Bahn\'ik, \vSt\vep\'an and Barnett-Cowan, Michael and Bosco, Frank A. and Chandler, Jesse and Chartier, Christopher R. and Cheung, Felix and Christopherson, Cody D. and Cordes, Andreas and Cremata, Edward J. and Della Penna, Nicolas and Estel, Vivien and Fedor, Anna and Fitneva, Stanka A. and Frank, Michael C. and Grange, James A. and Hartshorne, Joshua K. and Hasselman, Fred and Henninger, Felix and van der Hulst, Marije and Jonas, Kai J. and Lai, Calvin K. and Levitan, Carmel A. and Miller, Jeremy K. and Moore, Katherine S. and Meixner, Johannes M. and Munaf{\`o}, Marcus R. and Neijenhuijs, Koen I. and Nilsonne, Gustav and Nosek, Brian A. and Plessow, Franziska and Prenoveau, Jason M. and Ricker, Ashley A. and Schmidt, Kathleen and Spies, Jeffrey R. and Stieger, Stefan and Strohminger, Nina and Sullivan, Gavin B. and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and Vanpaemel, Wolf and Vianello, Michelangelo and Voracek, Martin and Zuni, Kellylynn},
	date = {2016-03-04},
	date-modified = {2023-07-08 17:58:18 +0200},
	doi = {10.1126/science.aad9163},
	file = {/home/vboyce/Zotero/storage/SZGMIEPM/Anderson et al. - 2016 - Response to Comment on ``Estimating the reproducibi.pdf},
	issn = {0036-8075, 1095-9203},
	journaltitle = {Science},
	keywords = {read},
	langid = {english},
	number = {6277},
	options = {useprefix=true},
	pages = {1037--1037},
	shortjournal = {Science},
	title = {Response to {{Comment}} on ``{{Estimating}} the Reproducibility of Psychological Science''},
	url = {https://www.science.org/doi/10.1126/science.aad9163},
	urldate = {2023-04-04},
	volume = {351},
	bdsk-url-1 = {https://www.science.org/doi/10.1126/science.aad9163},
	bdsk-url-2 = {https://doi.org/10.1126/science.aad9163}}

@online{bak-coleman2022,
	abstract = {Replication surveys are becoming a common tool for assessing the knowledge production of scientific disciplines. In psychology, economics, and preclinical cancer biology, replication rates near 50\% have been argued as evidence the disciplines are not reliably producing knowledge, are rife with questionable research practices, and warrant reform. Concerns over failed replications have eroded faith in science, with claims that the vast majority of published research is false. However, these claims are often made under the assumption that effect sizes are fixed and point null hypotheses can be true in practice. Here we derive a theoretical model of the publication process that instead accounts for variation in observed effect sizes. We show that replication rates provide little insight into whether a scientific discipline is reliably and efficiently producing knowledge. In applying our model to data from a large-scale replication survey, we reveal that concerns over the reliability of scientific research may be overstated. Finally, we highlight how proposed reforms may be ineffective at improving replicability and can be detrimental to orthogonal measures of scientific productivity.},
	author = {Bak-Coleman, Joseph and Mann, Richard P. and West, Jevin and Bergstrom, Carl T.},
	date = {2022-04-27T16:11:25},
	doi = {10.31235/osf.io/rkyf7},
	file = {/home/vboyce/Zotero/storage/HHJMQHCD/Bak-Coleman et al. - 2022 - Replication does not reliably measure scientific p.pdf},
	keywords = {read,Social and Behavioral Sciences,Social Statistics},
	langid = {american},
	pubstate = {preprint},
	title = {Replication Does Not Reliably Measure Scientific Productivity},
	url = {https://osf.io/preprints/socarxiv/rkyf7/},
	urldate = {2023-03-09},
	bdsk-url-1 = {https://osf.io/preprints/socarxiv/rkyf7/},
	bdsk-url-2 = {https://doi.org/10.31235/osf.io/rkyf7}}

@article{camerer2016,
	abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	date = {2016-03-25},
	doi = {10.1126/science.aaf0918},
	file = {/home/vboyce/Zotero/storage/EH39UNZJ/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf},
	issn = {0036-8075, 1095-9203},
	journaltitle = {Science},
	keywords = {read},
	langid = {english},
	number = {6280},
	pages = {1433--1436},
	shortjournal = {Science},
	title = {Evaluating Replicability of Laboratory Experiments in Economics},
	url = {https://www.science.org/doi/10.1126/science.aaf0918},
	urldate = {2023-03-17},
	volume = {351},
	bdsk-url-1 = {https://www.science.org/doi/10.1126/science.aaf0918},
	bdsk-url-2 = {https://doi.org/10.1126/science.aaf0918}}

@article{camerer2018,
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	date = {2018-08-27},
	doi = {10.1038/s41562-018-0399-z},
	file = {/home/vboyce/Zotero/storage/DBNTT4GA/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf},
	issn = {2397-3374},
	journaltitle = {Nature Human Behaviour},
	keywords = {read},
	langid = {english},
	number = {9},
	pages = {637--644},
	shortjournal = {Nat Hum Behav},
	title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	urldate = {2023-02-11},
	volume = {2},
	bdsk-url-1 = {https://www.nature.com/articles/s41562-018-0399-z},
	bdsk-url-2 = {https://doi.org/10.1038/s41562-018-0399-z}}

@article{crump2013,
	abstract = {Amazon Mechanical Turk (AMT) is an online crowdsourcing service where anonymous online workers complete web-based tasks for small sums of money. The service has attracted attention from experimental psychologists interested in gathering human subject data more efficiently. However, relative to traditional laboratory studies, many aspects of the testing environment are not under the experimenter's control. In this paper, we attempt to empirically evaluate the fidelity of the AMT system for use in cognitive behavioral experiments. These types of experiment differ from simple surveys in that they require multiple trials, sustained attention from participants, comprehension of complex instructions, and millisecond accuracy for response recording and stimulus presentation. We replicate a diverse body of tasks from experimental psychology including the Stroop, Switching, Flanker, Simon, Posner Cuing, attentional blink, subliminal priming, and category learning tasks using participants recruited using AMT. While most of replications were qualitatively successful and validated the approach of collecting data anonymously online using a web-browser, others revealed disparity between laboratory results and online results. A number of important lessons were encountered in the process of conducting these replications that should be of value to other researchers.},
	author = {Crump, Matthew J. C. and McDonnell, John V. and Gureckis, Todd M.},
	date = {2013-03-13},
	doi = {10.1371/journal.pone.0057410},
	file = {/home/vboyce/Zotero/storage/ZEC2UBQV/Crump et al. - 2013 - Evaluating Amazon's Mechanical Turk as a Tool for .pdf;/home/vboyce/Zotero/storage/PQIINGVU/article.html},
	issn = {1932-6203},
	journaltitle = {PLOS ONE},
	keywords = {Attention,Experimental psychology,Human learning,Internet,Learning,Learning curves,Payment,Reaction time,read},
	langid = {english},
	number = {3},
	pages = {e57410},
	publisher = {{Public Library of Science}},
	shortjournal = {PLOS ONE},
	title = {Evaluating {{Amazon}}'s {{Mechanical Turk}} as a {{Tool}} for {{Experimental Behavioral Research}}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410},
	urldate = {2020-09-30},
	volume = {8},
	bdsk-url-1 = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057410},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0057410}}

@article{devezer2019,
	abstract = {Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists' choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.},
	author = {Devezer, Berna and Nardin, Luis G. and Baumgaertner, Bert and Buzbas, Erkan Ozge},
	date = {2019-05-15},
	doi = {10.1371/journal.pone.0216125},
	file = {/home/vboyce/Zotero/storage/I6T3XBPA/Devezer et al. - 2019 - Scientific discovery in a model-centric framework.pdf},
	issn = {1932-6203},
	journaltitle = {PLOS ONE},
	keywords = {annoying,Markov models,read,Replication studies,Reproducibility,Scientists,Species diversity,Statistical data,Statistical theories,Stochastic processes},
	langid = {english},
	number = {5},
	pages = {e0216125},
	publisher = {{Public Library of Science}},
	shortjournal = {PLOS ONE},
	shorttitle = {Scientific Discovery in a Model-Centric Framework},
	title = {Scientific Discovery in a Model-Centric Framework: {{Reproducibility}}, Innovation, and Epistemic Diversity},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125},
	urldate = {2023-04-24},
	volume = {14},
	bdsk-url-1 = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0216125}}

@article{devezer2021,
	abstract = {Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, most methodological reform attempts suffer from similar mistakes and over-generalizations to the ones they aim to address. We argue that this can be attributed in part to lack of formalism and first principles. Considering the costs of allowing false claims to become canonized, we argue for formal statistical rigor and scientific nuance in methodological reform. To attain this rigor and nuance, we propose a five-step formal approach for solving methodological problems. To illustrate the use and benefits of such formalism, we present a formal statistical analysis of three popular claims in the metascientific literature: (i) that reproducibility is the cornerstone of science; (ii) that data must not be used twice in any analysis; and (iii) that exploratory projects imply poor statistical practice. We show how our formal approach can inform and shape debates about such methodological claims.},
	author = {Devezer, Berna and Navarro, Danielle J. and Vandekerckhove, Joachim and Ozge Buzbas, Erkan},
	date = {2021-03-31},
	doi = {10.1098/rsos.200805},
	file = {/home/vboyce/Zotero/storage/X9M3W72Y/Devezer et al. - 2021 - The case for formal methodology in scientific refo.pdf},
	journaltitle = {Royal Society Open Science},
	keywords = {annoying,double-dipping,exploratory research,read,replication,reproducibility,scientific reform},
	number = {3},
	pages = {200805},
	publisher = {{Royal Society}},
	shortjournal = {R. Soc. Open Sci.},
	title = {The Case for Formal Methodology in Scientific Reform},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.200805},
	urldate = {2023-04-24},
	volume = {8},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/10.1098/rsos.200805},
	bdsk-url-2 = {https://doi.org/10.1098/rsos.200805}}

@article{dreber2015,
	abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants' individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a ``statistically significant'' finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
	author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
	date = {2015-12-15},
	doi = {10.1073/pnas.1516179112},
	file = {/home/vboyce/Zotero/storage/HIYRRZKP/Dreber et al. - 2015 - Using prediction markets to estimate the reproduci.pdf},
	journaltitle = {Proceedings of the National Academy of Sciences},
	keywords = {read},
	number = {50},
	pages = {15343--15347},
	publisher = {{Proceedings of the National Academy of Sciences}},
	shortjournal = {Proc. Natl. Acad. Sci.},
	title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1516179112},
	urldate = {2023-03-17},
	volume = {112},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1516179112},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1516179112}}

@article{ebersole2016,
	abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences---conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
	author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and van Allen, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
	date = {2016-11-01},
	doi = {10.1016/j.jesp.2015.10.012},
	file = {/home/vboyce/Zotero/storage/DRUUKDVH/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf;/home/vboyce/Zotero/storage/44D4H5C4/S0022103115300123.html},
	issn = {0022-1031},
	journaltitle = {Journal of Experimental Social Psychology},
	keywords = {Cognitive psychology,Individual differences,Participant pool,read,Replication,Sampling effects,Situational effects,Social psychology},
	langid = {english},
	options = {useprefix=true},
	pages = {68--82},
	series = {Special {{Issue}}: {{Confirmatory}}},
	shortjournal = {Journal of Experimental Social Psychology},
	shorttitle = {Many {{Labs}} 3},
	title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
	urldate = {2023-02-10},
	volume = {67},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
	bdsk-url-2 = {https://doi.org/10.1016/j.jesp.2015.10.012}}

@article{olsson2020,
	author = {Olsson-Collentine, Anton and van Assen, Marcel ALM and Wicherts, Jelte},
	publisher = {Psychological Bulletin},
	title = {Heterogeneity in direct replications in psychology and its association with effect size},
	year = {2020}}

@article{ebersole2020,
	abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3?9; median total sample = 1,279.5, range = 276?3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (?r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00?.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19?.50).},
	author = {Ebersole, Charles R and Mathur, Maya B and Baranski, Erica and Bart-Plange, Diane-Jo and Buttrick, Nicholas R and Chartier, Christopher R and ... and Nosek, Brian A.},
	date = {2020-09-01},
	date-modified = {2023-07-08 18:00:06 +0200},
	doi = {10.1177/2515245920958687},
	file = {/home/vboyce/Zotero/storage/3UTAEQGU/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf},
	issn = {2515-2459},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	keywords = {read},
	number = {3},
	options = {useprefix=true},
	pages = {309--331},
	publisher = {{SAGE Publications Inc}},
	shortjournal = {Adv. Methods Pract. Psychol. Sci.},
	shorttitle = {Many {{Labs}} 5},
	title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
	url = {https://doi.org/10.1177/2515245920958687},
	urldate = {2023-02-10},
	volume = {3},
	bdsk-url-1 = {https://doi.org/10.1177/2515245920958687}}

@article{errington2021,
	abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary -- the replication was either a success or a failure -- and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	date = {2021-12-07},
	doi = {10.7554/eLife.71601},
	editor = {Pasqualini, Renata and Franco, Eduardo},
	file = {/home/vboyce/Zotero/storage/S78PG95J/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf},
	issn = {2050-084X},
	journaltitle = {eLife},
	keywords = {credibility,meta-analysis,read,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
	pages = {e71601},
	publisher = {{eLife Sciences Publications, Ltd}},
	title = {Investigating the Replicability of Preclinical Cancer Biology},
	url = {https://doi.org/10.7554/eLife.71601},
	urldate = {2022-11-16},
	volume = {10},
	bdsk-url-1 = {https://doi.org/10.7554/eLife.71601}}

@article{etz2016,
	abstract = {We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors---a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis---for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75\% of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor {$<$} 10). The majority of the studies (64\%) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15\%), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10\%), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.},
	author = {Etz, Alexander and Vandekerckhove, Joachim},
	date = {2016-02-26},
	doi = {10.1371/journal.pone.0149794},
	file = {/home/vboyce/Zotero/storage/8L573IDM/Etz and Vandekerckhove - 2016 - A Bayesian Perspective on the Reproducibility Proj.pdf},
	issn = {1932-6203},
	journaltitle = {PLOS ONE},
	keywords = {Analysts,Experimental psychology,Psychology,Publication ethics,read,Replication studies,Reproducibility,Statistical data,Statistical distributions},
	langid = {english},
	number = {2},
	pages = {e0149794},
	publisher = {{Public Library of Science}},
	shortjournal = {PLOS ONE},
	shorttitle = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}},
	title = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}: {{Psychology}}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149794},
	urldate = {2023-03-17},
	volume = {11},
	bdsk-url-1 = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149794},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0149794}}

@article{forsell2019,
	abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p\,{$<$}\,0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
	author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
	date = {2019-12-01},
	doi = {10.1016/j.joep.2018.10.009},
	file = {/home/vboyce/Zotero/storage/EQMDAYS5/Forsell et al. - 2019 - Predicting replication outcomes in the Many Labs 2.pdf;/home/vboyce/Zotero/storage/CYUI5JQH/S0167487018303283.html},
	issn = {0167-4870},
	journaltitle = {Journal of Economic Psychology},
	keywords = {Beliefs,Prediction markets,read,Replications,Reproducibility},
	langid = {english},
	pages = {102117},
	series = {Replications in {{Economic Psychology}} and {{Behavioral Economics}}},
	shortjournal = {Journal of Economic Psychology},
	title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
	url = {https://www.sciencedirect.com/science/article/pii/S0167487018303283},
	urldate = {2023-03-17},
	volume = {75},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167487018303283},
	bdsk-url-2 = {https://doi.org/10.1016/j.joep.2018.10.009}}

@article{frank2012,
	abstract = {Replication is held as the gold standard for ensuring the reliability of published scientific literature. But conducting direct replications is expensive, time-...},
	author = {Frank, Michael C. and Saxe, Rebecca},
	date = {2012-11-07},
	doi = {10.1177/1745691612460686},
	file = {/home/vboyce/Zotero/storage/MYPG288Y/Frank and Saxe - 2012 - Teaching Replication.pdf;/home/vboyce/Zotero/storage/R5PGKLUR/1745691612460686.html},
	ids = {frankTeachingReplication2012a},
	journaltitle = {Perspectives on Psychological Science},
	keywords = {read},
	langid = {english},
	publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
	shortjournal = {Perspect. Psychol. Sci.},
	shorttitle = {Teaching {{Replication}}},
	title = {Teaching {{Replication}}:},
	url = {https://journals.sagepub.com/doi/10.1177/1745691612460686},
	urldate = {2020-09-14},
	bdsk-url-1 = {https://journals.sagepub.com/doi/10.1177/1745691612460686},
	bdsk-url-2 = {https://doi.org/10.1177/1745691612460686}}

@article{gelman2018,
	abstract = {No replication is truly direct, and I recommend moving away from the classification of replications as ``direct'' or ``conceptual'' to a framework in which we accept that treatment effects vary across conditions. Relatedly, we should stop labeling replications as successes or failures and instead use continuous measures to compare different studies, again using meta-analysis of raw data where possible.},
	author = {Gelman, Andrew},
	doi = {10.1017/S0140525X18000638},
	issn = {0140-525X, 1469-1825},
	journaltitle = {Behavioral and Brain Sciences},
	keywords = {read},
	langid = {english},
	pages = {e128},
	publisher = {{Cambridge University Press}},
	shortjournal = {Behav. Brain Sci.},
	title = {Don't Characterize Replications as Successes or Failures},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/dont-characterize-replications-as-successes-or-failures/1AB661F1E7A08E9C5870258878DBA0EA},
	urldate = {2023-04-04},
	volume = {41},
	year = {2018/ed},
	bdsk-url-1 = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/dont-characterize-replications-as-successes-or-failures/1AB661F1E7A08E9C5870258878DBA0EA},
	bdsk-url-2 = {https://doi.org/10.1017/S0140525X18000638}}

@article{gilbert2016,
	abstract = {A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.},
	author = {Gilbert, Daniel T. and King, Gary and Pettigrew, Stephen and Wilson, Timothy D.},
	date = {2016-03-04},
	doi = {10.1126/science.aad7243},
	file = {/home/vboyce/Zotero/storage/232VAWWQ/Gilbert et al. - 2016 - Comment on ``Estimating the reproducibility of psyc.pdf},
	journaltitle = {Science},
	keywords = {read},
	number = {6277},
	pages = {1037--1037},
	publisher = {{American Association for the Advancement of Science}},
	title = {Comment on ``{{Estimating}} the Reproducibility of Psychological Science''},
	url = {https://www.science.org/doi/10.1126/science.aad7243},
	urldate = {2023-03-17},
	volume = {351},
	bdsk-url-1 = {https://www.science.org/doi/10.1126/science.aad7243},
	bdsk-url-2 = {https://doi.org/10.1126/science.aad7243}}

@article{hagger2016,
	abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals (CIs) that encompassed zero (d = 0.04, 95\% CI [?0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
	author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Sch{\"u}tz, A. and Stamos, A. and Tingh{\"o}g, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
	date = {2016-07-01},
	doi = {10.1177/1745691616652873},
	file = {/home/vboyce/Zotero/storage/SYM3U87Q/Hagger et al. - 2016 - A Multilab Preregistered Replication of the Ego-De.pdf},
	issn = {1745-6916},
	journaltitle = {Perspectives on Psychological Science},
	keywords = {read},
	langid = {english},
	number = {4},
	pages = {546--573},
	publisher = {{SAGE Publications Inc}},
	shortjournal = {Perspect Psychol Sci},
	title = {A {{Multilab Preregistered Replication}} of the {{Ego-Depletion Effect}}},
	url = {https://doi.org/10.1177/1745691616652873},
	urldate = {2023-04-27},
	volume = {11},
	bdsk-url-1 = {https://doi.org/10.1177/1745691616652873}}

@article{hawkins,
	abstract = {Replications are important to science, but who will do them? One proposal is that students can conduct replications as part of their training. As a proof-of-concept for this idea, here we report a series of 11 pre-registered replications of findings from the 2015 volume of Psychological Science, all conducted as part of a graduate-level course. Congruent with larger, more systematic efforts, replications typically yielded smaller effects than originals: The modal outcome was partial support for the original claim. This work documents the challenges facing motivated students as they attempt to replicate previously published results on a first attempt. We describe the workflow and pedagogical methods that were used in the class and discuss implications both for the adoption of this pedagogical model and for replication research more broadly.},
	author = {Hawkins, Robert X D and Smith, Eric N and Au, Carolyn and Arias, Juan Miguel and Hermann, Eric and Keil, Martin and Lampinen, Andrew and Raposo, Sarah and Salehi, Shima and Salloum, Justin and Tan, Jed and Frank, Michael C},
	date-modified = {2023-07-08 17:51:13 +0200},
	file = {/home/vboyce/Zotero/storage/R9HV5BKA/Hawkins et al. - Improving the Replicability of Psychological Scien.pdf},
	journal = {Advances in Methods and Practices in Psychological Science},
	keywords = {read},
	langid = {english},
	pages = {41},
	title = {Improving the {{Replicability}} of {{Psychological Science Through Pedagogy}}},
	year = {2018}}

@article{hedges2019,
	abstract = {Formal empirical assessments of replication have recently become more prominent in several areas of science, including psychology. These assessments have used different statistical approaches to determine if a finding has been replicated. The purpose of this article is to provide several alternative conceptual frameworks that lead to different statistical analyses to test hypotheses about replication. All of these analyses are based on statistical methods used in meta-analysis. The differences among the methods described involve whether the burden of proof is placed on replication or nonreplication, whether replication is exact or allows for a small amount of "negligible heterogeneity," and whether the studies observed are assumed to be fixed (constituting the entire body of relevant evidence) or are a sample from a universe of possibly relevant studies. The statistical power of each of these tests is computed and shown to be low in many cases, raising issues of the interpretability of tests for replication. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	date = {2019-10},
	doi = {10.1037/met0000189},
	eprint = {30070547},
	eprinttype = {pmid},
	file = {/home/vboyce/Zotero/storage/Z58642LV/Hedges and Schauer - 2019 - Statistical analyses for studying replication Met.pdf},
	issn = {1939-1463},
	journaltitle = {Psychological Methods},
	keywords = {{Data Interpretation, Statistical},Humans,Meta-Analysis as Topic,{Models, Statistical},Psychology,read,Reproducibility of Results,Research Design},
	langid = {english},
	number = {5},
	pages = {557--570},
	shortjournal = {Psychol Methods},
	shorttitle = {Statistical Analyses for Studying Replication},
	title = {Statistical Analyses for Studying Replication: {{Meta-analytic}} Perspectives},
	volume = {24},
	bdsk-url-1 = {https://doi.org/10.1037/met0000189}}

@report{hoogeveen2019,
	abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social science literature could not be replicated successfully.  Here we assess the extent to which a finding's replication success relates to its intuitive plausibility. Each of 27 high-profile social science findings was evaluated by 233 people without a PhD in psychology. Results showed that these laypeople predicted replication success with above-chance performance (i.e., 58\%). In addition, when laypeople were informed about the strength of evidence from the original studies, this boosted their prediction performance to 67\%. We discuss the prediction patterns and apply signal detection theory to disentangle detection ability from response bias. Our study suggests that laypeople's predictions contain useful information for assessing the probability that a given finding will replicate successfully.},
	author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
	date = {2019-09-25},
	doi = {10.31234/osf.io/egw9d},
	institution = {{PsyArXiv}},
	keywords = {read},
	title = {Laypeople {{Can Predict Which Social Science Studies Replicate}}},
	type = {preprint},
	url = {https://osf.io/egw9d},
	urldate = {2019-09-30},
	bdsk-url-1 = {https://osf.io/egw9d},
	bdsk-url-2 = {https://doi.org/10.31234/osf.io/egw9d}}

@article{isager2020,
	abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, replicating researchers are constrained by resources, and will almost always have to choose one replication effort to focus on from a set of potential candidates. To select a candidate efficiently in these cases, we need methods for deciding which out of all candidates considered would be the most useful to replicate, given some overall goal researchers wish to achieve. In this article we assume that the overall goal researchers wish to achieve is to maximize the utility gained by conducting the replication study. We then propose a general rule for study selection in replication research based on the *replication value* of the set of claims considered for replication. The *replication value* of a claim is defined as the maximum expected utility we could gain by conducting a replication of the claim, and is a function of (1) the value of being certain about the claim, and (2) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using *replication value* as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of *replication value*. Our goal in this article is not to define concrete guidelines for study selection, but to provide the necessary theoretical foundations on which such concrete guidelines could be built.},
	author = {Isager, Peder M. and van Aert, Robbie C. M. and Bahn{\'\i}k, {\v S}t{\v e}p{\'a}n and Brandt, Mark and DeSoto, K. Andrew and Giner-Sorolla, Roger and Krueger, Joachim and Perugini, Marco and Ropovik, Ivan and van 't Veer, Anna and Vranka, Marek A. and Lakens, Daniel},
	date = {2020-09-02T07:00:32},
	doi = {10.31222/osf.io/2gurz},
	file = {/home/vboyce/Zotero/storage/AMASWN9W/Isager et al. - 2020 - Deciding what to replicate A decision model for r.pdf;/home/vboyce/Zotero/storage/NNVA39DJ/Isager et al. - 2020 - Deciding what to replicate A decision model for r.pdf},
	ids = {isagerDecidingWhatReplicate2020a},
	keywords = {Design of Experiments and Sample Surveys,expected utility,Physical Sciences and Mathematics,read,replication,replication value,Statistics and Probability,study selection},
	publisher = {{MetaArXiv}},
	shorttitle = {Deciding What to Replicate},
	title = {Deciding What to Replicate: {{A}} Decision Model for Replication Study Selection under Resource and Knowledge Constraints.},
	url = {https://osf.io/preprints/metaarxiv/2gurz/},
	urldate = {2021-08-24},
	bdsk-url-1 = {https://osf.io/preprints/metaarxiv/2gurz/},
	bdsk-url-2 = {https://doi.org/10.31222/osf.io/2gurz}}

@article{jekel2020,
	abstract = {The Hagen Cumulative Science Project is a large-scale replication project based on students? thesis work. In the project, we aim to (a) teach students to conduct the entire research process for conducting a replication according to open science standards and (b) contribute to cumulative science by increasing the number of direct replications. We describe the procedural steps of the project from choosing suitable replication studies to guiding students through the process of conducting a replication, and processing results in a meta-analysis. Based on the experience of more than 80 replications, we summarize how such a project can be implemented. We present practical solutions that have been shown to be successful as well as discuss typical obstacles and how they can be solved. We argue that replication projects are beneficial for all groups involved: Students benefit by being guided through a highly structured protocol and making actual contributions to science. Instructors benefit by using time resources effectively for cumulative science and fulfilling teaching obligations in a meaningful way. The scientific community benefits from the resulting greater number of replications and teaching state-of-the-art methodology. We encourage the use of student thesis-based replication projects for thesis work in academic bachelor and master curricula.},
	author = {Jekel, Marc and Fiedler, Susann and Allstadt Torras, Ramona and Mischkowski, Dorothee and Dorrough, Angela Rachael and Gl{\"o}ckner, Andreas},
	date = {2020-03-01},
	doi = {10.1177/1475725719868149},
	file = {/home/vboyce/Zotero/storage/LIXYF3KI/Jekel et al. - 2020 - How to Teach Open Science Principles in the Underg.pdf},
	issn = {1475-7257},
	journaltitle = {Psychology Learning \& Teaching},
	keywords = {read},
	number = {1},
	pages = {91--106},
	publisher = {{SAGE Publications}},
	shortjournal = {Psychol. Learn. Teach.},
	title = {How to {{Teach Open Science Principles}} in the {{Undergraduate Curriculum}}---{{The Hagen Cumulative Science Project}}},
	url = {https://doi.org/10.1177/1475725719868149},
	urldate = {2023-04-04},
	volume = {19},
	bdsk-url-1 = {https://doi.org/10.1177/1475725719868149}}

@article{jern2018,
	abstract = {Some have argued that having students conduct rigorous replications of published studies would provide benefits to both psychological science and the students themselves. However, while it seems clear that replications are beneficial to psychological science, there is little empirical evidence that having students conduct replications provides benefits to students. In this study, I conducted a preliminary test (N = 37) of one purported benefit to students of conducting a classroom replication: the development of scientific critical thinking skills. Students completed a 1-term research methods course centered on a class replication project. I assessed students' critical thinking development after completing the course. The results were largely inconclusive, showing no significant change in performance between a pretest (M = 11.00, SD = 3.44) and a posttest (M = 10.30, SD = 2.62), t(36) = −1.21, p = .23. This study highlights the need for additional research on the question of whether having students conduct replications provides educational benefits beyond those offered by other pedagogical methods. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
	author = {Jern, Alan},
	date = {2018},
	doi = {10.1037/stl0000104},
	file = {/home/vboyce/Zotero/storage/BL62BTSS/Jern - 2018 - A preliminary study of the educational benefits of.pdf;/home/vboyce/Zotero/storage/JT5ET6TR/2018-11647-005.html},
	issn = {2332-211X},
	journaltitle = {Scholarship of Teaching and Learning in Psychology},
	keywords = {Classrooms,Critical Thinking,Experimental Replication,Methodology,Psychology Education,read,Undergraduate Education},
	location = {{US}},
	pages = {64--68},
	publisher = {{Educational Publishing Foundation}},
	shortjournal = {Scholarsh. Teach. Learn. Psychol.},
	title = {A Preliminary Study of the Educational Benefits of Conducting Replications in the Classroom},
	volume = {4},
	bdsk-url-1 = {https://doi.org/10.1037/stl0000104}}

@article{klein2014,
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect -- imagined contact reducing prejudice -- showed weak support for replicability. And two effects -- flag priming influencing conservatism and currency priming influencing system justification -- did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn\'ik, \vSt\vep\'an and Bernstein, Michael J.  and ... and Nosek, Brian A.},
	date = {2014-05-01},
	date-modified = {2023-07-08 18:03:09 +0200},
	doi = {10.1027/1864-9335/a000178},
	file = {/home/vboyce/Zotero/storage/5N98UHLQ/Klein et al. - 2014 - Investigating Variation in Replicability A ``Many .pdf},
	issn = {1864-9335, 2151-2590},
	journaltitle = {Social Psychology},
	keywords = {read},
	langid = {english},
	number = {3},
	options = {useprefix=true},
	pages = {142--152},
	shortjournal = {Social Psychology},
	shorttitle = {Investigating {{Variation}} in {{Replicability}}},
	title = {Investigating {{Variation}} in {{Replicability}}: {{A}} ``{{Many Labs}}'' {{Replication Project}}},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
	urldate = {2023-02-10},
	volume = {45},
	year = {2014},
	bdsk-url-1 = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
	bdsk-url-2 = {https://doi.org/10.1027/1864-9335/a000178}}

@article{klein2018,
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen?s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and ... and Nosek, Brian A.},
	date = {2018-12-01},
	date-modified = {2023-07-08 18:02:18 +0200},
	doi = {10.1177/2515245918810225},
	file = {/home/vboyce/Zotero/storage/RI74ZVZ8/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf},
	issn = {2515-2459},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	keywords = {read},
	number = {4},
	options = {useprefix=true},
	pages = {443--490},
	publisher = {{SAGE Publications Inc}},
	shortjournal = {Adv. Methods Pract. Psychol. Sci.},
	shorttitle = {Many {{Labs}} 2},
	title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
	url = {https://doi.org/10.1177/2515245918810225},
	urldate = {2023-02-10},
	volume = {1},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1177/2515245918810225}}

@article{klein2022,
	abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 17 Labs and N = 1,550 participants, after exclusions) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not fully understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
	author = {Klein, Richard A and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Hilgard, Joseph and Ahn, Paul Hangsan and Brady, Abbie J. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret T. and Cromar, Ryan and Gardiner, Gwendolyn and Gosnell, Courtney L. and Grahe, Jon and Hall, Calvin and Howard, Irene and Joy-Gaba, Jennifer A. and Kolb, Miranda and Legg, Angela M. and Levitan, Carmel A. and Mancini, Anthony D. and Manfredi, Dylan and Miller, Jason and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine L. M. and Storage, Daniel and Swanson, Trevor and Van Swol, Lyn M. and Vaughn, Leigh Ann and Vidamuerte, Devere and Wiggins, Brady and Ratliff, Kate A.},
	date = {2022-04-29},
	doi = {10.1525/collabra.35271},
	file = {/home/vboyce/Zotero/storage/SVP6S3SB/Klein et al. - 2022 - Many Labs 4 Failure to Replicate Mortality Salien.pdf;/home/vboyce/Zotero/storage/7KS2AIAS/168050.html},
	issn = {2474-7394},
	journaltitle = {Collabra: Psychology},
	keywords = {read},
	number = {1},
	pages = {35271},
	shortjournal = {Collabra: Psychology},
	shorttitle = {Many {{Labs}} 4},
	title = {Many {{Labs}} 4: {{Failure}} to {{Replicate Mortality Salience Effect With}} and {{Without Original Author Involvement}}},
	url = {https://doi.org/10.1525/collabra.35271},
	urldate = {2023-02-10},
	volume = {8},
	bdsk-url-1 = {https://doi.org/10.1525/collabra.35271}}

@article{lewandowsky2020,
	abstract = {There has been much concern about the ``replication crisis'' in psychology and other disciplines. Here the authors show that an efficient solution to the crisis would not insist on replication before publication, and would instead encourage publication before replication, with the findings marked as preliminary.},
	author = {Lewandowsky, Stephan and Oberauer, Klaus},
	date = {2020-01-17},
	doi = {10.1038/s41467-019-14203-0},
	file = {/home/vboyce/Zotero/storage/GVMLLYIE/Lewandowsky and Oberauer - 2020 - Low replicability can support robust and efficient.pdf;/home/vboyce/Zotero/storage/LWXTQE3H/s41467-019-14203-0.html},
	issn = {2041-1723},
	journaltitle = {Nature Communications},
	keywords = {read},
	langid = {english},
	number = {1},
	pages = {1--12},
	shortjournal = {Nat Commun},
	title = {Low Replicability Can Support Robust and Efficient Science},
	url = {https://www.nature.com/articles/s41467-019-14203-0},
	urldate = {2020-01-21},
	volume = {11},
	bdsk-url-1 = {https://www.nature.com/articles/s41467-019-14203-0},
	bdsk-url-2 = {https://doi.org/10.1038/s41467-019-14203-0}}

@article{mathur2020,
	abstract = {Increasingly, researchers are attempting to replicate published original studies by using large, multisite replication projects, at least 134 of which have been completed or are on going. These designs are promising to assess whether the original study is statistically consistent with the replications and to reassess the strength of evidence for the scientific effect of interest. However, existing analyses generally focus on single replications; when applied to multisite designs, they provide an incomplete view of aggregate evidence and can lead to misleading conclusions about replication success. We propose new statistical metrics representing firstly the probability that the original study's point estimate would be at least as extreme as it actually was, if in fact the original study were statistically consistent with the replications, and secondly the estimated proportion of population effects agreeing in direction with the original study. Generalized versions of the second metric enable consideration of only meaningfully strong population effects that agree in direction, or alternatively that disagree in direction, with the original study. These metrics apply when there are at least 10 replications (unless the heterogeneity estimate τ\^=0, in which case the metrics apply regardless of the number of replications). The first metric assumes normal population effects but appears robust to violations in simulations; the second is distribution free. We provide R packages (Replicate and MetaUtility).},
	author = {Mathur, Maya B. and VanderWeele, Tyler J.},
	date = {2020},
	doi = {10.1111/rssa.12572},
	file = {/home/vboyce/Zotero/storage/Q4YLCSED/Mathur and VanderWeele - 2020 - New statistical metrics for multisite replication .pdf;/home/vboyce/Zotero/storage/Q583USBP/rssa.html},
	issn = {1467-985X},
	journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	keywords = {Effect sizes,Heterogeneity,read,Replication,Reproducibility},
	langid = {english},
	number = {3},
	pages = {1145--1166},
	shortjournal = {J. R. Stat. Soc. Ser. A Stat. Soc.},
	title = {New Statistical Metrics for Multisite Replication Projects},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12572},
	urldate = {2023-04-04},
	volume = {183},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12572},
	bdsk-url-2 = {https://doi.org/10.1111/rssa.12572}}

@article{mcshane2019,
	abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next---an issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching large-scale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous---even if the sign of the effect is consistent---the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
	author = {McShane, Blakeley B. and Tackett, Jennifer L. and B{\"o}ckenholt, Ulf and Gelman, Andrew},
	date = {2019-03-29},
	doi = {10.1080/00031305.2018.1505655},
	file = {/home/vboyce/Zotero/storage/T9294332/McShane et al. - 2019 - Large-Scale Replication Projects in Contemporary P.pdf},
	issn = {0003-1305},
	issue = {sup1},
	journaltitle = {The American Statistician},
	keywords = {Between-study variation,Heterogeneity,Hierarchical,Meta-Analysis,Multilevel,Null hypothesis significance testing,p-value,Psychology,read,Replication},
	pages = {99--105},
	publisher = {{Taylor \& Francis}},
	shortjournal = {Am. Stat.},
	title = {Large-{{Scale Replication Projects}} in {{Contemporary Psychological Research}}},
	url = {https://doi.org/10.1080/00031305.2018.1505655},
	urldate = {2023-02-10},
	volume = {73},
	bdsk-url-1 = {https://doi.org/10.1080/00031305.2018.1505655}}

@article{nosek2020,
	abstract = {To avoid stalemates and provide lessons, replicators and original researchers must reach agreement on a study design and set out expectations ahead of time.},
	annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Publishing, Research data, Research management},
	author = {Nosek, Brian A. and Errington, Timothy M.},
	date = {2020-07},
	doi = {10.1038/d41586-020-02142-6},
	file = {/home/vboyce/Zotero/storage/2WGSH9TU/Nosek and Errington - 2020 - The best time to argue about what a replication me.pdf;/home/vboyce/Zotero/storage/YQF3YWQH/d41586-020-02142-6.html},
	issue = {7817},
	journaltitle = {Nature},
	keywords = {Publishing,read,Research data,Research management},
	langid = {english},
	number = {7817},
	pages = {518--520},
	publisher = {{Nature Publishing Group}},
	shorttitle = {The Best Time to Argue about What a Replication Means?},
	title = {The Best Time to Argue about What a Replication Means? {{Before}} You Do It},
	url = {https://www.nature.com/articles/d41586-020-02142-6},
	urldate = {2023-03-17},
	volume = {583},
	bdsk-url-1 = {https://www.nature.com/articles/d41586-020-02142-6},
	bdsk-url-2 = {https://doi.org/10.1038/d41586-020-02142-6}}

@article{nosek2022,
	abstract = {Replication---an important, uncommon, and misunderstood practice---is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
	author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
	date = {2022-01-04},
	doi = {10.1146/annurev-psych-020821-114157},
	eprint = {34665669},
	eprinttype = {pmid},
	file = {/home/vboyce/Zotero/storage/MCGCYCM7/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf;/home/vboyce/Zotero/storage/V6P5NF42/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf},
	ids = {nosek2022a},
	issn = {0066-4308, 1545-2085},
	journaltitle = {Annual Review of Psychology},
	keywords = {generalizability,metascience,read,replication,reproducibility,research methods,robustness,statistical inference,theory,validity},
	langid = {english},
	number = {1},
	pages = {719--748},
	shortjournal = {Annu. Rev. Psychol.},
	title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157},
	urldate = {2023-02-10},
	volume = {73},
	bdsk-url-1 = {https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157},
	bdsk-url-2 = {https://doi.org/10.1146/annurev-psych-020821-114157}}

@article{odonnell2021,
	abstract = {Empirical audit and review is an approach to assessing the evidentiary value of a research area. It involves identifying a topic and selecting a cross-section of studies for replication. We apply the method to research on the psychological consequences of scarcity. Starting with the papers citing a seminal publication in the field, we conducted replications of 20 studies that evaluate the role of scarcity priming in pain sensitivity, resource allocation, materialism, and many other domains. There was considerable variability in the replicability, with some strong successes and other undeniable failures. Empirical audit and review does not attempt to assign an overall replication rate for a heterogeneous field, but rather facilitates researchers seeking to incorporate strength of evidence as they refine theories and plan new investigations in the research area. This method allows for an integration of qualitative and quantitative approaches to review and enables the growth of a cumulative science.},
	author = {O'Donnell, Michael and Dev, Amelia S. and Antonoplis, Stephen and Baum, Stephen M. and Benedetti, Arianna H. and Brown, N. Derek and Carrillo, Belinda and Choi, Andrew L. and Connor, Paul and Donnelly, Kristin and Ellwood-Lowe, Monica E. and Foushee, Ruthe and Jansen, Rachel and Jarvis, Shoshana N. and Lundell-Creagh, Ryan and Ocampo, Joseph M. and Okafor, Gold N. and Azad, Zahra Rahmani and Rosenblum, Michael and Schatz, Derek and Stein, Daniel H. and Wang, Yilu and Moore, Don A. and Nelson, Leif D.},
	date = {2021-11-02},
	doi = {10.1073/pnas.2103313118},
	file = {/home/vboyce/Zotero/storage/UBHIV97L/O'Donnell et al. - 2021 - Empirical audit and review and an assessment of ev.pdf},
	issn = {0027-8424, 1091-6490},
	journaltitle = {Proceedings of the National Academy of Sciences},
	keywords = {read},
	langid = {english},
	number = {44},
	pages = {e2103313118},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	title = {Empirical Audit and Review and an Assessment of Evidentiary Value in Research on the Psychological Consequences of Scarcity},
	url = {https://pnas.org/doi/10.1073/pnas.2103313118},
	urldate = {2023-04-04},
	volume = {118},
	bdsk-url-1 = {https://pnas.org/doi/10.1073/pnas.2103313118},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2103313118}}

@article{brms,
	author = {Paul-Christian B{\"u}rkner},
	doi = {10.18637/jss.v080.i01},
	encoding = {UTF-8},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--28},
	title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
	volume = {80},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.18637/jss.v080.i01}}

@inproceedings{carvalho09,
	abstract = {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.},
	address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
	editor = {van Dyk, David and Welling, Max},
	month = {16--18 Apr},
	pages = {73--80},
	pdf = {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Handling Sparsity via the Horseshoe},
	url = {https://proceedings.mlr.press/v5/carvalho09a.html},
	volume = {5},
	year = {2009},
	bdsk-url-1 = {https://proceedings.mlr.press/v5/carvalho09a.html}}

@article{openscienceconsortium2015,
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\textbackslash\% of original effect sizes were in the 95\textbackslash\% confidence interval of the replication effect size; 39\textbackslash\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\textbackslash\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	author = {{Open Science Consortium}},
	file = {/home/vboyce/Zotero/storage/372DFPZ2/RPP_SCIENCE_2015.pdf;/home/vboyce/Zotero/storage/8JGRI4V2/science.html},
	journaltitle = {Science},
	keywords = {read},
	title = {Estimating the Reproducibility of Psychological Science},
	url = {https://www.science.org/doi/full/10.1126/science.aac4716?casa_token=IJ35TwwlcjsAAAAA%3AqiP68QbVAHleIg9zD3WugKWuV6Oa5rswS0VQnDsCq5I14ME4WIQabNGVD_T6SBSuAt6voVHNnWc0sw},
	urldate = {2023-03-17},
	year = {2015},
	bdsk-url-1 = {https://www.science.org/doi/full/10.1126/science.aac4716?casa_token=IJ35TwwlcjsAAAAA%3AqiP68QbVAHleIg9zD3WugKWuV6Oa5rswS0VQnDsCq5I14ME4WIQabNGVD_T6SBSuAt6voVHNnWc0sw}}

@article{patil2016,
	abstract = {A recent study of the replicability of key psychological findings is a major contribution toward understanding the human side of the scientific process. Despite the careful and nuanced analysis reported, the simple narrative disseminated by the mass, social, and scientific media was that in only 36\% of the studies were the original results replicated. In the current study, however, we showed that 77\% of the replication effect sizes reported were within a 95\% prediction interval calculated using the original effect size. Our analysis suggests two critical issues in understanding replication of psychological studies. First, researchers' intuitive expectations for what a replication should show do not always match with statistical estimates of replication. Second, when the results of original studies are very imprecise, they create wide prediction intervals-and a broad range of replication effects that are consistent with the original estimates. This may lead to effects that replicate successfully, in that replication results are consistent with statistical expectations, but do not provide much information about the size (or existence) of the true effect. In this light, the results of the Reproducibility Project: Psychology can be viewed as statistically consistent with what one might expect when performing a large-scale replication experiment.},
	author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
	date = {2016-07},
	doi = {10.1177/1745691616646366},
	eprint = {27474140},
	eprinttype = {pmid},
	file = {/home/vboyce/Zotero/storage/4I4NTQ52/Patil et al. - 2016 - What Should Researchers Expect When They Replicate.pdf},
	issn = {1745-6924},
	journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
	keywords = {Behavioral Research,{Data Interpretation, Statistical},Humans,p values,prediction intervals,Psychology,read,replication,reproducibility,Reproducibility of Results,Reproducibility Project: Psychology},
	langid = {english},
	number = {4},
	pages = {539--544},
	pmcid = {PMC4968573},
	shortjournal = {Perspect Psychol Sci},
	shorttitle = {What {{Should Researchers Expect When They Replicate Studies}}?},
	title = {What {{Should Researchers Expect When They Replicate Studies}}? {{A Statistical View}} of {{Replicability}} in {{Psychological Science}}},
	volume = {11},
	bdsk-url-1 = {https://doi.org/10.1177/1745691616646366}}

@article{pawel2020,
	abstract = {Throughout the last decade, the so-called replication crisis has stimulated many researchers to conduct large-scale replication projects. With data from four of these projects, we computed probabilistic forecasts of the replication outcomes, which we then evaluated regarding discrimination, calibration and sharpness. A novel model, which can take into account both inflation and heterogeneity of effects, was used and predicted the effect estimate of the replication study with good performance in two of the four data sets. In the other two data sets, predictive performance was still substantially improved compared to the naive model which does not consider inflation and heterogeneity of effects. The results suggest that many of the estimates from the original studies were inflated, possibly caused by publication bias or questionable research practices, and also that some degree of heterogeneity between original and replication effects should be expected. Moreover, the results indicate that the use of statistical significance as the only criterion for replication success may be questionable, since from a predictive viewpoint, non-significant replication results are often compatible with significant results from the original study. The developed statistical methods as well as the data sets are available in the R package ReplicationSuccess.},
	author = {Pawel, Samuel and Held, Leonhard},
	date = {2020-04-22},
	doi = {10.1371/journal.pone.0231416},
	file = {/home/vboyce/Zotero/storage/P6HK7WJE/Pawel and Held - 2020 - Probabilistic forecasting of replication studies.pdf},
	issn = {1932-6203},
	journaltitle = {PLOS ONE},
	keywords = {Economics,Experimental economics,Experimental psychology,Forecasting,read,Replication studies,Social psychology,Social sciences,Test statistics},
	langid = {english},
	number = {4},
	pages = {e0231416},
	publisher = {{Public Library of Science}},
	shortjournal = {PLOS ONE},
	title = {Probabilistic Forecasting of Replication Studies},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231416},
	urldate = {2023-03-17},
	volume = {15},
	bdsk-url-1 = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231416},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0231416}}

@article{pownall2021,
	abstract = {Recently, there has been a growing emphasis on embedding open and reproducible approaches into research. One essential step in accomplishing this larger goal is to embed such practices into undergraduate and postgraduate research training. However, this often requires substantial time and resources to implement. Also, while many pedagogical resources are regularly developed for this purpose, they are not often openly and actively shared with the wider community. The creation and public sharing of open educational resources is useful for educators who wish to embed open scholarship and reproducibility into their teaching and learning. In this article, we describe and openly share a bank of teaching resources and lesson plans on the broad topics of open scholarship, open science, replication, and reproducibility that can be integrated into taught courses to support educators and instructors. These resources were created as part of the Society for the Improvement of Psychological Science (SIPS) hackathon at the 2021 Annual Conference, and we detail this collaborative process in the article. By sharing these open pedagogical resources, we aim to reduce the labor required to develop and implement open scholarship content to further the open scholarship and open educational materials movement. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	author = {Pownall, Madeleine and Azevedo, Flavio and Aldoh, Alaa and Elsherif, Mahmoud and Vasilev, Martin and Pennington, Charlotte R. and Robertson, Olly and Tromp, Myrthe Vel and Liu, Meng and Makel, Matthew C. and Tonge, Natasha and Moreau, David and Horry, Ruth and Shaw, John and Tzavella, Loukia and McGarrigle, Ronan and Talbot, Catherine and Parsons, Sam},
	date = {2021},
	doi = {10.1037/stl0000307},
	file = {/home/vboyce/Zotero/storage/2TMKCJZA/Pownall et al. - 2021 - Embedding open and reproducible science into teach.pdf;/home/vboyce/Zotero/storage/3AGXGDH7/2022-15361-001.html},
	issn = {2332-211X},
	journaltitle = {Scholarship of Teaching and Learning in Psychology},
	keywords = {College Students,Lesson Plans,Nontraditional Education,Open Data,Open Science,read,School Learning,Sciences,Teachers,Teaching,Teaching Methods,Training},
	location = {{US}},
	pages = {No Pagination Specified-No Pagination Specified},
	publisher = {{Educational Publishing Foundation}},
	shortjournal = {Scholarsh. Teach. Learn. Psychol.},
	shorttitle = {Embedding Open and Reproducible Science into Teaching},
	title = {Embedding Open and Reproducible Science into Teaching: {{A}} Bank of Lesson Plans and Resources},
	bdsk-url-1 = {https://doi.org/10.1037/stl0000307}}

@report{protzko2020,
	abstract = {Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigor-enhancing practices: confirmatory tests, large sample sizes, preregistration, and methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50\%, replication attempts here produced the expected effects with significance testing (p\&lt;.05) in 86\% of attempts, slightly exceeding maximum expected replicability based on observed effect sizes and sample sizes. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97\% that of the original study. This high replication rate justifies confidence in rigor enhancing methods to increase the replicability of new discoveries.},
	author = {Protzko, John and Krosnick, Jon and Nelson, Leif D. and Nosek, Brian A. and Axt, Jordan and Berent, Matthew and Buttrick, Nick and DeBell, Matthew and Ebersole, Charles R. and Lundmark, Sebastian and MacInnis, Bo and O'Donnell, Michael and Perfecto, Hannah and Pustejovsky, James E and Roeder, Scott S. and Walleczek, Jan and Schooler, Jonathan},
	date = {2020-09-10},
	doi = {10.31234/osf.io/n2a9x},
	file = {/home/vboyce/Zotero/storage/HZ2FV597/Protzko et al. - 2020 - High Replicability of Newly-Discovered Social-beha.pdf},
	institution = {{PsyArXiv}},
	keywords = {read},
	langid = {english},
	title = {High {{Replicability}} of {{Newly-Discovered Social-behavioral Findings}} Is {{Achievable}}},
	type = {preprint},
	url = {https://osf.io/n2a9x},
	urldate = {2023-04-05},
	bdsk-url-1 = {https://osf.io/n2a9x},
	bdsk-url-2 = {https://doi.org/10.31234/osf.io/n2a9x}}

@article{quintana2021,
	abstract = {Requiring undergraduate students to perform what is termed original research for their thesis, an investigation that cannot constitute a replication of an existing study, is a failed opportunity for science and education, argues Daniel Quintana.},
	author = {Quintana, Daniel S.},
	date = {2021-09},
	doi = {10.1038/s41562-021-01192-8},
	file = {/home/vboyce/Zotero/storage/AIIH4N6M/Quintana - 2021 - Replication studies for undergraduate theses to im.pdf},
	issn = {2397-3374},
	issue = {9},
	journaltitle = {Nature Human Behaviour},
	keywords = {Behavioral Sciences,Experimental Psychology,general,Life Sciences,Microeconomics,Neurosciences,Personality and Social Psychology,read},
	langid = {english},
	number = {9},
	pages = {1117--1118},
	publisher = {{Nature Publishing Group}},
	shortjournal = {Nat Hum Behav},
	title = {Replication Studies for Undergraduate Theses to Improve Science and Education},
	url = {https://www.nature.com/articles/s41562-021-01192-8},
	urldate = {2023-04-04},
	volume = {5},
	bdsk-url-1 = {https://www.nature.com/articles/s41562-021-01192-8},
	bdsk-url-2 = {https://doi.org/10.1038/s41562-021-01192-8}}

@unpublished{ramscar2015,
	author = {Ramscar, Michael and Shaoul, Cyrus and Baayen, R Harald},
	date-modified = {2023-07-08 16:32:12 +0200},
	file = {/home/vboyce/Zotero/storage/VAL7VCFH/Ramscar et al. - Why many priming results don't (and won't) replica.pdf},
	langid = {english},
	title = {Why Many Priming Results Don't (and Won't) Replicate: {{A}} Quantitative Analysis},
	year = {2015}}

@article{schmidt2009,
	abstract = {Replication is one of the most important tools for the verification of facts within the empirical sciences. A detailed examination of the notion of replication reveals that there are many different meanings to this concept and the relevant procedures, but hardly any systematic literature. This paper analyzes the concept of replication from a theoretical point of view. It demonstrates that the theoretical demands are scarcely met in everyday work within the social sciences. Some demands are just not feasible, whereas others are constricted by restrictions relating to publication. A new classification scheme based on a functional approach that distinguishes between different types of replication is proposed. Next, it will be argued that replication addresses the important connection between existing and new knowledge. To do so it has to be applied explicitly and systematically. The paper ends with a description of procedures how this could be done and a set of recommendations how to handle the concept of replication in the future to exploit its potential to the full.},
	author = {Schmidt, Stefan},
	date = {2009-06-01},
	doi = {10.1037/a0015108},
	file = {/home/vboyce/Zotero/storage/26KGZ44C/Schmidt - 2009 - Shall we Really do it Again The Powerful Concept .pdf},
	issn = {1089-2680},
	journaltitle = {Review of General Psychology},
	keywords = {read},
	langid = {english},
	number = {2},
	pages = {90--100},
	publisher = {{SAGE Publications Inc}},
	shortjournal = {Rev. Gen. Psychol.},
	shorttitle = {Shall We {{Really}} Do It {{Again}}?},
	title = {Shall We {{Really}} Do It {{Again}}? {{The Powerful Concept}} of {{Replication}} Is {{Neglected}} in the {{Social Sciences}}},
	url = {https://doi.org/10.1037/a0015108},
	urldate = {2023-04-04},
	volume = {13},
	bdsk-url-1 = {https://doi.org/10.1037/a0015108}}

@article{simonsohn2015,
	abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ?unsuccessful? replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ?protecting? true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
	author = {Simonsohn, Uri},
	date = {2015-05-01},
	doi = {10.1177/0956797614567341},
	file = {/home/vboyce/Zotero/storage/ARIPXHJ6/Simonsohn - 2015 - Small Telescopes Detectability and the Evaluation.pdf},
	issn = {0956-7976},
	journaltitle = {Psychological Science},
	keywords = {read},
	langid = {english},
	number = {5},
	pages = {559--569},
	publisher = {{SAGE Publications Inc}},
	shortjournal = {Psychol Sci},
	shorttitle = {Small {{Telescopes}}},
	title = {Small {{Telescopes}}: {{Detectability}} and the {{Evaluation}} of {{Replication Results}}},
	url = {https://doi.org/10.1177/0956797614567341},
	urldate = {2023-04-04},
	volume = {26},
	bdsk-url-1 = {https://doi.org/10.1177/0956797614567341}}

@article{vanbavel2016,
	abstract = {In recent years, scientists have paid increasing attention to reproducibility. For example, the Reproducibility Project, a large-scale replication attempt of 100 studies published in top psychology journals found that only 39\% could be unambiguously reproduced. There is a growing consensus among scientists that the lack of reproducibility in psychology and other fields stems from various methodological factors, including low statistical power, researcher's degrees of freedom, and an emphasis on publishing surprising positive results. However, there is a contentious debate about the extent to which failures to reproduce certain results might also reflect contextual differences (often termed ``hidden moderators'') between the original research and the replication attempt. Although psychologists have found extensive evidence that contextual factors alter behavior, some have argued that context is unlikely to influence the results of direct replications precisely because these studies use the same methods as those used in the original research. To help resolve this debate, we recoded the 100 original studies from the Reproducibility Project on the extent to which the research topic of each study was contextually sensitive. Results suggested that the contextual sensitivity of the research topic was associated with replication success, even after statistically adjusting for several methodological characteristics (e.g., statistical power, effect size). The association between contextual sensitivity and replication success did not differ across psychological subdisciplines. These results suggest that researchers, replicators, and consumers should be mindful of contextual factors that might influence a psychological process. We offer several guidelines for dealing with contextual sensitivity in reproducibility.},
	author = {Van Bavel, Jay J. and Mende-Siedlecki, Peter and Brady, William J. and Reinero, Diego A.},
	date = {2016-06-07},
	doi = {10.1073/pnas.1521897113},
	file = {/home/vboyce/Zotero/storage/3UHVUHMG/Van Bavel et al. - 2016 - Contextual sensitivity in scientific reproducibili.pdf},
	journaltitle = {Proceedings of the National Academy of Sciences},
	keywords = {read},
	number = {23},
	pages = {6454--6459},
	publisher = {{Proceedings of the National Academy of Sciences}},
	shortjournal = {Proc. Natl. Acad. Sci.},
	title = {Contextual Sensitivity in Scientific Reproducibility},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1521897113},
	urldate = {2023-04-05},
	volume = {113},
	bdsk-url-1 = {https://www.pnas.org/doi/full/10.1073/pnas.1521897113},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1521897113}}

@article{wagge2019,
	author = {Wagge, Jordan R. and Brandt, Mark J. and Lazarevic, Ljiljana B. and Legate, Nicole and Christopherson, Cody and Wiggins, Brady and Grahe, Jon E.},
	date = {2019},
	file = {/home/vboyce/Zotero/storage/K5DMQS64/Wagge et al. - 2019 - Publishing Research With Undergraduate Students vi.pdf},
	issn = {1664-1078},
	journaltitle = {Frontiers in Psychology},
	keywords = {read},
	shortjournal = {Front. Psychol.},
	shorttitle = {Publishing {{Research With Undergraduate Students}} via {{Replication Work}}},
	title = {Publishing {{Research With Undergraduate Students}} via {{Replication Work}}: {{The Collaborative Replications}} and {{Education Project}}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00247},
	urldate = {2023-04-04},
	volume = {10},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00247}}

@article{wilson2020,
	abstract = {The perceived replication crisis and the reforms designed to address it are grounded in the notion that science is a binary signal detection problem. However, contrary to null hypothesis significance testing (NHST) logic, the magnitude of the underlying effect size for a given experiment is best conceptualized as a random draw from a continuous distribution, not as a random draw from a dichotomous distribution (null vs. alternative). Moreover, because continuously distributed effects selected using a               P               {$<$} 0.05 filter must be inflated, the fact that they are smaller when replicated (reflecting regression to the mean) is no reason to sound the alarm. Considered from this perspective, recent replication efforts suggest that most published               P               {$<$} 0.05 scientific findings are ``true'' (i.e., in the correct direction), with observed effect sizes that are inflated to varying degrees. We propose that original science is a screening process, one that adopts NHST logic as a useful fiction for selecting true effects that are potentially large enough to be of interest to other scientists. Unlike original science, replication science seeks to precisely measure the underlying effect size associated with an experimental protocol via large-               N               direct replication, without regard for statistical significance. Registered reports are well suited to (often resource-intensive) direct replications, which should focus on influential findings and be published regardless of outcome. Conceptual replications play an important but separate role in validating theories. However, because they are part of NHST-based original science, conceptual replications cannot serve as the field's self-correction mechanism. Only direct replications can do that.},
	author = {Wilson, Brent M. and Harris, Christine R. and Wixted, John T.},
	date = {2020-03-17},
	doi = {10.1073/pnas.1914237117},
	file = {/home/vboyce/Zotero/storage/J47QEVYF/Wilson et al. - 2020 - Science is not a signal detection problem.pdf},
	issn = {0027-8424, 1091-6490},
	journaltitle = {Proceedings of the National Academy of Sciences},
	keywords = {read},
	langid = {english},
	number = {11},
	pages = {5559--5567},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	title = {Science Is Not a Signal Detection Problem},
	url = {https://pnas.org/doi/full/10.1073/pnas.1914237117},
	urldate = {2023-04-05},
	volume = {117},
	bdsk-url-1 = {https://pnas.org/doi/full/10.1073/pnas.1914237117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1914237117}}

@article{yang2020,
	abstract = {Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literature as quickly as replicating papers. This dynamic weakens the literature, raises research costs, and demonstrates the need for new approaches for estimating a study's replicability. Here, we trained an artificial intelligence model to estimate a paper's replicability using ground truth data on studies that had passed or failed manual replication tests, and then tested the model's generalizability on an extensive set of out-of-sample studies. The model predicts replicability better than the base rate of reviewers and comparably as well as prediction markets, the best present-day method for predicting replicability. In out-of-sample tests on manually replicated papers from diverse disciplines and methods, the model had strong accuracy levels of 0.65 to 0.78. Exploring the reasons behind the model's predictions, we found no evidence for bias based on topics, journals, disciplines, base rates of failure, persuasion words, or novelty words like ``remarkable'' or ``unexpected.'' We did find that the model's accuracy is higher when trained on a paper's text rather than its reported statistics and that n-grams, higher order word combinations that humans have difficulty processing, correlate with replication. We discuss how combining human and machine intelligence can raise confidence in research, provide research self-assessment techniques, and create methods that are scalable and efficient enough to review the ever-growing numbers of publications---a task that entails extensive human resources to accomplish with prediction markets and manual replication alone.},
	author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
	date = {2020-05-19},
	doi = {10.1073/pnas.1909046117},
	file = {/home/vboyce/Zotero/storage/8C637NTJ/Yang et al. - 2020 - Estimating the deep replicability of scientific fi.pdf},
	journaltitle = {Proceedings of the National Academy of Sciences},
	keywords = {read},
	number = {20},
	pages = {10762--10768},
	publisher = {{Proceedings of the National Academy of Sciences}},
	shortjournal = {Proc. Natl. Acad. Sci.},
	title = {Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence},
	url = {https://www.pnas.org/doi/10.1073/pnas.1909046117},
	urldate = {2023-05-15},
	volume = {117},
	bdsk-url-1 = {https://www.pnas.org/doi/10.1073/pnas.1909046117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1909046117}}

@article{youyou2023,
	abstract = {Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper's likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors' cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author's university prestige and a paper's citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability.},
	author = {Youyou, Wu and Yang, Yang and Uzzi, Brian},
	date = {2023-02-07},
	doi = {10.1073/pnas.2208863120},
	file = {/home/vboyce/Zotero/storage/CZQBAK2A/Youyou et al. - 2023 - A discipline-wide investigation of the replicabili.pdf;/home/vboyce/Zotero/storage/S6I6C8U9/Youyou et al. - 2023 - A discipline-wide investigation of the replicabili.pdf},
	ids = {youyou2023a},
	journaltitle = {Proceedings of the National Academy of Sciences},
	keywords = {jesus fucking christ the awfulness,read},
	number = {6},
	pages = {e2208863120},
	publisher = {{Proceedings of the National Academy of Sciences}},
	shortjournal = {Proc. Natl. Acad. Sci.},
	title = {A Discipline-Wide Investigation of the Replicability of {{Psychology}} Papers over the Past Two Decades},
	url = {https://www.pnas.org/doi/10.1073/pnas.2208863120},
	urldate = {2023-01-31},
	volume = {120},
	bdsk-url-1 = {https://www.pnas.org/doi/10.1073/pnas.2208863120},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2208863120}}

@online{zotero-2604,
	file = {/home/vboyce/Zotero/storage/ZUASFZZM/pnas.html},
	keywords = {read},
	title = {Empirical Audit and Review and an Assessment of Evidentiary Value in Research on the Psychological Consequences of Scarcity | {{PNAS}}},
	url = {https://www.pnas.org/doi/10.1073/pnas.2103313118#sec-2},
	urldate = {2022-12-01},
	bdsk-url-1 = {https://www.pnas.org/doi/10.1073/pnas.2103313118#sec-2}}

@online{zotero-3286,
	file = {/home/vboyce/Zotero/storage/TEX8XS9Y/New Statistical Metrics for Multisite Replication .pdf;/home/vboyce/Zotero/storage/B9WN5HW4/7056432.html},
	keywords = {read},
	title = {New {{Statistical Metrics}} for {{Multisite Replication Projects}} | {{Journal}} of the {{Royal Statistical Society Series A}}: {{Statistics}} in {{Society}} | {{Oxford Academic}}},
	url = {https://academic.oup.com/jrsssa/article/183/3/1145/7056432},
	urldate = {2023-05-12},
	bdsk-url-1 = {https://academic.oup.com/jrsssa/article/183/3/1145/7056432}}
