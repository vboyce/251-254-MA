@article{altmejd2019,
  title = {Predicting the Replicability of Social Science Lab Experiments},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  date = {2019-12-05},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {12},
  pages = {e0225826},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0225826},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225826},
  urldate = {2023-03-17},
  abstract = {We measure how accurately replication of experimental results can be predicted by black-box statistical models. With data from four large-scale replication projects in experimental psychology and economics, and techniques from machine learning, we train predictive models and study which variables drive predictable replication. The models predicts binary replication with a cross-validated accuracy rate of 70\% (AUC of 0.77) and estimates of relative effect sizes with a Spearman ρ of 0.38. The accuracy level is similar to market-aggregated beliefs of peer scientists [1, 2]. The predictive power is validated in a pre-registered out of sample test of the outcome of [3], where 71\% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to ρ = 0.25. Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two-variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing the process of evaluation of new findings and guiding resources to those direct replications that are likely to be most informative.},
  langid = {english},
  keywords = {Experimental economics,Experimental psychology,Forecasting,Machine learning,Machine learning algorithms,Replication studies,Scientists,Trees},
  file = {/home/vboyce/Zotero/storage/98PWUT3J/Altmejd et al. - 2019 - Predicting the replicability of social science lab.pdf}
}

@misc{bak-coleman2022,
  title = {Replication Does Not Reliably Measure Scientific Productivity},
  author = {Bak-Coleman, Joseph and Mann, Richard P. and West, Jevin and Bergstrom, Carl T.},
  date = {2022-04-27T16:11:25},
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/rkyf7},
  url = {https://osf.io/preprints/socarxiv/rkyf7/},
  urldate = {2023-03-09},
  abstract = {Replication surveys are becoming a common tool for assessing the knowledge production of scientific disciplines. In psychology, economics, and preclinical cancer biology, replication rates near 50\% have been argued as evidence the disciplines are not reliably producing knowledge, are rife with questionable research practices, and warrant reform. Concerns over failed replications have eroded faith in science, with claims that the vast majority of published research is false. However, these claims are often made under the assumption that effect sizes are fixed and point null hypotheses can be true in practice. Here we derive a theoretical model of the publication process that instead accounts for variation in observed effect sizes. We show that replication rates provide little insight into whether a scientific discipline is reliably and efficiently producing knowledge. In applying our model to data from a large-scale replication survey, we reveal that concerns over the reliability of scientific research may be overstated. Finally, we highlight how proposed reforms may be ineffective at improving replicability and can be detrimental to orthogonal measures of scientific productivity.},
  langid = {american},
  keywords = {read,Social and Behavioral Sciences,Social Statistics},
  file = {/home/vboyce/Zotero/storage/HHJMQHCD/Bak-Coleman et al. - 2022 - Replication does not reliably measure scientific p.pdf}
}

@article{camerer2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  date = {2016-03-25},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0918},
  url = {https://www.science.org/doi/10.1126/science.aaf0918},
  urldate = {2023-03-17},
  abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/EH39UNZJ/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf}
}

@article{camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  date = {2018-08-27},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {2},
  number = {9},
  pages = {637--644},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  url = {https://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2023-02-11},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/DBNTT4GA/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{dreber2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  date = {2015-12-15},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci.},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1516179112},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1516179112},
  urldate = {2023-03-17},
  abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants’ individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a “statistically significant” finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
  file = {/home/vboyce/Zotero/storage/HIYRRZKP/Dreber et al. - 2015 - Using prediction markets to estimate the reproduci.pdf}
}

@article{ebersole2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and van Allen, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2016-11-01},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
  urldate = {2023-02-10},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  keywords = {Cognitive psychology,Individual differences,Participant pool,read,Replication,Sampling effects,Situational effects,Social psychology},
  file = {/home/vboyce/Zotero/storage/DRUUKDVH/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf;/home/vboyce/Zotero/storage/44D4H5C4/S0022103115300123.html}
}

@article{ebersole2020,
  title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and Bart-Plange, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarević, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Baník, Gabriel and Baskin, Ernest and Belopavlović, Radomir and Bernstein, Michael H. and Białek, Michał and Bloxsom, Nicholas G. and Bodroža, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Brühlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and Čolić, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falcão, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, Máire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Hałasa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and Hüffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and Jiménez-Leal, William and Johannesson, Magnus and Joy-Gaba, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Kołodziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and de Lima, Tiago Jessé Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, Łukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafał and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orlić, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedović, Ivana and Pękala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrović, Boban and Pfeiffer, Thomas and Pieńkosz, Damian and Preti, Emanuele and Purić, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemysław and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and Schulz-Hardt, Stefan and Schütz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, Rúben and Sioma, Barbara and Skorb, Lauren and de Souza, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilović, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Szöke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and Žeželj, Iris and Zrubka, Mark and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2020-09-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Adv. Methods Pract. Psychol. Sci.},
  volume = {3},
  number = {3},
  pages = {309--331},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920958687},
  url = {https://doi.org/10.1177/2515245920958687},
  urldate = {2023-02-10},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3?9; median total sample = 1,279.5, range = 276?3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (?r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00?.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19?.50).},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/3UTAEQGU/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf}
}

@article{errington2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  date = {2021-12-07},
  journaltitle = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  url = {https://doi.org/10.7554/eLife.71601},
  urldate = {2022-11-16},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,read,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/home/vboyce/Zotero/storage/S78PG95J/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{etz2016,
  title = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}: {{Psychology}}},
  shorttitle = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  date = {2016-02-26},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {11},
  number = {2},
  pages = {e0149794},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0149794},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149794},
  urldate = {2023-03-17},
  abstract = {We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75\% of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor {$<$} 10). The majority of the studies (64\%) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15\%), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10\%), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.},
  langid = {english},
  keywords = {Analysts,Experimental psychology,Psychology,Publication ethics,Replication studies,Reproducibility,Statistical data,Statistical distributions},
  file = {/home/vboyce/Zotero/storage/8L573IDM/Etz and Vandekerckhove - 2016 - A Bayesian Perspective on the Reproducibility Proj.pdf}
}

@article{forsell2019,
  title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
  author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
  date = {2019-12-01},
  journaltitle = {Journal of Economic Psychology},
  shortjournal = {Journal of Economic Psychology},
  series = {Replications in {{Economic Psychology}} and {{Behavioral Economics}}},
  volume = {75},
  pages = {102117},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2018.10.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0167487018303283},
  urldate = {2023-03-17},
  abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p\,{$<$}\,0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
  langid = {english},
  keywords = {Beliefs,Prediction markets,Replications,Reproducibility},
  file = {/home/vboyce/Zotero/storage/EQMDAYS5/Forsell et al. - 2019 - Predicting replication outcomes in the Many Labs 2.pdf;/home/vboyce/Zotero/storage/CYUI5JQH/S0167487018303283.html}
}

@article{frank2012,
  ids = {frankTeachingReplication2012a},
  title = {Teaching {{Replication}}:},
  shorttitle = {Teaching {{Replication}}},
  author = {Frank, Michael C. and Saxe, Rebecca},
  date = {2012-11-07},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect. Psychol. Sci.},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/1745691612460686},
  url = {https://journals.sagepub.com/doi/10.1177/1745691612460686},
  urldate = {2020-09-14},
  abstract = {Replication is held as the gold standard for ensuring the reliability of published scientific literature. But conducting direct replications is expensive, time-...},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/MYPG288Y/Frank and Saxe - 2012 - Teaching Replication.pdf;/home/vboyce/Zotero/storage/R5PGKLUR/1745691612460686.html}
}

@article{gilbert2016,
  title = {Comment on “{{Estimating}} the Reproducibility of Psychological Science”},
  author = {Gilbert, Daniel T. and King, Gary and Pettigrew, Stephen and Wilson, Timothy D.},
  date = {2016-03-04},
  journaltitle = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aad7243},
  url = {https://www.science.org/doi/10.1126/science.aad7243},
  urldate = {2023-03-17},
  abstract = {A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.},
  file = {/home/vboyce/Zotero/storage/232VAWWQ/Gilbert et al. - 2016 - Comment on “Estimating the reproducibility of psyc.pdf}
}

@article{hawkins,
  title = {Improving the {{Replicability}} of {{Psychological Science Through Pedagogy}}},
  author = {Hawkins, Robert X D and Smith, Eric N and Au, Carolyn and Arias, Juan Miguel and Hermann, Eric and Keil, Martin and Lampinen, Andrew and Raposo, Sarah and Salehi, Shima and Salloum, Justin and Tan, Jed and Frank, Michael C},
  pages = {41},
  abstract = {Replications are important to science, but who will do them? One proposal is that students can conduct replications as part of their training. As a proof-of-concept for this idea, here we report a series of 11 pre-registered replications of findings from the 2015 volume of Psychological Science, all conducted as part of a graduate-level course. Congruent with larger, more systematic efforts, replications typically yielded smaller effects than originals: The modal outcome was partial support for the original claim. This work documents the challenges facing motivated students as they attempt to replicate previously published results on a first attempt. We describe the workflow and pedagogical methods that were used in the class and discuss implications both for the adoption of this pedagogical model and for replication research more broadly.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/R9HV5BKA/Hawkins et al. - Improving the Replicability of Psychological Scien.pdf}
}

@report{hoogeveen2019,
  type = {preprint},
  title = {Laypeople {{Can Predict Which Social Science Studies Replicate}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  date = {2019-09-25},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/egw9d},
  url = {https://osf.io/egw9d},
  urldate = {2019-09-30},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social science literature could not be replicated successfully.  Here we assess the extent to which a finding’s replication success relates to its intuitive plausibility. Each of 27 high-profile social science findings was evaluated by 233 people without a PhD in psychology. Results showed that these laypeople predicted replication success with above-chance performance (i.e., 58\%). In addition, when laypeople were informed about the strength of evidence from the original studies, this boosted their prediction performance to 67\%. We discuss the prediction patterns and apply signal detection theory to disentangle detection ability from response bias. Our study suggests that laypeople’s predictions contain useful information for assessing the probability that a given finding will replicate successfully.},
  keywords = {read}
}

@article{klein2014,
  title = {Investigating {{Variation}} in {{Replicability}}: {{A}} “{{Many Labs}}” {{Replication Project}}},
  shorttitle = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‘t Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2014-05-01},
  journaltitle = {Social Psychology},
  shortjournal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000178},
  url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
  urldate = {2023-02-10},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/5N98UHLQ/Klein et al. - 2014 - Investigating Variation in Replicability A “Many .pdf}
}

@article{klein2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Lee Nichols, Austin and Ocampo, Aaron and O’Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ’t Veer, Anna Elisabeth and Vásquez- Echeverría, Alejandro and Ann Vaughn, Leigh and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  options = {useprefix=true},
  date = {2018-12-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Adv. Methods Pract. Psychol. Sci.},
  volume = {1},
  number = {4},
  pages = {443--490},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  url = {https://doi.org/10.1177/2515245918810225},
  urldate = {2023-02-10},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen?s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/RI74ZVZ8/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@article{klein2022,
  title = {Many {{Labs}} 4: {{Failure}} to {{Replicate Mortality Salience Effect With}} and {{Without Original Author Involvement}}},
  shorttitle = {Many {{Labs}} 4},
  author = {Klein, Richard A and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Hilgard, Joseph and Ahn, Paul Hangsan and Brady, Abbie J. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret T. and Cromar, Ryan and Gardiner, Gwendolyn and Gosnell, Courtney L. and Grahe, Jon and Hall, Calvin and Howard, Irene and Joy-Gaba, Jennifer A. and Kolb, Miranda and Legg, Angela M. and Levitan, Carmel A. and Mancini, Anthony D. and Manfredi, Dylan and Miller, Jason and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine L. M. and Storage, Daniel and Swanson, Trevor and Van Swol, Lyn M. and Vaughn, Leigh Ann and Vidamuerte, Devere and Wiggins, Brady and Ratliff, Kate A.},
  date = {2022-04-29},
  journaltitle = {Collabra: Psychology},
  shortjournal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {35271},
  issn = {2474-7394},
  doi = {10.1525/collabra.35271},
  url = {https://doi.org/10.1525/collabra.35271},
  urldate = {2023-02-10},
  abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 17 Labs and N = 1,550 participants, after exclusions) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not fully understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/SVP6S3SB/Klein et al. - 2022 - Many Labs 4 Failure to Replicate Mortality Salien.pdf;/home/vboyce/Zotero/storage/7KS2AIAS/168050.html}
}

@article{lewandowsky2020,
  title = {Low Replicability Can Support Robust and Efficient Science},
  author = {Lewandowsky, Stephan and Oberauer, Klaus},
  date = {2020-01-17},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {1--12},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-14203-0},
  url = {https://www.nature.com/articles/s41467-019-14203-0},
  urldate = {2020-01-21},
  abstract = {There has been much concern about the “replication crisis” in psychology and other disciplines. Here the authors show that an efficient solution to the crisis would not insist on replication before publication, and would instead encourage publication before replication, with the findings marked as preliminary.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/GVMLLYIE/Lewandowsky and Oberauer - 2020 - Low replicability can support robust and efficient.pdf;/home/vboyce/Zotero/storage/LWXTQE3H/s41467-019-14203-0.html}
}

@article{mcshane2019,
  title = {Large-{{Scale Replication Projects}} in {{Contemporary Psychological Research}}},
  author = {McShane, Blakeley B. and Tackett, Jennifer L. and Böckenholt, Ulf and Gelman, Andrew},
  date = {2019-03-29},
  journaltitle = {The American Statistician},
  shortjournal = {Am. Stat.},
  volume = {73},
  pages = {99--105},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1505655},
  url = {https://doi.org/10.1080/00031305.2018.1505655},
  urldate = {2023-02-10},
  abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next—an issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching large-scale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous—even if the sign of the effect is consistent—the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
  issue = {sup1},
  keywords = {Between-study variation,Heterogeneity,Hierarchical,Meta-Analysis,Multilevel,Null hypothesis significance testing,p-value,Psychology,read,Replication},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1505655},
  file = {/home/vboyce/Zotero/storage/T9294332/McShane et al. - 2019 - Large-Scale Replication Projects in Contemporary P.pdf}
}

@article{nosek2020,
  title = {The Best Time to Argue about What a Replication Means? {{Before}} You Do It},
  shorttitle = {The Best Time to Argue about What a Replication Means?},
  author = {Nosek, Brian A. and Errington, Timothy M.},
  date = {2020-07},
  journaltitle = {Nature},
  volume = {583},
  number = {7817},
  pages = {518--520},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-02142-6},
  url = {https://www.nature.com/articles/d41586-020-02142-6},
  urldate = {2023-03-17},
  abstract = {To avoid stalemates and provide lessons, replicators and original researchers must reach agreement on a study design and set out expectations ahead of time.},
  issue = {7817},
  langid = {english},
  keywords = {Publishing,Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Publishing, Research data, Research management},
  file = {/home/vboyce/Zotero/storage/2WGSH9TU/Nosek and Errington - 2020 - The best time to argue about what a replication me.pdf;/home/vboyce/Zotero/storage/YQF3YWQH/d41586-020-02142-6.html}
}

@article{nosek2022,
  ids = {nosek2022a},
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aurélien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Michèle B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Schönbrodt, Felix D. and Vazire, Simine},
  date = {2022-01-04},
  journaltitle = {Annual Review of Psychology},
  shortjournal = {Annu. Rev. Psychol.},
  volume = {73},
  number = {1},
  eprint = {34665669},
  eprinttype = {pmid},
  pages = {719--748},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-020821-114157},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157},
  urldate = {2023-02-10},
  abstract = {Replication—an important, uncommon, and misunderstood practice—is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
  langid = {english},
  keywords = {generalizability,metascience,read,replication,reproducibility,research methods,robustness,statistical inference,theory,validity},
  file = {/home/vboyce/Zotero/storage/MCGCYCM7/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf;/home/vboyce/Zotero/storage/V6P5NF42/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}

@article{openscienceconsortium2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {Open Science Consortium},
  date = {2015},
  journaltitle = {Science},
  url = {https://www.science.org/doi/full/10.1126/science.aac4716?casa_token=IJ35TwwlcjsAAAAA%3AqiP68QbVAHleIg9zD3WugKWuV6Oa5rswS0VQnDsCq5I14ME4WIQabNGVD_T6SBSuAt6voVHNnWc0sw},
  urldate = {2023-03-17},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\textbackslash\% of original effect sizes were in the 95\textbackslash\% confidence interval of the replication effect size; 39\textbackslash\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\textbackslash\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/372DFPZ2/RPP_SCIENCE_2015.pdf;/home/vboyce/Zotero/storage/8JGRI4V2/science.html}
}

@article{patil2016,
  title = {What {{Should Researchers Expect When They Replicate Studies}}? {{A Statistical View}} of {{Replicability}} in {{Psychological Science}}},
  shorttitle = {What {{Should Researchers Expect When They Replicate Studies}}?},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  date = {2016-07},
  journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {11},
  number = {4},
  eprint = {27474140},
  eprinttype = {pmid},
  pages = {539--544},
  issn = {1745-6924},
  doi = {10.1177/1745691616646366},
  abstract = {A recent study of the replicability of key psychological findings is a major contribution toward understanding the human side of the scientific process. Despite the careful and nuanced analysis reported, the simple narrative disseminated by the mass, social, and scientific media was that in only 36\% of the studies were the original results replicated. In the current study, however, we showed that 77\% of the replication effect sizes reported were within a 95\% prediction interval calculated using the original effect size. Our analysis suggests two critical issues in understanding replication of psychological studies. First, researchers' intuitive expectations for what a replication should show do not always match with statistical estimates of replication. Second, when the results of original studies are very imprecise, they create wide prediction intervals-and a broad range of replication effects that are consistent with the original estimates. This may lead to effects that replicate successfully, in that replication results are consistent with statistical expectations, but do not provide much information about the size (or existence) of the true effect. In this light, the results of the Reproducibility Project: Psychology can be viewed as statistically consistent with what one might expect when performing a large-scale replication experiment.},
  langid = {english},
  pmcid = {PMC4968573},
  keywords = {Behavioral Research,Data Interpretation; Statistical,Humans,p values,prediction intervals,Psychology,replication,reproducibility,Reproducibility of Results,Reproducibility Project: Psychology},
  file = {/home/vboyce/Zotero/storage/4I4NTQ52/Patil et al. - 2016 - What Should Researchers Expect When They Replicate.pdf}
}

@article{pawel2020,
  title = {Probabilistic Forecasting of Replication Studies},
  author = {Pawel, Samuel and Held, Leonhard},
  date = {2020-04-22},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231416},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231416},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231416},
  urldate = {2023-03-17},
  abstract = {Throughout the last decade, the so-called replication crisis has stimulated many researchers to conduct large-scale replication projects. With data from four of these projects, we computed probabilistic forecasts of the replication outcomes, which we then evaluated regarding discrimination, calibration and sharpness. A novel model, which can take into account both inflation and heterogeneity of effects, was used and predicted the effect estimate of the replication study with good performance in two of the four data sets. In the other two data sets, predictive performance was still substantially improved compared to the naive model which does not consider inflation and heterogeneity of effects. The results suggest that many of the estimates from the original studies were inflated, possibly caused by publication bias or questionable research practices, and also that some degree of heterogeneity between original and replication effects should be expected. Moreover, the results indicate that the use of statistical significance as the only criterion for replication success may be questionable, since from a predictive viewpoint, non-significant replication results are often compatible with significant results from the original study. The developed statistical methods as well as the data sets are available in the R package ReplicationSuccess.},
  langid = {english},
  keywords = {Economics,Experimental economics,Experimental psychology,Forecasting,Replication studies,Social psychology,Social sciences,Test statistics},
  file = {/home/vboyce/Zotero/storage/P6HK7WJE/Pawel and Held - 2020 - Probabilistic forecasting of replication studies.pdf}
}
