
\documentclass{stanfordletter}
%\makelabels
\usepackage{todonotes}
\usepackage{varioref}
\usepackage{xr}
%\externaldocument[paper-]{../revision}
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber,style=apa,doi=false,url=false,hyperref=true,apamaxprtauth=30,uniquename=false]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
%\bibliography{../draft0}
\newcommand{\citet}[1]{\textcite{#1}}
\newcommand{\citep}[1]{\parencite{#1}}

%\makelabels
\newcommand{\theysaid}[1]{\begin{leftbar} \noindent 
		\textsl{ #1}\end{leftbar}}
\newcommand{\revised}[1]{\begin{quote}		#1 \end{quote}}


\begin{document}
	\name{Veronica Boyce}
	\signature{Veronica Boyce \\ Maya Mathur \\ Michael C. Frank}
	
	
	\begin{letter}{Dr Christina Demski \\ Associate Editor, Royal Society Open Science}
		
		
          \opening{Dear Dr Christina Demski,} 
          
          Thank you for your speedy response to our submission. 
          To address reviewer points, we are quoting the decision letter with our point-by-point responses and revised text. 
          
          Before we get to responses to reviewers, we should mention for transparency that we found an error in our code for calculating p-values that affected 26 rows of the data. This has been fixed and all analyses downstream of the problem have been rerun and the changes propagated to the manuscript, with no substantial changes to results or interpretation. (We were missing an absolute value function; which led to taking the wrong tail for p-values where the t-value was negative.)
          
          \theysaid{Both reviewers were in agreement that the manuscript is well written and describes a dataset and series of analyses that will be of interest to the readers of the journal. There were only a few minor points that they like to see addressed and responded to before it can be published. We would like to see your responses to these suggestions.}
          
          \theysaid{Reviewer comments to Author:
          
          Reviewer: 1
          Comments to the Author(s)
          I have read this manuscript with great enthusiasm. It provides an important infusion of new data into a field that is very much oversaturated with analyses of what little data there is.
        }
        
        \theysaid{
          
          I find the report to be clearly written, well-documented, of a sensible length, and appropriately cautious when discussing the generalizability and potential causal implications of the reported results. But for a single minor issue, I believe the report could be accepted as is.
          
          The only thing I miss in the current manuscript is some description/discussion of how close/conceptual replications in the sample tended to be. See LeBel et. al. (2018, figure 1; https://doi.org/10.1177/2515245918787489) for a taxonomy of replication closeness. It would not need to be a long section, and I am not saying the authors need to code closeness for all studies included, or anything like that. However, I recommend adding a short paragraph to give readers a sense of how closely the original procedures were followed in the replications (on average/in general). The rationale being that I would generally expect a higher replication rate the closer replication procedures are to the original. Having a sense of what mix of close/conceptual replications in this sample is thus relevant for my interpretation of the overall replicability estimate.}
          
          TODO our response!
          
          We've added TODO what we add
          
          We considered coding for closeness (and started to do so in a preliminary stage) but couldn't come up with a scheme that felt meaningfully uniform. We thank the reviewer for the pointer to LeBel!
          
          From memory, I think we have a range from exact replication to very close replication to close replication. 
          
          For studies that were originally done online and had open materials, may have approached exact replication. (Ex. the study I did, there was javascript for running the study available and I rehosted it, so the procedure was identical). 
          
          Many switched to online and may have had procedural differences as a result (or as a result of instruction text not being available and students recreating from what the manuscript said). 
          
          When possible the stimuli were usually the same. In a couple cases, populations and stimuli were farther due to language changes. In some cases, stimuli weren't available, and so were recreated to match as was possible. 
          
          Population was different in some cases due to switches from college student to crowdworker; in one case from high school to young adult crowd worker; and in some cases from "hiring manager" to crowdworker. 
          
          
          This is why we're careful to frame as "how well can grad students redo" -- closeness was dependent on factors like avialable and ease, and varies by study in how much it calls into question the original study results. 
          
          \theysaid{Reviewer: 2
          	Comments to the Author(s)
          	This is an excellent contribution, and I have only minor suggestions for the authors. I should state upfront that, unfortunately, I was more rushed than I expected to be when doing this review, and so did not access the data or code, or any supplemental materials.  I glanced at the preregistration.  I also lack the expertise to evaluate the statistical approach.
          }
          	
          \theysaid{	1a. I believe there are some deviations from the preregistration (e.g., year of publication is mentioned as a potential predictor (with no directional hypothesis) in the preregistration but is not in the results; other variables appear in the manuscript but not in the preregistration). The authors should be clear about these deviations and also any decisions that were not constrained by the preregistration. Perhaps a disclosure table would be a good way to share this information.}
          
          TODO check deviations
          
          We did include publication year in the models (and in the not-pre-registered list of correlations). 
          
          
          \theysaid{
          	1b. Even though the analyses are preregistered, I assume that the decisions made in the preregistration were not made without any knowledge of the data.  That is, the people involved in making the decisions in the preregistration presumably knew/remembered some of the outcomes of the replication studies.  Of course this is fine, but I think it makes the choices and analyses slightly data-dependent, and therefore more susceptible to inadvertently overfitting the models to this dataset. I think it would be good to point this out to readers, and calibrate claims appropriately (I think the claims are already well-calibrated, in that the authors don't make strong claims, which is appropriate for several reasons).
          }
          
          In fact, much of the data-coding had been done when the pre-registration was finalized, due to the need to figure out what variables were actually codeable. 
          
          
          
          \theysaid{
          	2. There are some descriptive variables that I would have liked to see in the main text, such as the number of studies at each level of each of the predictor variables, the correlations among the predictor variables, and the correlations among the outcome variables.
          }
          
          \theysaid{
          	3. I was curious to know the magnitude of the difference in effect sizes between the original studies and replication studies, for: 1) all studies, and 2) just the "successful" replications.
          }
          
          \theysaid{
          	4. The authors are appropriately cautious in drawing conclusions, for example about the effect of within-participant designs.  I would suggest adding that it's possible one factor that contributed to this effect is that within-participant designs could be more susceptible to artifacts, such as demand characteristics, that could inflate the replicability of the result but not its accuracy.  This is also a reason to caution against simply adopting a within-participant design as a way to improve replicability in future studies, without considering the potential costs to other aspects of validity/accuracy of the results.
          }
          \theysaid{
          	5. I was curious whether the authors considered inviting the graduate students who ran the studies as co-authors/contributors on this paper. I could see good reasons for either decision, but just wanted to raise the question.
          }
          
          \theysaid{
          	6. Personally, I was curious about how the "date of original study publication" fared as a predictor, as well as whether being published in Psych Science was a predictor (I have a conflict of interest in asking about this, I am the incoming editor in chief of Psych Science).  Of course readers can presumably analyze these themselves, but I wonder if it would be worth including at least publication year, as that was in the preregistration (and I also thought of it before looking at the preregistration, and I expect other readers might wonder the same thing).
          }
          We do have publication year. We've added a more clear mention in the text. It is correlated with practices that have changed over time, such as open data, open materials, and online samples, which basically means it's correlated with closeness of replication. 
          
          Psych Science makes up TODO \% of the sample, so we couldn't really use it as a predictor, since other journals are much much less represented. 
          
          
          	\theysaid{
          	Minor typos:
          	section 2.1 "of replications has outcomes with" should be "had"
          	section 4.3 "ramapped" should be "remapped"
          }
          Fixed, thank you!
          
          
         
          
          \closing{Sincerely,}
		
	\end{letter}
	
\end{document}




