---
title: "Eleven years of student replication projects provide evidence on the correlates of replicability in psychology"
author:
  - name: Veronica Boyce
    affiliation: Stanford
    footnote:
      - corresp
  - name: Maya Mathur
    affiliation: Stanford
  - name: Michael C. Frank
    affiliation: Stanford
address:
  - code: Stanford
    address: Stanford University
footnote:
  - code: corresp
    text: "Corresponding author. Email: vboyce@stanford.edu"
bibliography: ["251MA.bib"] # Replace with one or more of your own bibtex files. Better BibTeX for Zotero is your friend
csl: dmk-format.csl # Use any CSL style. See https://www.zotero.org/styles for a good list. Ignored if citation_package: natbib
link-citations: TRUE
output:
  bookdown::pdf_document2:
    toc: FALSE
    keep_tex: TRUE
    template: generic_article_template.tex
    #md_extensions: "-autolink_bare_uris"
    number_sections: TRUE
    citation_package: default # Can also be "natbib"
lang: en # Main document language in BCP47 format
geometry: "margin=25mm"
papersize: a4
#linestretch: 2 # for double spacing
endfloat: FALSE # Set to TRUE to turn on latex endfloat package to place figures and tables at end of document
# endfloatoption: # See endfloat documentation for more possibilities
#   - tablesfirst # Default
#   - nomarkers # Default
numberlines: FALSE
authblk: TRUE # FALSE = author affiliations in footnotes; TRUE = author affiliations in a block below author names
footnotehyper: FALSE # TRUE will give you enhanced table footnote capabilities. Set to FALSE to be able to use French blocks. Needed due to what appears to be a latex bug.
urlcolor: blue
linkcolor: blue
citecolor: blue
graphics: TRUE # Needed to be able to include images
tables: TRUE # Needed to be able to include tables
# fancyhdr:
#   first:
#     #headleft: "REPORT-NO-XXXX"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 0pt
#     #footleft: A left foot
#     footrulewidth: 0pt
#   subsequent:
#     #headleft: "NEXT-PAGE-HEADER-LEFT"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 1pt
#     footrulewidth: 0pt

header-includes:
 - \usepackage{tikz}
 - \usetikzlibrary{positioning,chains}
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\textfraction}{0.00}
 - \renewcommand{\topfraction}{1}
 - \renewcommand{\bottomfraction}{1}
 - \renewcommand{\floatpagefraction}{1}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
#knitr::opts_chunk$set(fig.pos = 'th') # Places figures at top or here
knitr::opts_chunk$set(out.width = '100%', dpi=300,
                      fig.width=8, fig.width=8) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

options(knitr.table.format="latex") # For kable tables to work without setting format option

knitr::opts_chunk$set(echo=F, warning=F, message=F)#dev = "png", dev.args = list(type = "cairo-png")
 library("papaja")
library("bookdown")
library("rticles")
 library(here)
 library(tidyverse)
 library(brms)
library(tidybayes)
library(kableExtra)
library(viridis)
library(cowplot)
library(ggthemes)
#r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")

model_location="code/models"

d <- read_csv(here("data", "processed_data.csv")) |> filter(status!="unusable")
```

<!---------------------- Abstract --------------------->

::: {.abstract data-latex="" lang=en}

A cumulative science relies on knowing how much to trust published findings. Large-scale replication studies provide a measure of how reliable the published literature in a field is by determining what fraction of studies replicate. Because of the effort involved in conducting a large number of replications, few large-scale replications are available. We add 176 replication projects conducted by students of experimental studies primarily in psychology. Roughly half the studies were replicated successfully, and we report how features of the studies and the replications influenced the replication outcome. 

:::


<!------------ Main text -------------------->

<!--introduction outline: 

Things don't replicate that well and it feels like a problem for cumulative science 
* For a cumulative science, we want (many) of the load-bearing empirical results that form the bases of theories to hold up. 
* People were acting as if publication of results meant they were true/reliable, trying to build on things; caused crises when some prominent results failed to replicate. Called into question reliability of literature as a whole -- lead to a few measurements. 
* Large scale replications of chunks of the literature, but aren't many (list and explain)
* These rates feel lower than we want

Why? Many approaches, no definitive answers.
* Why are rates what they are. Both want to know to fix but also to watch out / calibrate caution. Can be approached from theoretical, interventional, and correlational dimensions.
* Theoretical -- we know people do bad practices and can model how this leads to lower replicability. Requires things we already have ideas about
* Interventional -- try things they think will make it better (like not doing bad practices), see what happens. Not everything is intervention prone, need to have strong belief first. Expensive!
* Correlational -- markets/etc say there is some predictive value to be had, but don't say if it's transparent and explainable. RP:P. 


What's missing from the conversation and why our approach is good. 
* There's been a lot of discussion around replicabilty and it's role and yadda yadda (much overfocused on RP:P results because that's what's available). Growing sophistication and valid points, but we think it misses the mark about what practicing scientists are concerned about. 
* Don't want to build on something and fail a bunch trying to get your variant to work and then realize it just doesn't work. Replicability (as we mean it here) A useful framing is "ability to do cumulative science" -- close to what people mean, may disagree with many things
* How this approach differs and what it gets us
    * (built in feasibility under reasonable / small resource constraints)
    * interest based sampling 
    * functional approach sidesteps issues of "truth" (may even be reliable in a different environment, but would be)
    
Teaser:
* We do x,y,z, and find that larger effect sizes and within subjects designs are the two strongest correlates of replicability in this sample. 

-->
<!--

# Text slush pile 

The perceived need for more replications in tension with the lack of publishing incentives for doing replications led several to suggest that pedagogical replications could fill this gap. Doing replications is a way for students to learn to do experiments in a scaffolded way and leads to more useful results than what students could design from scratch on the same timescale [@quintana2021; @wagge2019;@frank2012; @hawkins]. [TODO needs transition]



 * Don't want to build on something and fail a bunch trying to get your variant to work and then realize it just doesn't work. Replicability (as we mean it here) A useful framing is "ability to do cumulative science" -- close to what people mean, may disagree with many things
We think this is a valuable estimand because it's estimating what people actually care about: is this result of interest sturdy enough to support cumulative science? 

[who knows where this paragraph should go] Replications are one way of approximating whether an effect in the target paper is true, and how likely results are to replicate in some sort of platonic ideal world. There is no one answer here; replication projects measure something else that could be treated as an approximation. However, what a replication is really measuring is how likely it to get that effect given some conditions. Which the right conditions are is a potential point of contention (see discussion around closeness and sample size, power etc).  Here, we are explicit in what sample of replicators and replication conditions were are sampling, and thus what we can generalize to. We are estimating how likely replication is when done online by a graduate student, under constraints that are typical for graduate students (limited time, limited budget). As much of scientific process is in fact performed by graduate students, we think this in itself is an interesting question. 

We do not interpret our results as saying that all non-replications were false positives (presumably some are replicable under other circumstances and others are not). Some of the factors we look at are more easily interpreted as being about the original study than others. We do not assign causal explanation to the predictors because there are multiple plausible interpretations: they could be correlates of QRPs in the original, they could be correlates of harder-to-detect or more fragile effects, they could be correlates of less-close replications, they could even be correlates of another stronger predictor. 

[ there is controversy here, but to even discuss it, we need empirics] Even those who argue that replication is not so essential still rely on data from replications to make their cases that replication rates are not as low as they seem and that these replication rates are acceptable. @lewandowsky2020

circle back to the idea that student replications (either as class or before extension into their own work) would be valuable 

To understand how replicable the literature as a whole is, we need empirical data about replication attempts across a wide range of studies. Most replications are targeted replications of individual results of interest to the researcher, but these are not sampled randomly from the literature, are performed in inconsistent ways, and may be selectively published depending on the results. Thus, they are not ideal for estimating the overall replicability of the field. 


There have been a limited number of large-scale replication efforts, which sample studies from a discipline to estimate overall replicability. Due to the arduous process of replicating many studies, there have not been many large-scale replication efforts. Thus, all of the argumentation around predictors of replications and field-wide replication rates is fit to a small number of data points. 
Psychology is in the middle of heated debate over its research practices and the subject of many reform efforts.  Many of these issues concern how trustworth the published literature is (or isn't), how it got to be this way, and what measures can be taken to improve the quality of findings for the future. These issues center around issues of reproducibility  and replicability. Concerns about replication rates have become a large point of discussion in psychology and other fields, with large scale replications spurring these discussions and providing the empirical data analyzed by all sides. 

  [idk, maybe worth framing that this was a big deal study when it came out]. TODO some discussion of how many papers have reanalysed this cutting the data different ways. 

 Many labs 5 was a re-replication attempt on 10 of RP:P and rescued 2/10 [@ebersole2020]. TODO look up details
 
 as validity, interestingness, and generalizability, can also be important
Cumulative science and theory-building rely on empirical results that Replicability is a desirable feature for any study that we build theory or future experiments off of. It's a pre-requisite for building theories that we have phenomena for these studies to explain, but results that cannot be observed consistently aren't results to build theories around. However, to be a load-bearing empirical result, a study needs to do much more than just replicate, it also needs to be interesting, have a believable theoretical interpretation, generalize appropriately, and have reliable constructs. 

## predictors
In addition to determining estimated overall replicaiton rates for fields and journals, there's also merit in knowing what features of experiments (and replication attempts) are predictive of replication success. Blah blah stakeholders and resources. Scientists may want to build their work on studies that are likely to replicate, so that they can replicate and build on work without wasting resources. Similarly investing in interventions may want to start with things that are more likely to replicate. There's some signal here, as people are able to predict replication success at above chance CITATIONS 

RP:P looked at how replication rates varied across subsamples of their studies [@openscienceconsortium2015]. 

There are reasons to believe that experimental factors such as number of items or between and within subject designs may also be predictive (cite our old paper), and could pontentially be some of the reason for subfield differences. TODO maybe foreshadow that there are reasons to believe there are correlations!

While many replications of individual studies are conducted, these may have been selected for non-representative reasons, such as a high prior on replicating (e.g. as a demonstration), or a low or uncertain prior (e.g. for a high-value study). When sampling is likely related to replicability, the studies are less good for estimating base rates and predictors of replicability in the literature as a whole. 

using pedagogical replications, which are good because they’re p(build on)
slightly different estimand but maybe a better one?
One key part of the question of whether a study is replicable or trustworthy is the idea of "if I did this, would I get the same pattern of results?". 


one strategy for the pedagogical bit at end of intro: stress “this is what people want to do, that is, pick up a project that they care about (not a random sample of the literature) and see if they can ‘get it to work’” - and “subjective replication success is our primary measure for the same reason….” 

Quantifying when an effect is a replication is hard, and many statistical metrics have been proposed, often with tradeoffs (TODO citations). We instead focus on a subjective score representing a more functionally oriented outcome: studies replicate if the results match the original enough that researchers would feel comfortable building on the study.  This matches the motivation for many replications (often in replicate and extend paradigms). SPELL OUT SAMPLING DIFFERENCE AND SUCCESS DIFFERENCE.

There's a "can I trust this finding" question, but a key part of it is, "if I did this, would I get the same pattern of results" -- and that's what we're measuring. 

We estimate this quantity in the context of a particular pedagogical situation: that of a graduate student entering a doctoral program in psychology... 

Prior literature, such as @frank2012 and @hawkins have advocated using research methods classes to conduct replications, both for the education and scientific value. 

# introduction
. Building a cumulative body of scientific theories similarly requires that the empirical results, especially the more foundational empirical results, be sturdy. Results that cannot be observed consistently are like dirt clots, they don't hold their shape and thus are not a strong foundation for the theories and studies built on top of them. Replications and the theory of replicability can be thought of as a way of testing whether the empirical results are sturdy. Replicating is a necessary feature for load-bearing empirical results. We note that replicability is not the only desirable feature of a study, and that replicating does not mean that a study is valid, or generalizable, or interesting. 

Knowing what predicts and drives non-replications would let us potentially increase the replication rate, and also calibrate our levels of caution and skepticism toward results. Theoretical, interventional and correlational approaches do not yet provide comprehensive answers. 





-->

# Introduction

A cumulative science requires foundational empirical results that can be build upon in theories and future experiments. These load-bearing results need to be reliable; when they are not robust, attempts to build on them waste resources, trying to find explanations and effects where there may be none. 

Scientists, policymakers, and the public often treat publication as a signal that a result is (likely to be) true. This trust in published findings leads scientists to attempt to build on them, but failures to build on them are explained away and rarely published, leaving the original results uncontested in the literature, while failures accumulate in private. 

Publicized failures to replicate called into question the tacit assumption that most of the literature was robust and would replicate. Certain prominent findings turned out to not replicate at all [ex. terror management theory; @klein2022], and a large scale replication project pegged the replication rate for findings in top-tier psychology journals at around 40% [@openscienceconsortium2015]. 

Few large scale replications have been conducted in psychology, due to the resources required to run large numbers of replication studies. We are aware of three. RP:P sampled roughly 100 studies from articles published in three top psychology journals in 2008 and found a overall replication rate of around 40% [@openscienceconsortium2015]. ManyLabs investigated heterogeneity using short target studies that compared between two conditions each, but these studies are not representative of the psychology literature as a whole. Across Many Labs 1-3, 29 of 51 target effects (57%) replicated [@klein2014; @klein2018; @ebersole2016]. @camerer2018 replicated all 21 behavioral social science studies from Nature and Science from 2010-2015 that were feasible; the replication rate was around 60%. These roughly 170 replications of psychology studies are the primary empirical results on which discussions of replication rates in psychology are based. 

A replication rate around 50% runs counter to the prior assumption that most findings in the literature was robust enough to support cumulative science, and raises the question: What makes the replication rates in psychology so low? 

Theoretical approaches to understanding the replication rate have modeled how flexible research practices, selective reporting, and a bias toward positive results could lead to a high rate of (non-replicable) false positives (CITE). However, while scientists admit to engaging in these practices, we don't yet know what proportion of non-replications are driven by these factors or how to identify these studies in the literature. 

Experimental approaches attempt to intervene on the low replication rate, by seeing if changing certain factors affects whether studies replicate. @protzko2020 showed, across 16 studies, that better methodological practices, such as transparency, large sample sizes, and confirmatory tests, led to replication rates that matched theoretical expectations based on the effect sizes and sample sizes, with replication effect sizes comparable with the original.  Many Labs 5 examined how adding expert advice to a replication process might intervene to improve the replicate rate, and found that it mostly didn't, at least on the 10 studies they looked at [@ebersole2020]. These types of experiments are valuable for testing potential causes of non-replication, but they don't scale well do to expense. Additionally, not all potential influences on non-replication are experimentally manipulable.

Correlational approaches looking for predictors of replicability are the most popular. Prediction markets and elicitations have established that people can predict what studies will replicate above chance [@hoogeveen2019;@dreber2015; @forsell2019;@camerer2018], but have not identified many concrete predictors that differentiate replications from non-replications. Machine learning approaches CITE similarly show predictive signal, but do not provide explainable predictor variables. @opensicenceconsortium2015 looked for simple correlates of replicability in the RP:P sample and found that studies in cognitive psychology (as opposed to social psychology) and studies with larger effect sizes and smaller p-values were more likely to replicate. Correlations approaches depend on data from replications, generally drawing heavily from the same small set of data points. In particular, the RP:P dataset itself is much discussed and reanalyzed [@etz2016;@gilbert2016;@patil2016;@anderson2016] to the point that much of what we think we know about replicability may be overfit to the 100 studies included in in RP:P. 

Many of the discussions and reanalyses have raised questions about what the right metrics for measuring replicability are and what standards are reasonable to expect. Some of this contention is based around notions that failed replications are referenda on the truth of the original finding. 

Prior approaches to replicability have focused on interpreting results in terms of a potentially problematic estimand: the probability of a finding in the literature being somehow truly replicable. Critics have pointed out that "true" replicability may not be possible to estimate outside of a specific sample [@vanbavel2016] or even time period [@ramscar]. 

Further, the methods for estimating this quantity have been theoretically problematic. Sampling schemes for prior work typically do not reflect an entirely random sample from the literature; instead they sample from specific journals where results may be of more interest and adjust the sample for feasibility concerns. These are reasonable sampling choices, but they undermine the claim that the estimand is the level of "truth" in the literature as a whole. Sampling truely at random from the literature may not even be desirable, as  arguably a literature will succeed if useful discoveries come out of it, not if random findings are true [@wilson2020]. How much we care about whether a study is "trustworthy" is not uniformly distributed across the literature. 

In contrast, we have explicitly pursued a different estimand: the probability that a researcher, on selecting a finding of interest from the literature, can successfully achieve a result satisfactorily close enough to the original that they can build on it in their own work, with all the necessary compromises to the methods and sample of the original that may be required by the constraints of the situation. 

This framing of replicability matches our methodology. 

Rather than sampling at random from some parts of the literature; our sample of studies is selected based on what studies students were interested in and wanting to replicate, with some filtering for feasibility; this sampling reflects how scientists choose what studies to build on: those that are interesting and relatively doable given methodological and budgetary constraints.

We use a subjective replication score as our primary metric. Whether one feels confident in the results of a study given a replication is not always dependent on only one outcome measure (ex. interaction term) and particularly not dependent on only one statistical comparison between the two studies (ex. replication is p<.05 same direction as original). This avoids bright line distinctions and accomodates the range of outcome measures in our diverse set of studies.

All our replications were conducted under short time scales and relatively small budgets, which mimics the constraints many scientists are under when starting new projects. From the point of view of cumulative science, researchers want to know if they can get paradigms to work under their real-world constraints. The same issue that may cause a study not to replicate under constrained circumstances (despite hypothetically replicating under more favorable circumstances such as expert administration or larger budgets) will also plague attempts to build off those studies in constrained circumstances. Thus, we believe it is relevant to estimate replicability as done by a graduate student with limited resources. 

Overall, we take a functional approach to assessing replicability, by framing both our methods and interpretation around the idea of whether work can be repeated or built on by an early-career scientist. 

Our contribution is a new dataset of 176 replications of experimental studies from the social sciences, primarily psychology. These replications were conducted by students in graduate-level experimental methods class between 2011 and 2022 as individual course projects. We investigated predictors of replicability in this dataset and found that within-subjects designs and studies with large standardized effect sizes were positively correlated with replication success. 






```{=latex}
\definecolor{bad}{HTML}{FFCCCB}
		\definecolor{meh}{HTML}{efefef}
			\definecolor{good}{HTML}{abcdff}
	\tikzset{
		mynode/.style={
			draw, rectangle, align=center, text width=4.5cm, scale=1, font=\small, inner sep=.5ex},
		arrow/.style={
		 very thick,->,>=stealth}
	}
	
\begin{figure}[ht]
	
	\begin{tikzpicture}[
		node distance=.8cm,
		start chain=1 going below,
		every join/.style=arrow,
		]
		\coordinate[on chain=1] (tc);
		\node[mynode, on chain=1, fill=meh] (n2)
		{\textbf{210} projects from 2011-2022};
		\node[mynode, join, on chain=1, fill=meh] (n3)
		{\textbf{189} original - replication pairs};
		\node[mynode, join, on chain=1, fill=meh] (n4)
		{\textbf{177} experimental pairs};
		\node[mynode, join, on chain=1, fill=good] (n5)
		{\textbf{176} pairs included};
		\node[mynode, join, on chain=1, fill=good] (n6)
		{\textbf{136} pairs with ES};
		\node[mynode, join, on chain=1, fill=good] (n7)
		{\textbf{112} pairs with SMD};
		
		
		\begin{scope}[start chain=going right]
			\chainin (n2);
			\node[mynode, join, on chain, fill=bad]
			{ \textbf{2} missing projects \\ \textbf{19} reproducibility projects};
			\chainin (n3);
			\node[mynode, join, on chain, fill=bad]
			{\textbf{12} non-experimental pairs};
			\chainin (n4);
			\node[mynode, join, on chain, fill=bad]
			{\textbf{1} pair missing sample size};
		\end{scope}
	\end{tikzpicture}
\caption{Which studies were excluded for what reasons, and how many original-replication pairs are left.}\label{fig:prisma}
\end{figure}

```


# Results

```{r}
matching <- d |> filter(status!="unusable") |> mutate(match=ifelse(replicated_instructor_code==replicated_report_code,1,0)) |> group_by(match) |> tally()

pred_int_avail <- d |> filter(!is.na(predInt)&!is.na(p_orig)) |> tally()

```

PSYCH 251 is Stanford Psychology's graduate-level experimental methods class taught by MCF.  During the 10 week class, students replicate a published finding. They individually re-implement the study, write analysis code, pre-register their study, collect data using an online platform, and write up a structured replication report. Students are free to choose studies related to their research interests, with the default recommendation being an article from a recent year of Psychological Science. While this choice results in a non-random sample from the literature, the sample is representative of studies that are of interest to and doable by first year graduate students. 

The sample of replicated studies reflects the variability of the literature, including studies from different subfields, using different experimental methods and statistical outcomes. We leverage the naturally occurring variability in this sample of replications to examine how different demographic, experimental design, and statistical properties predict replication success. 

Many different measures can be used to define replication success of an individual statistical result, and there is much discussion over what each measure represents and which make sense [@simonsohn2015; @gelman2018; @mathur2020]. Because we operationalized replicability as whether a study could be built upon, we used a subjective rating of replication success as our primary outcome measure. This measure has the benefit that it was applicable across the diverse range of statistical measures and reporting practices present in the sample. It, unlike statistical measures of replication, could easily accommodate studies where there were multiple important outcome measures that together defined the pattern of interest. A holistic measure of replication success had been coded for each project when it was turned in at the end of the class. For reliability, VB independently code the replication success from the replication reports; discrepancies were resolved by discussion between MCF and VB (`r round(pluck(matching,2,1)/176*100)`% of cases). 

As a complement, we also used two statistical measure of replication on the subset of the data where they were computable (`r pluck(pred_int_avail,1)` cases, see Figure \ref{fig:prisma}). We measured p-original, the p-value on the null hypothesis that the original and replication statistics are from the same distribution, as a continuous variable, and we also determined whether the replication statistic fell within the prediction interval of the original statistic [@errington2021]. These both measure how similar the estimates from the two sets of data are. 

```{r smd, out.height="25%", fig.width=6, fig.height=3, fig.pos="ht", fig.cap="Relationship between SMD of the original study, SMD of the replication study, and subjective replication success rating, for those studies where SMD was applicable."}
d |> 
  filter(target_d_calc < 5, rep_d_calc < 5) |>
  ggplot(aes(x=target_d_calc, y=rep_d_calc, color=sub_rep))+
 
  geom_abline(slope=1, intercept=0, lty = 2)+
  geom_smooth(method = "lm") + 
   geom_point(size=2.5) +
  geom_hline(yintercept = 0, lty = 2)+
  scale_color_viridis(discrete=F, direction = 1, 
                      name = "Subjective\nReplication\nSuccess\n")+
  labs(color="subrep", x="Original SMD", y="Replication SMD") + 
  theme(legend.position = "right")

```

## Overall replication rate 

```{r}
sub_rep_rate <- d |> summarize(m=mean(sub_rep)*100)

pred_int_rate <- d |> filter(!is.na(predInt)) |> summarize(m=mean(as.numeric(predInt))*100)
porig_rate <- d |> filter(!is.na(p_orig)) |> summarize(m=median(p_orig))
```

Across the 176 studies, the overall subjective replication rate was `r round(pluck(sub_rep_rate,1))`%. `r round(pluck(pred_int_rate,1))`% (N=FOOBAR) of the studies had replication outcomes within the prediction interval of the original outcome. The median p_original value was `r round(pluck(porig_rate, 1),2)`. Figure \ref{fig:smd} shows the relationship between original SMD, replication SMD, and subjective replication score. Roughly speaking, there's a cluster of studies that replicate with similar effect sizes to the original and another cluster that fail to replicate with replication effect sizes near zero. On average, there is a diminution of effect sizes from original to replication. 

```{r cor}

for_cor <- read_csv(here("data","for_model.csv")) |> filter(include %in% c("stats", "exp", "pred_int")) |>
  select(sub_rep, 
         pub_year, subfield, open_data, open_mat, stanford, change_platform, log_ratio_ss, is_within, single_vignette, log_sample, log_trials) |> mutate(social=ifelse(subfield=="social", 1,0),other_psych=ifelse(subfield=="other-psych",1,0), non_psych=ifelse(subfield=="non-psych", 1,0)) 

sub_cor <- function(var, stat = "estimate") {
  if (stat == "estimate") {
    cor.test(for_cor$sub_rep, pull(for_cor, {{var}}))$estimate
  } else {
    cor.test(for_cor$sub_rep, pull(for_cor, {{var}}))$p.value
  }
}



preds <- c("pub_year", "open_data",  "open_mat", "stanford", "change_platform", 
           "log_ratio_ss", "is_within", "single_vignette", "log_sample", 
           "log_trials", "social", "other_psych", "non_psych")

cors <- tibble(preds = preds) |>
  mutate(r = sapply(preds, function(x) sub_cor(x, stat = "estimate")),
         p = sapply(preds, function(x) sub_cor(x, stat = "p"))) |> 
  mutate(Predictors=factor(preds, levels=c("social", "other_psych", "non_psych", "is_within", "single_vignette", "change_platform", "open_data", "open_mat", "stanford", "pub_year", "log_trials", "log_sample", "log_ratio_ss"), labels=c("Social", "Other psych", "Non psych", "Within subjects", "Single vignette", "Switch to online", "Open data", "Open materials", "Stanford", "Publication year", "Log trials", "Log original sample size", "Log rep/orig sample"))) |> arrange(desc(r)) |> select(Predictors, r, p)

library(kableExtra)
knitr::kable(cors, digits = 3, align='rcc',format="latex", position="!h", caption="The correlation of individual predictors with subjective replication outcomes. For subfield, cognitive psychology is treated as the baseline condition. See Methods for how these variables were coded.") |> kable_styling(full_width=F, font_size=10) 

```

```{r}


 within <- glm(sub_rep ~ is_within, data=for_cor, family=binomial)$coefficient[2] |> exp() 
 vignette <- glm(sub_rep ~ single_vignette, data=for_cor, family=binomial)$coefficient[2] |> exp()
 
  subfield <- glm(sub_rep ~ subfield, data=for_cor |> filter(subfield %in% c("social", "cognitive")), family=binomial)$coefficient[2] |> exp()


```

## Single predictors
Properties of both the original study and the replication can influence whether or not the replication is a success. We chose a set of predictor variables from the correlational results of RP:P and our own intuitions about experimental factors that might impact replication success as well as some covariates related to how close the replication would be. A full description of these features is given in methods. 

Many of these predictors individually correlate with subjective replication success (Table \ref{tab:cor}). Predictors of higher replicability included within-subjects designs, higher numbers of trials, and open data. Predictors of lower replicability included single vignetted studies, social psychology studies, and original-replication pairs where the replication switched to online. 

Distributions of study outcomes across some of these properties are shown in Figure \ref{fig:predictors-graph}. Both social and cognitive psychology studies were well represented, and the cognitive psychology studies replicated at `r round(1/subfield, 2)` times the rate of social psychology studies. Within and between subjects designs were both common, and within replicated `r within |> round(2)` times as much. Similarly, studies with multiple vignettes replicated `r round(1/vignette, 2)` times more than single vignetted studies. However, there were strong correlations among these experimental features and between these experimental features and subfield.

Studies with open data, which almost always also had open materials, tended to replicate more than studies without open data, although this may be linked to temporal trends. 

Nearly all replications studies were conducted online, but original studies were split between using in-person and online recruitment. Replications that switched to online were less likely to replicate than those that had the same modality as the original (generally both online, in a few cases both in-person).  While online studies in general show comparable results to studies conducted in person [@crump2013]TODO MORE CITATIONS, switching the modality does decrease the closeness of the replication, and some studies done in person may not have been well adapted (ex. inductions may be weaker or attention checks inadequate to the new sample). 


```{r predictors-graph, out.width="100%", fig.width=8, fig.height=4, fig.pos="ht", fig.cap="Distribution of subjective replication scores within categories. Bar heights are counts of studies."}
dat <- read_csv(here("data","for_model.csv")) |> filter(include %in% c("stats", "exp", "pred_int")) |> mutate(sub_rep=as.factor(sub_rep))


sub_with_leg <- dat |> mutate(subfield=factor(subfield, levels=c("cognitive", "social", "other-psych", "non-psych"), 
                                              labels=c("Cognitive\npsychology", "Social\npsychology", "Other\npsychology", "Not\npsychology"))) |>
  ggplot(aes(x=subfield, fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.x=element_blank(), axis.title.y=element_blank())+
  labs(fill="Subjective\nreplication\nscore")

leg <- get_legend(sub_with_leg)

subfield <- sub_with_leg+theme(legend.position="none")

open <- dat |> mutate(openness=case_when(
  open_mat==1& open_data==1 ~ "Open materials \n Open data",
  open_mat==1 ~ "Open materials \n No open data",
  open_data==1 ~ "No open materials \n Open data",
  T ~ "No open materials \n No open data")) |> 
  ggplot( aes(x=openness |> factor(), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position="none")

modal <- ggplot(dat, aes(x=ifelse(change_platform,"Switch to online\n","Same modality\n"), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position="none")


within <- ggplot(dat, aes(x=ifelse(is_within, "Within \nsubjects", "Between\nsubjects"), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
  coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position="none")


vignette <- ggplot(dat, aes(x=ifelse(single_vignette, "Single \nvignette", "Multiple \nvignettes"), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position="none")

top_row <- plot_grid(subfield, within, vignette, rel_widths=c(1, .6, .6), nrow=1)
mid_row <- plot_grid(open,  modal, leg, rel_widths=c(1.2, .6, .25), nrow=1)
plot_grid (top_row, mid_row, nrow=2)
```

```{r}
do_estimates <- function(model, variable){
  summary(model)$fixed |> as_tibble(rownames="var") |> filter(var==variable)|> mutate(text=str_c(round(Est.Error,2), ", CrI= [", round(`l-95% CI`,2),", ", round(`u-95% CI`,2),"]")) |> pull(text)
}

tier1_subjective <- read_rds(here(model_location, "tier1_subjective.rds"))

within_1 <- do_estimates(tier1_subjective, "is_within")

tier3_subjective <- read_rds(here(model_location, "tier3_subjective.rds"))

within_3 <- do_estimates(tier3_subjective, "is_within")

smd_3 <- do_estimates(tier3_subjective, "z_target_d_calc")

sens_3 <- read_rds(here(model_location, "sens_tier3_subjective.rds"))

within_sens <- do_estimates(sens_3, "is_within")

smd_sens <- do_estimates(sens_3, "z_target_d_calc")

pred_int <- read_rds(here(model_location, "tier3_predint.rds"))

within_2 <- do_estimates(pred_int, "is_within")

smd_2 <- do_estimates(pred_int, "z_target_d_calc")
```
 
## Regression model
While a number of the predictors show individual correlations with the subjective replication score, many of the predictors are also correlated with one another. In order to determine which predictors were the strongest, we ran a series of pre-registered regularized regression models (see Methods for details; see Supplement for all estimates from all models). The coefficient estimates of the primary model, predicting the subjective replication scores based on all the data, are shown in Figure \ref{fig:mod-results}. Due to a large number of predictors coupled with a small and noisy dataset, even with strong regularization, there is much uncertainty around the coefficients. The general directions of coefficients are consistent with the effects of the predictors in isolation. 

Within-subjects designs stand out as the strongest indicator of replicability in the model without statistical predictors (`r within_1`). When statistical predictors are added to the model, within-subjects designs remain predictive (`r within_3`). Standardized effect size is another strong predictor of subjective replication score (`r smd_3`). Both effects are robust to a sensitivity analysis including only studies with close replications and matching statistical tests (within-subjects `r within_sens`; effect size `r smd_sens`). 

We also ran models predicting whether the replication effect was within the prediction interval as the original effect and what the p-original was between the replication and original. Both these models had even more uncertain estimates. While the credible intervals are wide, the general patterns of predictors are similar to the subjective replication models. The strongest predictors are still within-subjects designs (`r within_2`) and studies with larger effect sizes (`r smd_2`). 

```{r mod-results, out.width="90%", fig.width=8, fig.height=5, fig.pos="ht", fig.cap="Coefficient estimates and uncertainty from a model predicting subjective replication scores from the full dataset."}

do_draws <- function(model){
  draws <- model |> gather_draws(`b_[a-zA-Z_(): ]+`, regex=TRUE) |> 
    mutate(Term=str_sub(.variable, 3,-1) |> str_replace("M","_"), Estimate=.value) |>  filter(Term!="Intercept") |> 
    mutate(Term=factor(Term, levels=c("subfieldsocial", "subfieldother_psych", "subfieldnon_psych", "is_within", "single_vignette", "change_platform", "open_data", "open_mat", "stanford", "z_pub_year", "z_log_trials", "z_log_sample", "z_log_ratio_ss","z_log_p","z_target_d_calc"), labels=c("Social", "Other psych", "Non psych", "Within subjects", "Single vignette", "Switch to online", "Open data", "Open materials", "Stanford", "Publication year", "Log trials", "Log original sample size", "Log rep/orig sample", "P-value", "Original effect size"))) 
}

tier1_subjective <- read_rds(here(model_location, "tier1_subjective.rds")) |> do_draws() |> mutate(type="Without stats predictors") |> mutate(sens="All data")
tier3_subjective <- read_rds(here(model_location, "tier3_subjective.rds"))|> do_draws() |> mutate(type="With stats predictors") |> mutate(sens="All data")

tier1_subjective |> union(tier3_subjective) |> ggplot(aes(y = reorder(Term, Estimate, mean), x = Estimate, color=type)) + geom_vline(xintercept=0)+
  scale_color_solarized()+
  stat_pointinterval(position=position_dodge(width=.7))+labs(y="", x="Estimate, 66%, 95% CrI predicting subjective replication score", color="", linetype="", shape="")+theme(axis.text=element_text(size=11),legend.position="bottom")

```


# Discussion

The replication rate in psychology is to the best estimates, somewhere around 50%, which is quite low. Many reasons for this low replication rate have been hypothesized, but studying it either experimentally or correlationally requires doing a number of replication experiments, which can be a lot of work. Here we took advantage of 11 years of graduate student replication projects to look at correlational predictors of replication in a previously-unused dataset. 

In line with previous replications, we found a FOOBAR replication rate, with some studies showing effect sizes similar to the original and others much smaller. Some individual correlates of replicability that stood out included within-subjects designs, work in the subfield of cognitive psychology, and the original and replication both using online samples.  Many of these correlates interrelate with one another, and we are still limited in our sample, so models with multiple predictors have a lot of uncertainty around the effects of each predictor. 

We are explicit in our goal of estimating how likely it is that a first-year graduate student, attempting to replicate a study of interest to them, gets replication results which are consistent enough with the original results that the student could build on the study in their own work. We can't estimate some platonic probability of truth for a study and neither can any replication study. Instead, we can estimate how likely studies are to get results of different levels of closeness in different circumstances. Much of the discussion around whether things should count as replication successes, or failures, or not replications at all is really about what the right thresholds are for closeness of results, and what the right circumstances are for a replication. Rather than try to find the one right answer to whether a direct replication was done in a certain right way, we think it's useful to explicitly label what the estimand really was: how likely it is to get the same results (under whatever metric) given circumstances like those used for the replication. 

In our case, our estimand is about how well first-year graduate students will do at the replication, which takes into account the limited time, limited budget, and limited experience. We think this is an important estimand, as much of the work of psychology is done by trainees, in circumstances like these. Thus, if some studies have delicate results that require large samples and very exact methods to achieve, they may not replicate under normal resources. 

Other replication projects have other estimands: @camerer2018 seems to ask something closer to how likely studies are to replicate when preformed by an expert with a large budget, and @ebersole2020 asks how likely certain studies are to replicate when performed with extensive feedback from the original authors.  

We do not interpret our results as saying that all non-replications were false positives (presumably some would be replicable under different implementations and budgets and others would not). There are many possible reasons for the non-replications in this sample. In some cases, it seemed that the problem may have been with the replication: for instance, if there were too few participants, or if there were high levels of wrong answers on attention checks, or participants speeding through without attention checks. For these cases, there was a clear next attempt that a student could make if they wanted to get the replication to work. In other cases, there might have been a priori reasons to distrust the original study results, such as exclusion criteria that seemed to be post-hoc, or a three-way interaction effect on a small sample (CITE THAT THIS IS SKETCH). In yet other cases, it's unclear why the results failed to replicate. 

[somehow transition here] Pedagogy is important for open science. It's one thing to require or incentivize scientists to use open science practices and conduct replicable and reproducible research, but using the right tools and workflows to do open science is something that has to be learned. Teaching it in the classroom addresses the knowing how point at the beginning and shows students how to have open science practices integrated in to their science at the beginning, before other habits can ossify. Doing replications give students the motivation to care about open science, as they see how much easier it is to implement the study with open materials versus the study where they have to make guesses about the study instructions from the methods section. In presenting work with classmates, students see that there is variation in how well studies replicate, with some replicating very cleanly and others not at all. This sort of first hand experience teaches that not everything they read in the literature may just work if done again. 

Our results are limited by the number and quality of the studies we included. These studies are not necessarily representative of the studies of interest to psychologists as a whole. The replication studies were not designed from the start with this analysis in mind, so analysis is limited by the choices and reporting used at the time of replications. TODO what are the limitations we want? 

TODO quick wrap up




# Methods
Our pre-registration, code, and coded data are available at TODO OSF REPO. 

## Dataset

```{r}
all <- read_csv(here("data", "raw_data.csv")) |> filter(include %in% c("stats", "pred_int", "exp")) 

ps <- all|> mutate(journal=str_to_lower(journal)) |>filter(journal=="psych sci")

```
The dataset of replication projects comes from class projects conducted in PSYCH 251 (earlier called PSYCH 254) a graduate-level experimental methods class taught by MCF from 2011 to 2022. This class is commonly taken by first year graduate students in psychology and related disciplines, and it has been a requirement of the Psychology PhD since around 2015. Each student chose a study to replicate, implemented the study, wrote analysis code, pre-registered their replication, ran the study, and turned in a structured final report including methods, analytic plan, changes from the original study, confirmatory and exploratory analyses, and discussion of outcomes. Students were encouraged to do experimental replications, but some students chose to replicate correlational outcomes or do computational reproducibility projects instead. We cannot include the full student reports for confidentiality reasons, but we include an example as well as the template given to students in the repo. TODO example and template

Students were free to choose what study they replicated; the recommended path for students who did not have their own ideas was to pick an interesting study from a recent year of Psychological Science (this led to a high fraction of Psych Science articles in the replication sample, `r nrow(ps)`, `r nrow(ps)/nrow(all)*100 |> round()`% of studies). 

We note that 4 (TODO check) of the replication projects were included in RP:P, and 10 of them were previously reported in @hawkins. 

## Coding procedure
We relied primarily on student reports to code the measured variables for the replications. We supplemented this with spreadsheets of information about projects from the time of the class and the original papers.

### Measures of replication success
Our primary replication outcome is experimenter and instructor rated replication success (0-1). The subjective replication success was recorded by the teaching staff for the majority of class replications at the time they were conducted. Where the values were missing they were filled in by MCF on the basis of the reports. For all studies, replication success was independently coded by VB on the basis of the reports. Where VB’s coding disagreed with the staff/MCF’s code, the difference was resolved by discussion between VB and MCF (`r pluck(matching,2,1)/176*100 |> round()`% of studies). These were coded on a [0, .25, .5, .75, 1] scale. 

This subjective replication outcome was chosen because it already existed, could be applied to all projects (regardless of type and detail of statistical reporting), and did not rely solely on one statistical measure. As a complement, we also identified a "key" statistical test for each paper (see below for details), and if possible, computed p_original and prediction interval at this statistic, following @errington2021. p_original was a continuous measure of the p-value on the hypothesis that the original and replication samples come from the same distribution. Prediction interval was a binary measure of whether the replication outcome fell within the prediction interval of the original outcome measure. 

### Demographic properties
We coded the subfield of the original study as a 4 way factor: cognitive psychology, social psychology, other psychology, and non-psychology. For each paper, we coded its year of publication, whether it had open materials, whether it had open data, and whether it had been conducted using an online, crowd-sourced platform (i.e. MTurk or Prolific). 

### Experimental design properties
We coded experimental design on the basis of student reports, which often quoted from the original methods, and if that did not suffice, the original paper itself. To assess the role of repeated measures, we coded the number of trials seen per participant, including filler trials and trials in all conditions, but excluding training or practice trials. 

We coded whether the manipulation in the study was instantiated in a single instance (“single vignette”). Studies with one induction or prime used per condition across participants were coded as having a single vignette. Studies with multiple instances of the manipulation (even if each participant only saw one) were coded as not being single vignette. While most studies with a single vignette only had one trial and vice versa, there were studies with a single induction and multiple test trials, and other studies with multiple scenarios instantiating the manipulation, but only one shown per participant.  

We coded the number of subjects, post-exclusions. We coded whether a study had a between-subjects, within-subjects, or mixed design; for analyses mixed studies were counted as within-subjects designs. In the analysis, we used a log-scale for number of subjects and numbers of trials. 

### Properties of replication
We coded whether the replication was conducted on a crowd-sourced platform; this was the norm for the class projects, but a few were done in person. As the predictor variable, we used whether the recruitment platform was changed between original and replication. This groups the few in-person replications in with the studies that were originally online and stayed online in a "no change" condition, in contrast with the studies that were originally in-person with online replications. 

We coded the replication sample size (after exclusions). This was transformed to the predictor variable log ratio of replication to original sample size. 

As a control variable, we included whether the original authors were faculty at Stanford at the time of the replication. This is to account for potential non-independence of the replication (ex. if replicating their advisor's work, students may have access to extra information about methods). 

We made note of studies to exclude from some of the sensitivity analyses, due to not quite aligned statistics, extremely small or unbalanced sample sizes, or where the key statistical measure the student chose was not of central importance to the original study. 

### Determination and coding of key statistical measure
For each study pair, we used one key measure of interest for which we calculated the predictor variables of p-value and effect size and the statistical outcome measures p_original and prediction interval. 
If the student specified a single key measure of interest and this was a measure that was reported in both the original paper and replication, we used that measure. If a student specified multiple, equally important, key measures, we used the first one. When students were not explicit about a key measure, we used other parts of their report (including introduction and power analysis) to determine what effect and therefore what result they considered key. In a few cases, we went back to the original paper to find what effect was considered crucial by the original authors. When the measures reported by the student did not cleanly match their explicit or implicitly stated key measure, we picked the most important (or first) of the measures that were reported in both the original and replication. These decisions could be somewhat subjective but importantly they were made without reference to replication outcomes. 

Whenever possible, we used per-condition means and standard deviations, or the test statistic of the key measure and its corresponding degrees of freedom (ex. T test, F test). We took the original statistic from the replication report if it quoted the relevant analysis or from the original paper if not. We took the replication statistics from the replication report. 

We then calculated p values, ES, p_orig, and predInt. We choose to recalculate p values and effect sizes from the means or test statistic rather than use reported measures when possible because we thought this would be more reliable and transparent. The means and test statistics are more likely to have been outputted programmatically and copied directly into the text. In contrast, p-values are often reported as <.001 rather than as a point value, and effect size derivations may be error prone. By recording the raw statistics we used and using our available code to calculate other measures, we are transparent, as the test statistics can be searched for in the papers, and all processing is documented in code. 

In some cases, p-values and or effect sizes were not calculable either due to insufficient reporting (ex. reporting a p-value but no other statistics from a test) or key measures where p-values and effect sizes did not apply (ex. PCA as measure of interest). Where studies reported beta estimates and standard errors or proportions, SMD isn't an applicable measure, but we were still able to calculate p_original and prediction interval. 

We separately coded whether the original and replication effects were in the same direction, using raw means and graphs. This is more reliable than the statistics because F-tests don't include the direction of effect, and some students may have flipped the direction in coding for betas or t-tests. In the processed data, the direction of the effect of the replication was always coded consistently with the original study’s coding, so a positive effect was in the same direction as the original and a negative effect in the opposite direction.  

In regressions, we used SMD and log p-value as predictors. 

## Modelling 

Due to the monotonic missingness of the data, we had more predictor variables and outcome variables for some original-replication pairs than others. To take full advantage of the data, we ran a series of models, with some models having fewer predictors, but more data, and others having more predictors, but more limited data. 

We ran a model predicting the subjective replication score on the basis of demographic and experimental predictors on the entire dataset; we ran two models predicting p_original and prediction interval from demographic and experimental predictors on the subset of data where we had p_original and prediction intervals. Then, on the smaller subset of the data where we had SMD and p-values, we re-ran these three models with those as additional predictor variables. 

The subjective replication scores were coded on [0, .25, .5, .75, 1], and we ramapped these to 1-5 to run an ordinal regression predicting replication score. We ran logistic regressions predicting prediction interval and linear regressions predicting p_original. 

All models used a horseshoe prior in brms. All models included random slopes for predictors nested within year the class occurred to control for variation between cohorts of students. We did not include any interaction terms in the models. All numeric predictor variables were z-scored after other transforms (e.g., logs) to ensure comparable regularization effects from the horseshoe prior. 

As a secondary sensitivity analysis, we examined the subset of the data where the statistical tests had the same specification, the result was of primary importance in the original paper (i.e. not a manipulation check), and there were no big issues with the replication.

Results from these models not reported in the main paper are reported in the supplement. 

# Acknowledgements {-}

Acknowledge people here. `{-}` useful to not number this section.

# References {-}

<!-- Use this magic to place references here. -->
<div id="refs"></div>

