---
title: PUT YOUR TITLE HERE
author:
  - name: Veronica Boyce
    affiliation: Stanford
    footnote:
      - corresp
  - name: Maya Mathur
    affiliation: Stanford
  - name: Michael C. Frank
    affiliation: Stanford
address:
  - code: Stanford
    address: Stanford University
footnote:
  - code: corresp
    text: "Corresponding author. Email: vboyce@stanford.edu"
bibliography: ["mybibfile.bib"] # Replace with one or more of your own bibtex files. Better BibTeX for Zotero is your friend
csl: dmk-format.csl # Use any CSL style. See https://www.zotero.org/styles for a good list. Ignored if citation_package: natbib
link-citations: TRUE
output:
  bookdown::pdf_document2:
    toc: FALSE
    keep_tex: TRUE
    template: generic_article_template.tex
    #md_extensions: "-autolink_bare_uris"
    number_sections: TRUE
    citation_package: default # Can also be "natbib"
lang: en # Main document language in BCP47 format
geometry: "margin=25mm"
papersize: a4
#linestretch: 2 # for double spacing
endfloat: FALSE # Set to TRUE to turn on latex endfloat package to place figures and tables at end of document
# endfloatoption: # See endfloat documentation for more possibilities
#   - tablesfirst # Default
#   - nomarkers # Default
numberlines: FALSE
authblk: TRUE # FALSE = author affiliations in footnotes; TRUE = author affiliations in a block below author names
footnotehyper: FALSE # TRUE will give you enhanced table footnote capabilities. Set to FALSE to be able to use French blocks. Needed due to what appears to be a latex bug.
urlcolor: blue
linkcolor: blue
citecolor: blue
graphics: TRUE # Needed to be able to include images
tables: TRUE # Needed to be able to include tables
# fancyhdr:
#   first:
#     #headleft: "REPORT-NO-XXXX"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 0pt
#     #footleft: A left foot
#     footrulewidth: 0pt
#   subsequent:
#     #headleft: "NEXT-PAGE-HEADER-LEFT"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 1pt
#     footrulewidth: 0pt

header-includes:
 - \usepackage{tikz}
 - \usetikzlibrary{positioning,chains}
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\textfraction}{0.00}
 - \renewcommand{\topfraction}{1}
 - \renewcommand{\bottomfraction}{1}
 - \renewcommand{\floatpagefraction}{1}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
#knitr::opts_chunk$set(fig.pos = 'th') # Places figures at top or here
knitr::opts_chunk$set(out.width = '100%', dpi=300,
                      fig.width=8, fig.width=8) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

options(knitr.table.format="latex") # For kable tables to work without setting format option

knitr::opts_chunk$set(echo=F, warning=F, message=F)#dev = "png", dev.args = list(type = "cairo-png")
 library("papaja")
library("bookdown")
library("rticles")
 library(here)
 library(tidyverse)
 library(brms)
library(tidybayes)
library(kableExtra)
library(viridis)
library(cowplot)
#r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")

model_location="code/models"

d <- read_csv(here("data", "processed_data.csv"))
```

<!---------------------- Abstract --------------------->

::: {.abstract data-latex="" lang=en}
TODO abstract
:::

<!-- Use class keywords to format keywords section -->
::: {.keywords data-latex="" lang=en}
One keyword; Yet another keyword
:::

<!------------ Main text -------------------->

# TODO list
- need to do supplement with the stuff we preregistered (sensitivity analysis, all models)
- somewhere need discussion of how nobody is estimating p(true), but we're just more honest about it 
- meanings and codings of predictors (perhaps copied from pre-reg) should go in methods
- how to introduce coding methods (outcome measures)
- add key figs and tables (max 8)
- many citations everywhere
- link to open stuffs

# introduction

[replication: it's a hot issue and concerns around replication have done a lot, see crisis/revolution] Replication is a hot topic. Some argue that it is a cornerstone of a cumulative science, and findings of low replication rates are a problem. 

[ there is controversy here, but to even discuss it, we need empirics] Even those who argue that replication is not so essential still rely on data from replications to make their cases that replication rates are not as low as they seem and that these replication rates are acceptable. 

[ there's ... not a lot of empirics] However, due to the arduous nature of collecting samples of replications, there have not been many large-scale replication efforts, so all of the argumentation around predictors of replications and field-wide replication rates is fit to a small number of data points. 

Many replications of individual effects have been performed, but these are less useful as pattern analysis because the varied sampling and reporting, and the fact they aren't all in one place. Also pub bias. 

We are aware of three large-scale replication efforts replicating experimental results in the existing psychology literature. The first is RP:P, which sampled roughly 100 studies from top psychology journals in 2008. They found an overall replication rate around 40%, which provided evidence to support growing concerns about the non-reliability of the literature. [idk, maybe worth framing that this was a big deal study when it came out]. TODO some discussion of how many papers have reanalysed this cutting the data different ways. 

The ManyLabs series of studies have also done large-scale replications of effects from psychology. Due to their primary goal of investigating different forms of hetereogeneity, their sampling has been non-representive, focussing on short studies with only two conditions. Across Many labs 1-3 foo bar replicated. Many labs 5 was a re-replicaiton attempt on 10 of RP:P and rescued 2/10. 

Camerer replicated the 21 behaviors studies published in Nature and Science from 2010-2015 that did not require special populations or special equipment. They found a roughly 60% replication rate. 

In addition to determining estimated overall replicaiton rates for fields and journals, there's also merit in knowing what features of experiments (and replication attempts) are predictive of replication success. Blah blah stakeholders and resources. There's some signal here, as people are able to predict replication success at above chance CITATIONS. RP:P looked at how replication rates varied across subsamples of their studies. They found that cognitive psychology studies replicated at higher, but still low rates (50% v 25%) compared to social psychology. They also found that larger effect sizes and smaller p-values of original studies were predictive of replicating. TODO do we talk about any other correlates. There are reasons to believe that experimental factors such as number of items or between and within subject designs may also be predictive (cite our old paper), and could pontentially be some of the reason for subfield differences. 

Here we introduce a new dataset of replications in the behavioral sciences, primarily psychology. Over the years 2011-2022, students in a graduate-level experimental methods class have conducted online replications as individual course projects. From this, we have 176 experimental replications that were codeable. This approximately doubles the set of experiments in the large-scale replication literature. We investigate predictors of replicability in this new dataset. 

## quick methods
Class, taught by last author, is aimed at first-year graduate students, but taken by ... Since around 2015, it has been required for incoming PhD students in the Psychology Department. Over the course of a quarter long class, students work through their individual replication projects, from choosing a study, to reimplementing it, to creating analysis code, piloting the study, pre-registering it, and finally running a full data collection and writing up their results. Students are free to choose studies related to their interests, although recent articles from Psychological Science are the recommended path under uncertainty (leading to a high proportion of Psych science articles in this replication sample). While the sampling procedure is non-random, it is generally representative of studies that are of interest to and doable by first year grad students. 

Studies vary along a number of dimensions, including statistical properties, subfield, and experimental properties. We leverage this naturally occurring variation to see if these properties predict replication success. 

# results


```{=latex}
\definecolor{bad}{HTML}{FFCCCB}
		\definecolor{meh}{HTML}{efefef}
			\definecolor{good}{HTML}{abcdff}
	\tikzset{
		mynode/.style={
			draw, rectangle, align=center, text width=4.5cm, scale=1, font=\small, inner sep=.5ex},
		arrow/.style={
		 very thick,->,>=stealth}
	}
	
\begin{figure}[ht]
	
	\begin{tikzpicture}[
		node distance=.8cm,
		start chain=1 going below,
		every join/.style=arrow,
		]
		\coordinate[on chain=1] (tc);
		\node[mynode, on chain=1, fill=meh] (n2)
		{\textbf{210} projects from 2011-2022};
		\node[mynode, join, on chain=1, fill=meh] (n3)
		{\textbf{189} original - replication pairs};
		\node[mynode, join, on chain=1, fill=meh] (n4)
		{\textbf{177} experimental pairs};
		\node[mynode, join, on chain=1, fill=good] (n5)
		{\textbf{176} pairs included};
		\node[mynode, join, on chain=1, fill=good] (n6)
		{\textbf{131} pairs with ES};
		\node[mynode, join, on chain=1, fill=good] (n7)
		{\textbf{107} pairs with SMD};
		
		
		\begin{scope}[start chain=going right]
			\chainin (n2);
			\node[mynode, join, on chain, fill=bad]
			{ \textbf{2} missing projects \\ \textbf{19} reproducibility projects};
			\chainin (n3);
			\node[mynode, join, on chain, fill=bad]
			{\textbf{12} non-experimental pairs};
			\chainin (n4);
			\node[mynode, join, on chain, fill=bad]
			{\textbf{1} pair missing sample size};
		\end{scope}
	\end{tikzpicture}
\caption{Which studies were excluded for what reasons, and how many original-replication pairs are left.}\label{fig:prisma}
\end{figure}

```


```{r smd, out.height="25%", fig.width=5, fig.height=3, fig.pos="ht", fig.cap="TODO caption here"}
d |> 
  filter(target_d_calc < 5, rep_d_calc < 5) |>
  ggplot(aes(x=target_d_calc, y=rep_d_calc, color=sub_rep))+
 
  geom_abline(slope=1, intercept=0, lty = 2)+
  geom_smooth(method = "lm") + 
   geom_point(size=2.5) +
  geom_hline(yintercept = 0, lty = 2)+
  scale_color_viridis(discrete=F, direction = 1, 
                      name = "Subjective\nReplication\nSuccess\n")+
  labs(color="subrep", x="Original SMD", y="Replication SMD") + 
  theme(legend.position = "right")

```

```{r mod-results, out.height="25%", fig.width=5, fig.height=3, fig.pos="ht", fig.cap="TODO caption here"}
tier1_subjective <- read_rds(here(model_location, "tier1_subjective.rds"))
tier1_subjective |> 
gather_draws(`b_[a-zA-Z_(): ]+`, regex=TRUE) |> mutate(Term=str_sub(.variable, 3,-1) |> str_replace("M","_"), Estimate=.value) |> mutate(Term=factor(Term, levels=c("subfieldsocial", "subfieldother_psych", "subfieldnon_psych", "is_within", "single_vignette", "change_platform", "open_data", "open_mat", "stanford", "z_pub_year", "z_log_trials", "z_log_sample", "z_log_ratio_ss"), labels=c("Social", "Other psych", "Non psych", "Within subjects", "Single vignette", "Switch to online", "Open data", "Open materials", "Stanford", "Publication year", "Log trials", "Log original sample size", "Log rep/orig sample"))) |> 
  ggplot(aes(y = reorder(Term, Estimate, mean), x = Estimate)) + geom_vline(xintercept=0)+
  stat_pointinterval(color="#440154")+labs(y="", x="Estimate, 66%, 95% CrI predicting subjective replication score")+theme(axis.text=element_text(size=11))

```

```{r}

for_cor <- read_csv(here("data","for_model.csv")) |> filter(include %in% c("stats", "exp", "pred_int")) |>
  select(sub_rep, 
         pub_year, subfield, open_data, open_mat, stanford, change_platform, log_ratio_ss, is_within, single_vignette, log_sample, log_trials) |> mutate(social=ifelse(subfield=="social", 1,0),other_psych=ifelse(subfield=="other-psych",1,0), non_psych=ifelse(subfield=="non-psych", 1,0)) 
sub_cor <- function(var, stat = "estimate") {
  if (stat == "estimate") {
    cor.test(for_cor$sub_rep, pull(for_cor, {{var}}))$estimate
  } else {
    cor.test(for_cor$sub_rep, pull(for_cor, {{var}}))$p.value
  }
}



preds <- c("pub_year", "open_data",  "open_mat", "stanford", "change_platform", 
           "log_ratio_ss", "is_within", "single_vignette", "log_sample", 
           "log_trials", "social", "other_psych", "non_psych")

cors <- tibble(preds = preds) |>
  mutate(r = sapply(preds, function(x) sub_cor(x, stat = "estimate")),
         p = sapply(preds, function(x) sub_cor(x, stat = "p"))) |> 
  mutate(Predictors=factor(preds, levels=c("social", "other_psych", "non_psych", "is_within", "single_vignette", "change_platform", "open_data", "open_mat", "stanford", "pub_year", "log_trials", "log_sample", "log_ratio_ss"), labels=c("Social", "Other psych", "Non psych", "Within subjects", "Single vignette", "Switch to online", "Open data", "Open materials", "Stanford", "Publication year", "Log trials", "Log original sample size", "Log rep/orig sample"))) |> arrange(r) |> select(Predictors, r, p)

library(kableExtra)
knitr::kable(cors, digits = 3, align='rcc',format="latex", position="!h", caption="TODO the correlation of individual predictors with subjective replication outcomes. For subfield, cognitive psychology is treated as the baseline condition.") |> kable_styling(full_width=F, font_size=10) 

```

```{r predictors-graph, out.width="100%", fig.width=8, fig.height=4, fig.pos="ht", fig.cap="y-axes are counts of studies in each category, with each subjective replication score. TODO caption here"}
dat <- read_csv(here("data","for_model.csv")) |> filter(include %in% c("stats", "exp", "pred_int")) |> mutate(sub_rep=as.factor(sub_rep))


sub_with_leg <- dat |> mutate(subfield=factor(subfield, levels=c("cognitive", "social", "other-psych", "non-psych"), 
                                              labels=c("Cognitive\npsychology", "Social\npsychology", "Other\npsychology", "Not\npsychology"))) |>
  ggplot(aes(x=subfield, fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.x=element_blank(), axis.title.y=element_blank())+
  labs(fill="Subjective\nreplication\nscore")

leg <- get_legend(sub_with_leg)

subfield <- sub_with_leg+theme(legend.position="none")

open <- dat |> mutate(openness=case_when(
  open_mat==1& open_data==1 ~ "Open materials \n Open data",
  open_mat==1 ~ "Open materials \n No open data",
  open_data==1 ~ "No open materials \n Open data",
  T ~ "No open materials \n No open data")) |> 
  ggplot( aes(x=openness |> factor(), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position="none")

modal <- ggplot(dat, aes(x=ifelse(change_platform,"Switch to online\n","Same modality\n"), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position="none")


within <- ggplot(dat, aes(x=ifelse(is_within, "Within \nsubjects", "Between\nsubjects"), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
  coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position="none")


vignette <- ggplot(dat, aes(x=ifelse(single_vignette, "Single \nvignette", "Multiple \nvignettes"), fill=sub_rep))+
  geom_bar(stat="count", position="dodge")+
  scale_fill_viridis(discrete=T)+
    coord_cartesian(ylim=c(0,55), expand=F)+
  theme(axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position="none")

top_row <- plot_grid(subfield, within, vignette, rel_widths=c(1, .6, .6), nrow=1)
mid_row <- plot_grid(open,  modal, leg, rel_widths=c(1.2, .6, .25), nrow=1)
plot_grid (top_row, mid_row, nrow=2)
```

As our primary outcome measure, we use a subjective measure of replication success. At the end of the class, the instruction team coded each project on a 0-1 scale indicating whether or not it replicated. This allows for nuance around marginal effects, covers studies with a broad range of statistical outcomes (including those that do not lend themselves to NHST), and includes studies with multiple important outcome measures. For robustness, first author independently coded all projects on the same scale off of the final write-ups. Disagreements (which occured foobar % of the time) were resolved by discussion between first and third authors. 

Over the years, there have been a number of student projects, of these we are able to include foobar (see Figure \ref{fig:prisma}). Because of variance in the experimental designs and reporting practices, we have variable statistical information. To address this, we conducted a series of models, balancing between including as many data points as possible, and including potentially relevant outcome and predictor variables. Here we focus on one set of models, the prediction of subjective replication outcome on the basis of the whole dataset without the statistical predictors. Other model results, including those from a sensitivity analysis, are available in the supplement. 

# discussion

[who knows where this paragraph should go] Replications are one way of approximating whether an effect in the target paper is true, and how likely results are to replicate in some sort of platonic ideal world. There is no one answer here; replication projects measure something else that could be treated as an approximation. However, what a replication is really measuring is how likely it to get that effect given some conditions. Which the right conditions are is a potential point of contention (see discussion around closeness and sample size, power etc).  Here, we are explicit in what sample of replicators and replication conditions were are sampling, and thus what we can generalize to. We are estimating how likely replication is when done online by a graduate student, under constraints that are typical for graduate students (limited time, limited budget). As much of scientific process is in fact performed by graduate students, we think this in itself is an interesting question. It contrasts with questions like how likely something is to replicate when performed by an expert with a large budget (perhaps this is the right framing on Camerer) or when performed with extensive feedback from the original authors, etc. 

We do not interpret our results as saying that all non-replications were false positives (presumably some are replicable under other circumstances and others are not). Some of the factors we look at are more easily interpreted as being about the original study than others. We do not assign causal explanation to the predictors because there are multiple plausible interpretations: they could be correlates of QRPs in the original, they could be correlates of harder-to-detect or more fragile effects, they could be correlates of less-close replications, they could even be correlates of another stronger predictor. 


# methods
Our pre-registration, code, and coded data is available at TODO OSF. 

## outcome measures
Our primary outcome of interest is experimenter and instructor rated replication success (0-1). We chose this outcome because A) it had been recorded for the majority of class replications at the time they were conducted, and B) it avoids some of the complex statistical issues associated with evaluating whether a replication is successful (e.g., is the p-value on the replication significant, do the confidence intervals not overlap, etc.).
As a complement, we will also use the following statistical outcomes as additional measures of replication success. These will be evaluated on one “key” statistical test per paper (typically identified by the student at the time of replication, but occasionally selected by our team if the student reported several key tests; see below). 
P_orig : what is the p value on the hypothesis that both results come from the same population
PredInt: is the replication effect size within the prediction interval of the original effect size
These measures are computed based on the replication project in Cancer Biology (RP:CB, Errington et al 2021). 

## coding procedure
In order to code the measured variables, we relied primarily on student replication reports. These were supplemented with spreadsheets for the class created at the time of the class and the original papers when student’s replications did not provide enough information or were unclear.

### replication success
The replication success was coded for many projects by the teaching staff in the year it was conducted. Replication success was independently coded by VB based on student’s replication reports -- this often but not always aligned with students’ assessments of whether the replication succeeded. Projects that were not given numeric codes by the teaching staff were coded for replication by MCF on the basis of the replication reports. Where VB’s coding disagreed with the staff/MCF’s code, the difference was resolved by discussion between VB and MCF. These were coded on a [0, .25, .5, .75, 1] scale. 

### Demographic properties
We coded the subfield of the original study as a 4 way factor: cognitive psychology, social psychology, other psychology, and non-psychology. For each paper, we coded its year of publication, whether it had open materials, whether it had open data, and whether it had been conducted using an online, crowd-sourced platform (i.e. MTurk or Prolific)

### Experimental design properties

Properties of the original experiment design were coded from the student’s replication reports which often included excerpts of the methods of the original. If this was insufficient, the original paper was referenced for clarification. To assess the role of repeated measures, we coded the number of trials seen per participant. We included filler trials and trials in all conditions, but excluded training or practice trials. We coded whether the manipulation in the study was instantiated in a single instance (“single vignette”). Studies with one induction or prime used per condition across participants were coded as having a single vignette. Studies with multiple instances of the manipulation (even if each participant only saw one) were coded as not being single vignette. While most studies with a signe vignette only had one trial and vice versa, there were studies with a single induction per condition, but multiple test trials, and studies with multiple scenarios instantiating the manipulation, only one scenario and 1 trial shown per participant.  

We coded the number of subjects, post-exclusions. We coded whether a study had a between-subjects, within-subjects, or mixed design; for analyses mixed studies were counted as within-subjects designs. 

### Properties of replication
For the replication, we coded whether the replication was conducted on a crowd-sourced platform. Nearly all replications were, but a handful were done in person. For models, we coded whether the recruitment platfrom was changed between original and replication. This groups the few inperson replications in with the studies that were originally online and stayed online in a "no change" condition, in contrast with the studies that were originally in-person with online replications. 

We coded the replication sample size (after exclusions). In our models, this was transformed to the log ratio of replication to original sample size. 

As a control variable, we included whether the original authors were faculty at Stanford at the time of the replication. This is to account for potential non-independence of the replication (ex. if replicating an advisor's work, students may have access to extra information about methods). 

We made note of studies that should be excluded from some of the sensitivity analyses, due to not quite aligned statistics, extremely small or unbalanced sample sizes, or key effects that were not key in the original. 

### Determination and coding of key statistical measure
For each paper, we used one “key measure of interest” for which we calculated the predictor variables of p-value and effect size and the statistical outcome measures p_orig and predInt. 
If the student specified a key measure of interest and this was a measure that was reported in both the original paper and replication, we used that measure. When the student specified a single key measure, we used that; if a student specified multiple, equally important key measures, we used the first one. When students were not explicit about a key measure, we used other parts of their report (including introduction and power analysis) to determine what effect and therefore what result they considered key. In a few cases, we went back to the original paper to find what effect was considered the crucial one by the original authors. In a few cases, the measures reported by the student did not cleanly match their explicit or implicitly stated key measure; in these cases we picked the most important (or first) of the measures that were reported in both the original and replication. These decisions can be somewhat subjective but importantly they were made without reference to replication outcomes.

Whenever possible, we used per-condition means and standard deviations, or the test statistic of the key measure and its corresponding degrees of freedom (ex. T test, F test). We took the original statistic from the replication report if it quoted the relevant analysis or from the original paper if not. We took the replication statistics from the replication report. 

We then calculated p values, ES, p_orig, and predInt. In cases where the test statistic was not sufficiently reported, but p-values and/or effect sizes were, we will use the reported p-value and or effect size. 

We choose to recalculate p values and effect sizes from the means or test statistic rather than use reported measures when possible because we think this approach is more reliable: The test statistic is more likely to have been outputted programmatically and copied directly into the text. In contrast, p-values are often reported as <.001 rather than as a point value, and effect size derivations may be error prone. By recording test statistics and using our available code to calculate other measures, we are transparent, as the test statistics are more likely to be unique and thus easy to search for in the papers, and all processing is documented in code. 

In some cases, p-values and or effect sizes were not calculable either due to insufficient reporting (ex. reporting a p-value but no other statistics from a test) or key measures where p-values and effect sizes did not apply (ex. PCA as measure of interest). In some other cases, both original and replication reported beta estimates for a coefficient along with the beta’s standard error or confidence interval. In these cases, we did not have the information to calculate standardized effect sizes, but we used the beta values to calculate p_orig and predInt. 

The direction of the effect of the replication was always coded consistently with the original study’s coding, so a positive effect was in the same direction as the original and a negative effect in the opposite direction. We separately coded whether the two effects were in the same direction, off of raw means or graphs. This is more reliable because F-tests don't include the direction of effect and also some students may have flipped the direction in coding for betas or t-tests. 



## modelling 

Due to the monotonic missingness of the data (see missingness discussion above), we have more predictor variables and outcome variables for some original-replication pairs than others. To take full advantage of the data, we will run a series of models, with some models having fewer predictors, but more data, and others having more predictors, but more limited data. 

In brief, these models are:

On all data:
Subjective replication score ~ demographic & experimental predictors
On subset of data with p_orig and pred_int:
P_orig ~ demographic & experimental predictors
In_pred_int ~ demographic & experimental predictors
On subset of data with standardized effect size: 
Subjective replication score ~ demographic & experimental & statistical predictors
P_orig ~ demographic & experimental & statistical predictors
In_pred_int ~ demographic & experimental & statistical predictors


Modelling approach 

The subjective replication scores were coded on [0, .25, .5, .75, 1], we will remap these to 1-5 and run an ordinal regression predicting replicate score. We will run logistic regressions predicting in_pred_int and linear regressions predicting P_orig. 

All models will use a horseshoe prior in brms. All models will include random slopes for predictors nested within years of the class (year) to control for variation between cohorts of students. We do not include any interaction terms in the models. All numeric predictor variables will be z-scored after other transforms (e.g., logs) to ensure comparable regularization effects from the horseshoe prior. 

Predictors:
General Demographics
Publication year 
Field (coded as a 4-way factor: cognitive v social v other-psych v not-psych)
Replication Closeness
Open data (binary variable)
Open materials (binary variable)
Stanford affiliation of original study (binary variable)
Change in platform as a binary scale (when both original and replication were on turk/prolific, or more rarely both were off turk/prolific are no-change; when original was off-turk/prolific but replication was on-turk/prolific are change)
Ratio of Replication sample size to original sample size: log (replication / original ) 

“Experimental” predictors
Predictors
Between versus within will be analyzed as a binary variable - for mixed designs, we will code these as within-participants if one component of the key analysis is within-participants. Our rationale is that there is some increase in precision of estimates based on having multiple observations per participant.
Single vignette is a binary variable
Number of trials per participant (coded on log scale)
Original sample size (coded on log scale)

“Statistical” Predictors
Predictors
Original p-value (coded on log scale)
Original effect size (coded on normal scale)

Sensitivity Analysis
As a secondary sensitivity analysis, we will examine the subset of the data where the statistical tests have the same specification, the result is of primary importance in the original paper (i.e. not a manipulation check), and there were no big issues with the replication. In other words, we will do an analysis where we exclude studies where something was “borderline”.  

Results of more models in supplement. 



# Acknowledgements {-}

Acknowledge people here. `{-}` useful to not number this section.

# References {-}

<!-- Use this magic to place references here. -->
<div id="refs"></div>

# (APPENDIX) Appendices {-}

<!-- The above header does not appear in output. Not exactly sure why. -->

# Appendix A {-}

Some appendix text.

# Appendix B {-}

More appendix text.
