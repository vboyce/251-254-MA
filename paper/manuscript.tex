% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  english,
  a4paper,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Eleven years of student replication projects provide evidence on the correlates of replicability in psychology},
  pdflang={en},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=25mm]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\usepackage{footnote} % For some unknown reason, footnotehyper clashes with French
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% dont indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

%%%%%%%% START HEADER PARTIAL %%%%%%%%%%%%

% Formatting of tables & knitr::kable and kableExtra functionality
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}

% Line numbering

% endfloat stuff

% fancyhdr pagestyle

% Environment for keywords
\makeatletter
\newcommand\keywordsname{Keywords}
\newenvironment*{keywords}[1][\keywordsname]{\if@twocolumn \else \small \quotation \fi \begin{center} \textbf{\textit{#1} \\}}{\end{center}\if@twocolumn \else \small \endquotation \fi}
\newenvironment*{keywordsinline}[1][\keywordsname]{\if@twocolumn \else \small \quotation \fi \begin{center} \textbf{\textit{#1}: }}{\end{center}\if@twocolumn \else \small \endquotation \fi}
\makeatother

% Environment for abstract that takes new abstract name
\newenvironment{renameableabstract}[1][\abstractname]{\let\oldabstractname\abstractname \renewcommand{\abstractname}{#1} \begin{abstract}}{\end{abstract} \renewcommand{\abstractname}{\oldabstractname}}

%%%%%%%% END HEADER PARTIAL %%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{positioning,chains}
\usepackage{setspace}\singlespacing
\renewcommand{\textfraction}{0.00}
\renewcommand{\topfraction}{1}
\renewcommand{\bottomfraction}{1}
\renewcommand{\floatpagefraction}{1}
\setcounter{topnumber}{3}
\setcounter{bottomnumber}{3}
\setcounter{totalnumber}{4}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifXeTeX
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{}
\else
  \usepackage[english,main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Eleven years of student replication projects provide evidence on the correlates of replicability in psychology}

%%%%%%% START AUTHOR PARTIAL %%%%%%%%%%%%%%%

%%%%% Authors, affiliations and author notes stuff %%%%%

% Macros for creating and referencing stored reference
\makeatletter
\def\MyNewLabel#1#2#3{\expandafter\gdef\csname #1@#2\endcsname{#3}}

\def\MyRef#1#2{\@ifundefined{#1@#2}{???}{\csname #1@#2\endcsname}}

\newcommand*\ifcounter[1]{%
  \ifcsname c@#1\endcsname
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
}
\makeatother

% Create labels for Addresses if the are given by code
\MyNewLabel{ADDRTXT}{Stanford}{Stanford University}

% Create labels for Footnotes if they are given by code
\MyNewLabel{ANOTETXT}{corresp}{Corresponding author. Email: \href{mailto:vboyce@stanford.edu}{\nolinkurl{vboyce@stanford.edu}}}

%%% Special footnotes for addresses and author footnotes
\usepackage{bigfoot}
\DeclareNewFootnote{Addr}[arabic] % Only used for NOT authblk
\DeclareNewFootnote{ANote}[fnsymbol]

%%% Address and author notes as a function of format %%%
 % Use authblk for affiliations %%%%%%%%%%%
\usepackage{authblk}

% Always separate by commas
\renewcommand\Authsep{, }
\renewcommand\Authand{, }
\renewcommand\Authands{, }

% Counter for addresses and footnotes
\newcounter{addrcnt}

% thanks definition that doesnt produce superscript marks
\makeatletter
\newcommand*\createaddrlblbycode[1]{%
  \ifcounter{ADDRLBL@#1}
    {}
    {\refstepcounter{addrcnt}\newcounter{ADDRLBL@#1}\setcounter{ADDRLBL@#1}{\value{addrcnt}}}%
}

\newcommand*\addrlblbycode[1]{\arabic{ADDRLBL@#1}}

\newcommand*\addrbycode[1]{%
  \ifcounter{ADDR@#1}
    {}
    {\newcounter{ADDR@#1}%
     \affil[\addrlblbycode{#1}]{\MyRef{ADDRTXT}{#1}}}%
}

\newcommand*\createanotelblbycode[1]{%
  \ifcounter{ANOTELBL@#1}
    {}
    {\refstepcounter{footnoteANote}\newcounter{ANOTELBL@#1}\setcounter{ANOTELBL@#1}{\value{footnoteANote}}}%
}

\newcommand*\anotelblbycode[1]{\fnsymbol{ANOTELBL@#1}}

\newcommand*\anotebycode[1]{%
  \ifcounter{ANOTE@#1}
    {}
    {\newcounter{ANOTE@#1}%
     \footnotetextANote[\value{ANOTELBL@#1}]{\MyRef{ANOTETXT}{#1}}}%
}
\makeatother


\createaddrlblbycode{Stanford}


\createanotelblbycode{corresp}

\author[%
\addrlblbycode{Stanford}%
,%
$\anotelblbycode{corresp}$%
]{Veronica Boyce}

\addrbycode{Stanford}


\createaddrlblbycode{Stanford}



\author[%
\addrlblbycode{Stanford}%
]{Maya Mathur}

\addrbycode{Stanford}


\createaddrlblbycode{Stanford}



\author[%
\addrlblbycode{Stanford}%
]{Michael C. Frank}

\addrbycode{Stanford}


%endif(authblk)

%%%%%%%%% END AUTHOR PARTIAL %%%%%%%%

\date{}

\begin{document}
\maketitle

%%%%%%%%%% START AFTER TITLE PARTIAL %%%%%%%%%%%%%
\anotebycode{corresp}


%%%%%%%%%% END AFTER TITLE PARTIAL %%%%%%%%%%%%%


\begin{otherlanguage}{english}

\begin{abstract}
Cumulative scientific progress requires empirical results that are robust enough to support theory construction and future extensions.
Yet in psychology, some prominent individual findings have failed to replicate, and large-scale investigations suggest that failures to replicate are widespread.
What predicts which studies will replicate?
The identification of features that predict replication success is limited by data, however: most studies use a common dataset of only \textasciitilde170 studies, which vary in how they define replication.
We introduce a new dataset of 176 replications conducted by students as part of a graduate-level experimental methods course.
We examine how often students were able replicate their chosen studies' results closely enough that they would be ready to build on it in future work.
Using this as a metric of replicability, 49\% of the studies in our sample replicate, with studies with larger original effect sizes and within-participants designs being more likely to replicate.
Consistent with prior reports, our results indicate that the robustness of the psychology literature is low enough to limit progress by student investigators.

\end{abstract}

\end{otherlanguage}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Cumulative science requires foundational empirical results that are robust enough to support theory construction and future experimental extension. Scientists often treat publication in a scientific journal as a signal that a result was (likely to be) true, which leads scientists to build on findings without verifying them first. The replication crisis called into question the tacit assumption that most of the literature was robust and would replicate. Certain prominent findings failed to replicate in multi-site replication attempts (ex. terror management theory, \protect\hyperlink{ref-klein2022}{Klein et al. 2022}, ego-depletion, \protect\hyperlink{ref-hagger2016}{Hagger et al. 2016}), and a large-scale replication project pegged the replication rate for findings in top-tier psychology journals at around 40\% (\protect\hyperlink{ref-openscienceconsortium2015}{Open Science Consortium 2015}). When the trust in the literature mismatches the robustness results, many scientists can waste time and resources becoming frustrated pursuing extensions to a popular, but unreliable result. This seems to be the current situation of the literature, which leads to question: what do we do to minimize the waste?

From a policy perspective, we need to understand the landscape of the problem before we can have research-informed science policy interventions. Thus, we might want to know 1) what quantity to measure if we want to capture frustration and waste due to non-cumulative findings and 2) from a descriptive perspective, what types of studies are most at risk for this issue. Such measurements have been the focus of much prior work, but there is limited consensus on what is being estimated.

Three large-scale investigations have addressed replication rates in psychology. Reproducibility Project: Psychology, mentioned above, sampled roughly 100 studies from articles published in three top psychology journals in 2008 and they found an overall replication rate of around 40\% (\protect\hyperlink{ref-openscienceconsortium2015}{Open Science Consortium 2015}). ManyLabs investigated heterogeneity using short target studies that compared between two conditions each. These study designs were not representative of the psychology literature as a whole, and due to the heterogeneity goal, they had large overall samples across multiple sites. Across Many Labs 1-3, in these high-powered replications, only 29 of 51 target effects (57\%) replicated (\protect\hyperlink{ref-klein2014}{Klein et al. 2014}, \protect\hyperlink{ref-ebersole2016}{Ebersole et al. 2016}, \protect\hyperlink{ref-klein2018}{Klein et al. 2018}). Camerer et al. (\protect\hyperlink{ref-camerer2018}{2018}) replicated all 21 behavioral social science studies from Nature and Science from 2010-2015 that were feasible; they consulted original authors and had high power to detect effects smaller than those reported. In this well-resourced environment with expert input, the replication rate was around 60\%. These studies all measured replication rates, but the details of their methods and outcome measures varied.

While their sampling and methods varied, these previous approaches to replicability have focused on interpreting their results in terms of a potentially problematic estimand: the probability of a finding in the literature being somehow truly replicable. Critics have pointed out that ``true'' replicability may not be possible to estimate outside of a specific sample (\protect\hyperlink{ref-vanbavel2016}{Van Bavel et al. 2016}) or even time period (\protect\hyperlink{ref-ramscar}{Ramscar et al. n.d.}). Further, the methods for estimating this quantity have been theoretically problematic. Sampling schemes did not reflect an entirely random sample from the literature; instead replication projects sampled from specific journals where results may be of more interest and adjusted the sample for feasibility concerns. These are reasonable sampling choices, but they undermine the claim that the estimand is the level of ``truth'' in the literature as a whole. Sampling truly at random from the literature may not even be desirable, as arguably a literature will succeed if useful discoveries come out of it, not if random findings are true (\protect\hyperlink{ref-wilson2020}{Wilson et al. 2020}). The importance of a study being replicable is not uniformly distributed across the literature.

Rather than aim for some measure of ``true'' replicability, it makes more sense to contextualize replication efforts based on their methods and outcome measures. With this lens, we could say that RP:P is looking at how well findings in well-known journals can be replicated in typical circumstances. Many Labs looked at which well-known two conditions studies replicate with huge samples, and Camerer et al. (\protect\hyperlink{ref-camerer2018}{2018}) was looking at what replicates when conducted in a highly-resourced environment with expert involvement. All of these are pontentially desirable estimands depending on what the specific circumstances and questions of interest are.

From the point of view of enabling cumulative science, an estimand of interest is how well work can be replicated by students, as students conduct most scientific work (\protect\hyperlink{ref-frank2022}{\textbf{frank2022?}}). In our current study, we use student success at replicating an effect as our estimand. In particular, our estimand is the probability that a researcher, on selecting a finding of interest from the literature, can successfully achieve a result satisfactorily close enough to the original that they can build on it in their own work, with all the necessary compromises to the methods and sample of the original that may be required by the constraints of the situation.

Our replication approach reflects this estimand. Our sample of studies is selected based on what studies students were interested in and wanting to replicate, with some filtering for feasibility; this sampling reflects how scientists choose what studies to build on: those that are interesting and relatively doable given methodological and budgetary constraints.

We use a subjective replication score as our primary metric. Whether one feels confident in the results of a study given a replication is not always dependent on only one outcome measure (ex. interaction term) and particularly not dependent on only one statistical comparison between the two studies (ex. replication is p\textless.05 same direction as original). This metric avoids bright line distinctions and accommodates the range of outcome measures in our diverse set of studies.

All our replications were conducted under short time scales and relatively small budgets, paralleling the constraints many scientists are under when starting new projects. From the point of view of cumulative science, researchers want to know if they can get paradigms to work under their real-world constraints.

Despite variation in how large-scale replication studies have defined replication successes, there is work suggesting some consistency in the predictors of replication success. Knowing these predictors is important for appropriately calibrating how we read the literature and for eventually identifying targets for interventions.

Prediction markets and elicitations have established that people can predict what studies will replicate above chance (\protect\hyperlink{ref-dreber2015}{Dreber et al. 2015}, \protect\hyperlink{ref-camerer2018}{Camerer et al. 2018}, \protect\hyperlink{ref-forsell2019}{Forsell et al. 2019}, \protect\hyperlink{ref-hoogeveen2019}{Hoogeveen et al. 2019}), but have not identified concrete predictors that differentiate replications from non-replications. Using machine learning approaches, Altmejd et al. (\protect\hyperlink{ref-altmejd2019}{2019}) examined statistical and demographic features of studies and identified larger sample sizes, larger effect sizes, and simple effects (as opposed to interaction terms) as predictive of replication. Open Science Consortium (\protect\hyperlink{ref-openscienceconsortium2015}{2015}) looked at correlates of replicability in the RP:P sample and found that studies in cognitive psychology (as opposed to social psychology) and studies with larger effect sizes and smaller p-values were more likely to replicate. One potential set of correlates to replicabilty that has not been thoroughly examined are experimental design features, such as between or within subjects designs and repeated measures.

Experimental approaches can be used to test potential interventions. Protzko et al. (\protect\hyperlink{ref-protzko2020}{2020}) showed, across 16 studies, that better methodological practices led to replication rates that matched theoretical expectations with replication effect sizes comparable with the original. Many Labs 5 added expert (original author) advice to a replication process, and found that it did not substantially increase the replication rate for the 10 studies (\protect\hyperlink{ref-ebersole2020}{Ebersole et al. 2020}). These types of experiments are valuable for testing potential causes of non-replication, but they don't scale well due to expense, and not all influences on non-replication may be experimentally manipulable.

Given the limitations of experimental approaches, we have to rely on correlational approaches for now. Correlational approaches depend on data from replications, generally drawing heavily from the same small set of data points. In particular, the RP:P dataset itself is much discussed and reanalyzed (\protect\hyperlink{ref-anderson2016}{Anderson et al. 2016}, \protect\hyperlink{ref-etz2016}{Etz \& Vandekerckhove 2016}, \protect\hyperlink{ref-gilbert2016}{Gilbert et al. 2016}, \protect\hyperlink{ref-patil2016}{Patil et al. 2016}) to the point that much of what we think we know about replicability may be overfit to the 100 studies included in in RP:P. We need more data to find potential targets of concern and intervention.

Our contribution is a new dataset of 176 replications of experimental studies from the social sciences, primarily psychology. These replications were conducted by students in graduate-level experimental methods class between 2011 and 2022 as individual course projects. We investigated statistical and experimental-design predictors of replicability in this dataset and found that within-subjects designs and studies with large standardized effect sizes were positively correlated with replication success.

\hypertarget{results}{%
\section{Results}\label{results}}

\definecolor{bad}{HTML}{FFCCCB}
        \definecolor{meh}{HTML}{efefef}
            \definecolor{good}{HTML}{abcdff}
    \tikzset{
        mynode/.style={
            draw, rectangle, align=center, text width=4.5cm, scale=1, font=\small, inner sep=.5ex},
        arrow/.style={
         very thick,->,>=stealth}
    }
    
\begin{figure}[ht]
    
    \begin{tikzpicture}[
        node distance=.8cm,
        start chain=1 going below,
        every join/.style=arrow,
        ]
        \coordinate[on chain=1] (tc);
        \node[mynode, on chain=1, fill=meh] (n2)
        {\textbf{210} projects from 2011-2022};
        \node[mynode, join, on chain=1, fill=meh] (n3)
        {\textbf{189} original - replication pairs};
        \node[mynode, join, on chain=1, fill=meh] (n4)
        {\textbf{177} experimental pairs};
        \node[mynode, join, on chain=1, fill=good] (n5)
        {\textbf{176} pairs included};
        \node[mynode, join, on chain=1, fill=good] (n6)
        {\textbf{136} pairs with some effect size};
        \node[mynode, join, on chain=1, fill=good] (n7)
        {\textbf{112} pairs with standardized effect size};
        
        
        \begin{scope}[start chain=going right]
            \chainin (n2);
            \node[mynode, join, on chain, fill=bad]
            { \textbf{2} missing projects \\ \textbf{19} reproducibility projects};
            \chainin (n3);
            \node[mynode, join, on chain, fill=bad]
            {\textbf{12} non-experimental pairs};
            \chainin (n4);
            \node[mynode, join, on chain, fill=bad]
            {\textbf{1} pair missing original sample size};
        \end{scope}
    \end{tikzpicture}
\caption{Of the 210 projects conducted for the class, 176 are included in our analysis, after excluding reproducibilty projects (with no new data collection), non-experimental replications, and missing projects. Of the 176, 136 report sufficient information to numerically compare outcome measures, and 112 report enough to use statistical predictors. TODO BETTER WAY TO LABEL AND TALK ABOUT THEIR 2 AND 3}\label{fig:prisma}
\end{figure}

PSYCH 251 is Stanford Psychology's graduate-level experimental methods class taught by MCF. During the 10 week class, each student replicated a published finding. They individually re-implemented the study, wrote analysis code, pre-registered their study, collected data using an online platform, and wrote up a structured replication report. Students were free to choose studies related to their research interests, with the default recommendation being an article from a recent year of Psychological Science. The resultant sample of studies is not a random sample from the literature but is representative of studies that are of interest to and doable by first year graduate students.

The sample of replicated studies reflects the variability of the literature, including studies from different subfields, with different experimental methods and statistical outcomes. We leveraged the naturally occurring variability in this sample of replications to examine how different demographic, experimental design, and statistical properties predict replication success.

Many different measures can be used to define replication success of an individual statistical result (\protect\hyperlink{ref-simonsohn2015}{Simonsohn 2015}, \protect\hyperlink{ref-gelman2018}{Gelman 2018/ed}, \protect\hyperlink{ref-mathur2020}{Mathur \& VanderWeele 2020}). However, a single test of a single statistical result doesn't capture the sense of whether a replication is close enough that one could feel confident extending the finding, so we used a subjective rating of replication success as our primary outcome measure. This subjective measure, unlike statistical measures, accommodated studies with multiple important outcome measures that together defined the pattern of interest and was applicable across the diverse range of statistical measures and reporting practices present in the sample. Importantly, a holistic measure of replication success had already been coded for most projects when they were turned in at the end of the class. For reliability, VB independently code the replication success from the students' written reports; discrepancies were resolved by discussion between MCF and VB (26\% of cases).

As a complement to our primary subjective outcome, we also used two statistical measures of replication on the subset of the data where they were computable for the key statistic of interest (136 cases, see Figure \ref{fig:prisma}). We used p-original, the p-value on the null hypothesis that the original and replication statistics are from the same distribution, and prediction interval, a binary measure of whether the replication statistic fell within the prediction interval of the original statistic (\protect\hyperlink{ref-errington2021}{Errington et al. 2021}). These both measure how similar the outcome estimates from the two sets of data are.

\begin{figure}[ht]
\includegraphics[width=1\linewidth,height=0.25\textheight]{manuscript_files/figure-latex/smd-1} \caption{Relationship between effect size of the original study, effect size of the replication study, and subjective replication success rating, for those studies where effect size was applicable.}\label{fig:smd}
\end{figure}

\hypertarget{overall-replication-rate}{%
\subsection{Overall replication rate}\label{overall-replication-rate}}

Across the 176 studies, the overall subjective replication rate was 49\%. 45\% (61/136) of the studies had replication outcomes within the prediction interval of the original outcome. The median p\_original value was 0.03. Figure \ref{fig:smd} shows the relationship between original standardized effect size, replication effect size, and subjective replication score. Some studies replicated withed similar size effects to the original, and others failed to replicate, with replication effect sizes near zero. On average, there was a diminution of effect sizes from original to replication.

\begin{table}[!h]

\caption{\label{tab:cor}The correlation of individual predictors with subjective replication outcomes. For subfield, cognitive psychology is treated as the baseline condition. See Methods for how these variables were coded.}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{r|c|c}
\hline
Predictors & r & p\\
\hline
Within subjects & 0.333 & 0.000\\
\hline
Log trials & 0.182 & 0.015\\
\hline
Open data & 0.150 & 0.047\\
\hline
Non psych & 0.080 & 0.294\\
\hline
Other psych & 0.075 & 0.322\\
\hline
Publication year & 0.064 & 0.399\\
\hline
Open materials & 0.002 & 0.979\\
\hline
Stanford & -0.027 & 0.725\\
\hline
Log rep/orig sample & -0.047 & 0.536\\
\hline
Log original sample size & -0.108 & 0.155\\
\hline
Switch to online & -0.158 & 0.037\\
\hline
Social & -0.246 & 0.001\\
\hline
Single vignette & -0.267 & 0.000\\
\hline
\end{tabular}
\end{table}

\hypertarget{single-predictors}{%
\subsection{Single predictors}\label{single-predictors}}

We investigated what features of the original study and replication were correlated with replication success, with the goal of being able to identify potential markers of replicability. We chose a set of predictor variables based on the correlational results of RP:P (\protect\hyperlink{ref-openscienceconsortium2015}{Open Science Consortium 2015}), our own intuitions of experimental design factors that might impact replication success, and some covariates related to how close the replication was. A full description of these features is given in Methods.

Many predictors individually correlate with subjective replication success (Table \ref{tab:cor}). Predictors of higher replicability included within-subjects designs, larger numbers of trials, and the original study having open data. Predictors of lower replicability included single vignetted studies with only one induction or example per condition, social psychology studies, and original-replication pairs where the original study was in-person and replication switched to online.

Distributions of study outcomes across some of these properties are shown in Figure \ref{fig:predictors-graph}. Both social and cognitive psychology studies were well represented in the replication sample, and the cognitive psychology studies replicated at 2.45 times the rate of social psychology studies. Within and between subjects designs were both common, and within-subjects designs replicated 3.35 times as much. Studies with multiple vignettes replicated 2.62 times more than single vignetted studies. However, there were strong correlations among these experimental features and between these experimental features and subfield.

Studies with open data, which almost always also had open materials, tended to replicate more than studies without open data. Nearly all replications studies were conducted online, but original studies were split between using in-person and online recruitment. Replications that switched to online were less likely to replicate than those that had the same modality as the original (generally both online, in a few cases both in-person). While online studies in general show comparable results to studies conducted in person (\protect\hyperlink{ref-crump2013}{Crump et al. 2013}), switching the modality does decrease the closeness of the replication, and some studies done in person may not have been well-adapted (ex. inductions may be weaker or attention checks inadequate to the new sample). These factors of open materials, open data, and online samples in original studies are more common in more recent studies, and so these effects may partially reflect temporal trends.

\begin{figure}[ht]
\includegraphics[width=1\linewidth]{manuscript_files/figure-latex/predictors-graph-1} \caption{Distribution of subjective replication scores within categories. Bar heights are counts of studies.}\label{fig:predictors-graph}
\end{figure}

\hypertarget{regression-model}{%
\subsection{Regression model}\label{regression-model}}

While a number of predictors show individual correlations with the subjective replication score, many of the predictors intercorrelate with one another. In order to determine which predictors were the strongest, we ran a series of pre-registered regularized regression models (see Methods for details; see Supplement for all estimates from all models). We ran models both using all the data, but without statistical predictors (that were uncodable for some studies) and models including statistical predictors, but limiting to the subset of data where all predictors were available. The coefficient estimates from two models predicting the subjective replication scores are shown in Figure \ref{fig:mod-results}. Due to a large number of predictors coupled with a small and noisy dataset there is much uncertainty around the coefficients even with strong regularization. The general directions of coefficients are consistent with the effects of the predictors in isolation.

Within-subjects designs stand out as the strongest indicator of replicability in the model without statistical predictors (0.55, CrI= {[}-0.01, 2{]}). When statistical predictors are added to the model, within-subjects designs remain predictive (0.64, CrI= {[}-0.03, 2.38{]}). Standardized effect size is another strong predictor of subjective replication score (0.59, CrI= {[}0.31, 2.58{]}). Both effects are robust to a sensitivity analysis including only studies with close replications and matching statistical tests (within-subjects 0.87, CrI= {[}-0.01, 3.34{]}; effect size 0.97, CrI= {[}0.76, 4.59{]}).

We also ran models predicting our secondary outcome measures: whether the replication effect was within the prediction interval as the original effect and what the p-original was between the replication and original. Both these models had even more uncertain estimates. While the credible intervals were wide, the general patterns of predictor direction and relative strength were similar to the subjective replication models. The strongest predictors were still within-subjects designs (0.71, CrI= {[}-0.12, 2.38{]}) and studies with larger effect sizes (0.3, CrI= {[}-0.31, 0.89{]}).

\begin{figure}[ht]
\includegraphics[width=0.9\linewidth]{manuscript_files/figure-latex/mod-results-1} \caption{Coefficients and uncertaintly estimates from a model of all predictors (N=112, shown in blue) and non-statistical predictors (N=176, shown in red) predicting subjective replication scores as the dependent variable. }\label{fig:mod-results}
\end{figure}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{question-summary-and-takehomes}{%
\subsection{question, summary, and takehomes}\label{question-summary-and-takehomes}}

NEED TO REDO!
MAY NEED SMOOTHER TRANSITIONS THROUGHOUT
Prior large scale replications of psychology findings have estimated the replication rate at somewhere around one half, which intuitively feels low. However, limited numbers of replications and limited research into specific predictors of replication failure mean that the reasons for non-replications are not well understood.

Overall, we take a functional approach to assessing replicability, by framing both our methods and interpretation around the idea of whether work can be repeated or built on by an early-career scientist.

Here we took advantage of 11 years of graduate student replication projects to look at correlational predictors of replication in a previously-unused dataset. In line with previous results, we found a 49\% replication rate, with some studies showing effect sizes similar to the original and others much smaller. When we looked at individual correlates of replicability, within-subjects designs, work in the subfield of cognitive psychology, and the original and replication both using online samples stood out as the strongest correlates. As many of these predictors interrelate with one another, we ran regularized regressions with all the predictors at once. Due to our small sample, model estimates were uncertain, but within-subjects designs and large original effect sizes were the strongest predictors.

The same issue that may cause a study not to replicate under constrained circumstances (despite hypothetically replicating under more favorable circumstances such as expert administration or larger budgets) will also plague attempts to build off those studies in constrained circumstances. Thus, we believe it is relevant to estimate replicability under the limitations of real world resource and expertise constraints.
\#\# intepretation - replication in light of the goal

Replication results should be interpreted in light of their methods and estimand.

Given our sample, we are able to estimate the correlates of studies replicating when the replications are conducted online, by graduate students with limited time and budget. We do not interpret our non-replications as indicating the original results were false positives (presumably some were and some weren't). There are many possible reasons for the non-replications in this sample. In some cases, the problem may be with the replication, such as too few participants, many exclusions for failed attention checks, or participants speeding through the study. When these issues are diagnosed, they suggest possible ways to ``rescue'' the replication by increasing the sample or changing the interface, without altering the underlying experiment; thus, while the replication did not succeed, after some troubleshooting, students may still be able to extend the work in the future. In other cases, there were a priori reasons to distrust the original study, such as exclusion criteria that seemed post-hoc or high-order interaction terms with a small sample. That said, not all scientists recognize the same factors as potential indications of low power or questionable research practices; students conducting these replications generally expected them to succeed. In many non-replication cases, it was unclear why the results failed to replicate.

\hypertarget{limitations}{%
\subsection{limitations}\label{limitations}}

We conducted an observational study of replication attempts, and our results are limited by the studies we included, which are limited in number and may not be representative of the studies of interest to psychologists as a whole. Our predictor variables were not manipulated, so they cannot be interpreted as causing (non-) replication, but only as correlational markers. Some of the correlates are most easily interpreted as being about the original study, and others reflect the closeness of the replication to the original. For instance, while within-subjects designs are more likely to replicate than between-subjects designs, this could be related to power, or the types of experiments that tend to be run in each design. Given the predictive value, slightly more skepticism and critical reading of between-subjects designs may be warranted, but this correlation, by itself, does not mean scientists should prefer to run within-subjects designs.

\hypertarget{conclusion}{%
\subsection{conclusion}\label{conclusion}}

Large scale replications are costly and arduous to run. The batch of replications presented here were pedagogical replications, done as part of a class. Trainee behavioral scientists need to learn experimental methods, and conducting replications as part of methods classes serve a dual purpose: they enables students to learn to do experiments in a scaffolded way, and they lead to more useful results than if students designed their own experiments from scratch on the same timescale (\protect\hyperlink{ref-frank2012}{Frank \& Saxe 2012}, \protect\hyperlink{ref-wagge2019}{Wagge et al. 2019}, \protect\hyperlink{ref-quintana2021}{Quintana 2021}, \protect\hyperlink{ref-hawkins}{Hawkins et al. n.d.}). Pedagogy has an important role to play in open science more broadly -- it's one thing to require or incentivize certain practices, but the tools and workflows of open science have to be learned. Teaching these skills and giving students an opportunity to practice using open, reproducible workflows shows students how to integrate open science practices into their workflows, before other habits can ossify. In doing replications, students may be motivated to value transparent practices, as they either struggle to reimplement studies from vague methods or appreciate the ease of working with available materials and analysis code. In presenting work with classmates, students see that there is variation in how well studies replicate, with some replicating very cleanly and others not at all. This sort of first hand experience teaches students that not everything they read in the literature may just work if done again.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

Our pre-registration, code, and coded data are available at TODO OSF REPO.

\hypertarget{dataset}{%
\subsection{Dataset}\label{dataset}}

The dataset of replication projects comes from class projects conducted in PSYCH 251 (earlier called PSYCH 254) a graduate-level experimental methods class taught at Stanford by MCF from 2011 to 2022. This class is commonly taken by first year graduate students in psychology and related disciplines, and it has been a requirement of the Psychology PhD since around 2015. Each student chose a study to replicate, implemented the study, wrote analysis code, pre-registered their replication, ran the study, and turned in a structured final report including methods, analytic plan, changes from the original study, confirmatory and exploratory analyses, and discussion of outcomes. Students were encouraged to do experimental replications, but some students chose to replicate correlational outcomes or do computational reproducibility projects instead. We cannot include the full student reports for confidentiality reasons, but we include an example as well as the template given to students at TODO example and template.

Students were free to choose what study to replicate; the recommended path for students who did not have their own ideas was to pick an interesting study from a recent year of Psychological Science (this led to a high fraction of Psych Science articles in the replication sample, 80, 45\% of studies).

We note that 4 (TODO check) of the replication projects were included in RP:P, and 10 of them were previously reported in Hawkins et al. (\protect\hyperlink{ref-hawkins}{n.d.}).

\hypertarget{coding-procedure}{%
\subsection{Coding procedure}\label{coding-procedure}}

We relied primarily on student reports to code the measured variables for the replications. We supplemented this with spreadsheets of information about projects from the time of the class and the original papers.

\hypertarget{measures-of-replication-success}{%
\subsubsection{Measures of replication success}\label{measures-of-replication-success}}

Our primary replication outcome was experimenter and instructor rated replication success. The subjective replication success was recorded by the teaching staff for the majority of class replications at the time they were conducted. Where the values were missing they were filled in by MCF on the basis of the reports. For all studies, replication success was independently coded by VB on the basis of the reports. Where VB's coding disagreed with the staff/MCF's code, the difference was resolved by discussion between VB and MCF (26\% of studies). Subjective replication scores were coded on a {[}0, .25, .5, .75, 1{]} scale.

This subjective replication outcome was chosen because it already existed, could be applied to all projects (regardless of type and detail of statistical reporting), and did not rely solely on one statistical measure. As a complement, we also identified a ``key'' statistical test for each paper (see below for details), and if possible, computed p\_original and prediction interval at this statistic, following Errington et al. (\protect\hyperlink{ref-errington2021}{2021}). p\_original was a continuous measure of the p-value on the hypothesis that the original and replication samples come from the same distribution. Prediction interval was a binary measure of whether the replication outcome fell within the prediction interval of the original outcome measure.

\hypertarget{demographic-properties}{%
\subsubsection{Demographic properties}\label{demographic-properties}}

We coded the subfield of the original study as a 4 way factor: cognitive psychology, social psychology, other psychology, and non-psychology. For each paper, we coded its year of publication, whether it had open materials, whether it had open data, and whether it had been conducted using an online, crowd-sourced platform (i.e.~MTurk or Prolific).

\hypertarget{experimental-design-properties}{%
\subsubsection{Experimental design properties}\label{experimental-design-properties}}

We coded experimental design on the basis of student reports, which often quoted from the original methods, and if that did not suffice, the original paper itself. To assess the role of repeated measures, we coded the number of trials seen per participant, including filler trials and trials in all conditions, but excluding training or practice trials.

We coded whether the manipulation in the study was instantiated in a single instance (``single vignette''). Studies with one induction or prime used per condition across participants were coded as having a single vignette. Studies with multiple instances of the manipulation (even if each participant only saw one) were coded as not being single vignette. While most studies with a single vignette only had one trial and vice versa, there were studies with a single induction and multiple test trials, and other studies with multiple scenarios instantiating the manipulation, but only one shown per participant.

We coded the number of subjects, post-exclusions. We coded whether a study had a between-subjects, within-subjects, or mixed design; for the analysis, mixed studies were counted as within-subjects designs. In the analysis, we used a log-scale for number of subjects and numbers of trials.

\hypertarget{properties-of-replication}{%
\subsubsection{Properties of replication}\label{properties-of-replication}}

We coded whether the replication was conducted on a crowd-sourced platform; this was the norm for the class projects, but a few were done in-person. For analysis, we coded this into a variable indicating if the recruitment platform changed between original and replication. This grouped the few in-person replications in with the studies that were originally online and stayed online in a ``no change'' condition, in contrast with the studies that were originally in-person with online replications.

We coded the replication sample size (after exclusions). This was transformed to the predictor variable log ratio of replication to original sample size.

As a control variable, we included whether the original authors were faculty at Stanford at the time of the replication. This was to account for potential non-independence of these replications (ex. if replicating their advisor's work, students may have access to extra information about methods).

We made note of studies to exclude for sensitivity analyses, due to not quite aligned statistics, extremely small or unbalanced sample sizes, or a student choosing a key statistical measure that was not of central importance to the original study.

\hypertarget{determination-and-coding-of-key-statistical-measure}{%
\subsubsection{Determination and coding of key statistical measure}\label{determination-and-coding-of-key-statistical-measure}}

For each study pair, we used one key measure of interest for which we calculated the predictor variables of p-value and effect size and the statistical outcome measures p\_original and prediction interval. If the student specified a single key measure of interest and this was a measure that was reported in both the original paper and replication, we used that measure. If a student specified multiple, equally important, key measures, we used the first one. When students were not explicit about a key measure, we used other parts of their report (including introduction and power analysis) to determine what effect and therefore what result they considered key. In a few cases, we went back to the original paper to find what effect was considered crucial by the original authors. When the measures reported by the student did not cleanly match their explicit or implicitly stated key measure, we picked the most important (or first) of the measures that were reported in both the original and replication. These decisions could be somewhat subjective but importantly they were made without reference to replication outcomes.

Whenever possible, we used per-condition means and standard deviations, or the test statistic of the key measure and its corresponding degrees of freedom (ex. T test, F test). We took the original statistic from the replication report if it quoted the relevant analysis or from the original paper if not. We took the replication statistics from the replication report.

We then calculated p values, ES, p\_orig, and predInt. We choose to recalculate p values and effect sizes from the means or test statistic rather than use reported measures when possible because we thought this would be more reliable and transparent. The means and test statistics are more likely to have been outputted programmatically and copied directly into the text. In contrast, p-values are often reported as \textless.001 rather than as a point value, and effect size derivations may be error prone. By recording the raw statistics we used and using our available code to calculate other measures, we are transparent, as the test statistics can be searched for in the papers, and all processing is documented in code.

In some cases, p-values and or effect sizes were not calculable either due to insufficient reporting (ex. reporting a p-value but no other statistics from a test) or key measures where p-values and effect sizes did not apply (ex. PCA as measure of interest). Where studies reported beta estimates and standard errors or proportions, standardized effects sizes are not an applicable measure, but we were still able to calculate p\_original and prediction interval.

We separately coded whether the original and replication effects were in the same direction, based on raw means and graphs. This is more reliable than the statistics because F-tests don't include the direction of effect, and some students may have flipped the direction in coding for betas or t-tests. In the processed data, the direction of the effect of the replication was always coded consistently with the original study's coding, so a positive effect was in the same direction as the original and a negative effect in the opposite direction.

In regression analyses, we used SMD and log p-value as predictors.

\hypertarget{modelling}{%
\subsection{Modelling}\label{modelling}}

Due to the monotonic missingness of the data, we had more predictor variables and outcome variables for some original-replication pairs than for others. To take full advantage of the data, we ran a series of models, with some models having fewer predictors, but more data, and others having more predictors, but less data.

We ran a model predicting the subjective replication score on the basis of demographic and experimental predictors on the entire dataset. We ran two models predicting p\_original and prediction interval from demographic and experimental predictors on the subset of data where we had p\_original and prediction intervals. Then, on the smaller subset of the data where we had effect sizes and p-values, we re-ran these three models with those as additional predictor variables.

The subjective replication scores were coded on {[}0, .25, .5, .75, 1{]}, and we ramapped these to 1-5 to run an ordinal regression predicting replication score. We ran logistic regressions predicting prediction interval and linear regressions predicting p\_original.

All models used a horseshoe prior in brms. All models included random slopes for predictors nested within year the class occurred to control for variation between cohorts of students. We did not include any interaction terms in the models. All numeric predictor variables were z-scored after other transforms (e.g., logs) to ensure comparable regularization effects from the horseshoe prior.

As a secondary sensitivity analysis, we examined the subset of the data where the statistical tests had the same specification, the result was of primary importance in the original paper (i.e.~not a manipulation check), and there were no big issues with the replication.

Results from these models not reported in the main paper are reported in the supplement.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

Acknowledge people here. \texttt{\{-\}} useful to not number this section.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-altmejd2019}{}}%
Altmejd A, Dreber A, Forsell E, Huber J, Imai T, Johannesson M, Kirchler M, Nave G, Camerer C (2019) Predicting the replicability of social science lab experiments. \emph{PLOS ONE} \textbf{14}:e0225826. doi:\href{https://doi.org/10.1371/journal.pone.0225826}{10.1371/journal.pone.0225826}

\leavevmode\vadjust pre{\hypertarget{ref-anderson2016}{}}%
Anderson CJ, Bahnk , Barnett-Cowan M, Bosco FA, Chandler J, Chartier CR, Cheung F, Christopherson CD, Cordes A, Cremata EJ, Della Penna N, Estel V, Fedor A, Fitneva SA, Frank MC, Grange JA, Hartshorne JK, Hasselman F, Henninger F, Hulst M van der, Jonas KJ, Lai CK, Levitan CA, Miller JK, Moore KS, Meixner JM, Munaf MR, Neijenhuijs KI, Nilsonne G, Nosek BA, Plessow F, Prenoveau JM, Ricker AA, Schmidt K, Spies JR, Stieger S, Strohminger N, Sullivan GB, Aert RCM van, Assen MALM van, Vanpaemel W, Vianello M, Voracek M, Zuni K (2016) Response to {Comment} on {``{Estimating} the reproducibility of psychological science.''} \emph{Science} \textbf{351}:1037--1037. doi:\href{https://doi.org/10.1126/science.aad9163}{10.1126/science.aad9163}

\leavevmode\vadjust pre{\hypertarget{ref-camerer2018}{}}%
Camerer CF, Dreber A, Holzmeister F, Ho T-H, Huber J, Johannesson M, Kirchler M, Nave G, Nosek BA, Pfeiffer T, Altmejd A, Buttrick N, Chan T, Chen Y, Forsell E, Gampa A, Heikensten E, Hummer L, Imai T, Isaksson S, Manfredi D, Rose J, Wagenmakers E-J, Wu H (2018) Evaluating the replicability of social science experiments in {Nature} and {Science} between 2010 and 2015. \emph{Nat Hum Behav} \textbf{2}:637--644. doi:\href{https://doi.org/10.1038/s41562-018-0399-z}{10.1038/s41562-018-0399-z}

\leavevmode\vadjust pre{\hypertarget{ref-crump2013}{}}%
Crump MJC, McDonnell JV, Gureckis TM (2013) Evaluating amazon's mechanical turk as a tool for experimental behavioral research. \emph{{PLOS} {ONE}} \textbf{8}:e57410. doi:\href{https://doi.org/10.1371/journal.pone.0057410}{10.1371/journal.pone.0057410}

\leavevmode\vadjust pre{\hypertarget{ref-dreber2015}{}}%
Dreber A, Pfeiffer T, Almenberg J, Isaksson S, Wilson B, Chen Y, Nosek BA, Johannesson M (2015) Using prediction markets to estimate the reproducibility of scientific research. \emph{Proc Natl Acad Sci} \textbf{112}:15343--15347. doi:\href{https://doi.org/10.1073/pnas.1516179112}{10.1073/pnas.1516179112}

\leavevmode\vadjust pre{\hypertarget{ref-ebersole2016}{}}%
Ebersole CR, Atherton OE, Belanger AL, Skulborstad HM, Allen JM, Banks JB, Baranski E, Bernstein MJ, Bonfiglio DBV, Boucher L, Brown ER, Budiman NI, Cairo AH, Capaldi CA, Chartier CR, Chung JM, Cicero DC, Coleman JA, Conway JG, Davis WE, Devos T, Fletcher MM, German K, Grahe JE, Hermann AD, Hicks JA, Honeycutt N, Humphrey B, Janus M, Johnson DJ, Joy-Gaba JA, Juzeler H, Keres A, Kinney D, Kirshenbaum J, Klein RA, Lucas RE, Lustgraaf CJN, Martin D, Menon M, Metzger M, Moloney JM, Morse PJ, Prislin R, Razza T, Re DE, Rule NO, Sacco DF, Sauerberger K, Shrider E, Shultz M, Siemsen C, Sobocko K, Weylin Sternglanz R, Summerville A, Tskhay KO, Allen Z van, Vaughn LA, Walker RJ, Weinberg A, Wilson JP, Wirth JH, Wortman J, Nosek BA (2016) Many {Labs} 3: {Evaluating} participant pool quality across the academic semester via replication. \emph{Journal of Experimental Social Psychology} \textbf{67}:68--82. doi:\href{https://doi.org/10.1016/j.jesp.2015.10.012}{10.1016/j.jesp.2015.10.012}

\leavevmode\vadjust pre{\hypertarget{ref-ebersole2020}{}}%
Ebersole CR, Mathur MB, Baranski E, Bart-Plange D-J, Buttrick NR, Chartier CR, Corker KS, Corley M, Hartshorne JK, IJzerman H, Lazarevi LB, Rabagliati H, Ropovik I, Aczel B, Aeschbach LF, Andrighetto L, Arnal JD, Arrow H, Babincak P, Bakos BE, Bank G, Baskin E, Belopavlovi R, Bernstein MH, Biaek M, Bloxsom NG, Bodroa B, Bonfiglio DBV, Boucher L, Brhlmann F, Brumbaugh CC, Casini E, Chen Y, Chiorri C, Chopik WJ, Christ O, Ciunci AM, Claypool HM, Coary S, oli MV, Collins WM, Curran PG, Day CR, Dering B, Dreber A, Edlund JE, Falco F, Fedor A, Feinberg L, Ferguson IR, Ford M, Frank MC, Fryberger E, Garinther A, Gawryluk K, Ashbaugh K, Giacomantonio M, Giessner SR, Grahe JE, Guadagno RE, Haasa E, Hancock PJB, Hilliard RA, Hffmeier J, Hughes S, Idzikowska K, Inzlicht M, Jern A, Jimnez-Leal W, Johannesson M, Joy-Gaba JA, Kauff M, Kellier DJ, Kessinger G, Kidwell MC, Kimbrough AM, King JPJ, Kolb VS, Koodziej S, Kovacs M, Krasuska K, Kraus S, Krueger LE, Kuchno K, Lage CA, Langford EV, Levitan CA, Lima TJS de, Lin H, Lins S, Loy JE, Manfredi D, Markiewicz , Menon M, Mercier B, Metzger M, Meyet V, Millen AE, Miller JK, Montealegre A, Moore DA, Muda R, Nave G, Nichols AL, Novak SA, Nunnally C, Orli A, Palinkas A, Panno A, Parks KP, Pedovi I, Pkala E, Penner MR, Pessers S, Petrovi B, Pfeiffer T, Piekosz D, Preti E, Puri D, Ramos T, Ravid J, Razza TS, Rentzsch K, Richetin J, Rife SC, Rosa AD, Rudy KH, Salamon J, Saunders B, Sawicki P, Schmidt K, Schuepfer K, Schultze T, Schulz-Hardt S, Schtz A, Shabazian AN, Shubella RL, Siegel A, Silva R, Sioma B, Skorb L, Souza LEC de, Steegen S, Stein LAR, Sternglanz RW, Stojilovi D, Storage D, Sullivan GB, Szaszi B, Szecsi P, Szke O, Szuts A, Thomae M, Tidwell ND, Tocco C, Torka A-K, Tuerlinckx F, Vanpaemel W, Vaughn LA, Vianello M, Viganola D, Vlachou M, Walker RJ, Weissgerber SC, Wichman AL, Wiggins BJ, Wolf D, Wood MJ, Zealley D, eelj I, Zrubka M, Nosek BA (2020) Many {Labs} 5: {Testing Pre-Data-Collection Peer Review} as an {Intervention} to {Increase Replicability}. \emph{Adv Methods Pract Psychol Sci} \textbf{3}:309--331. doi:\href{https://doi.org/10.1177/2515245920958687}{10.1177/2515245920958687}

\leavevmode\vadjust pre{\hypertarget{ref-errington2021}{}}%
Errington TM, Mathur M, Soderberg CK, Denis A, Perfito N, Iorns E, Nosek BA (2021) Investigating the replicability of preclinical cancer biology (R Pasqualini and E Franco, Eds.). \emph{eLife} \textbf{10}:e71601. doi:\href{https://doi.org/10.7554/eLife.71601}{10.7554/eLife.71601}

\leavevmode\vadjust pre{\hypertarget{ref-etz2016}{}}%
Etz A, Vandekerckhove J (2016) A {Bayesian Perspective} on the {Reproducibility Project}: {Psychology}. \emph{PLOS ONE} \textbf{11}:e0149794. doi:\href{https://doi.org/10.1371/journal.pone.0149794}{10.1371/journal.pone.0149794}

\leavevmode\vadjust pre{\hypertarget{ref-forsell2019}{}}%
Forsell E, Viganola D, Pfeiffer T, Almenberg J, Wilson B, Chen Y, Nosek BA, Johannesson M, Dreber A (2019) Predicting replication outcomes in the {Many Labs} 2 study. \emph{Journal of Economic Psychology} \textbf{75}:102117. doi:\href{https://doi.org/10.1016/j.joep.2018.10.009}{10.1016/j.joep.2018.10.009}

\leavevmode\vadjust pre{\hypertarget{ref-frank2012}{}}%
Frank MC, Saxe R (2012) Teaching {Replication}: \emph{Perspect Psychol Sci}. doi:\href{https://doi.org/10.1177/1745691612460686}{10.1177/1745691612460686}

\leavevmode\vadjust pre{\hypertarget{ref-gelman2018}{}}%
Gelman A (2018/ed) Don't characterize replications as successes or failures. \emph{Behav Brain Sci} \textbf{41}:e128. doi:\href{https://doi.org/10.1017/S0140525X18000638}{10.1017/S0140525X18000638}

\leavevmode\vadjust pre{\hypertarget{ref-gilbert2016}{}}%
Gilbert DT, King G, Pettigrew S, Wilson TD (2016) Comment on {``{Estimating} the reproducibility of psychological science.''} \emph{Science} \textbf{351}:1037--1037. doi:\href{https://doi.org/10.1126/science.aad7243}{10.1126/science.aad7243}

\leavevmode\vadjust pre{\hypertarget{ref-hagger2016}{}}%
Hagger MS, Chatzisarantis NLD, Alberts H, Anggono CO, Batailler C, Birt AR, Brand R, Brandt MJ, Brewer G, Bruyneel S, Calvillo DP, Campbell WK, Cannon PR, Carlucci M, Carruth NP, Cheung T, Crowell A, De Ridder DTD, Dewitte S, Elson M, Evans JR, Fay BA, Fennis BM, Finley A, Francis Z, Heise E, Hoemann H, Inzlicht M, Koole SL, Koppel L, Kroese F, Lange F, Lau K, Lynch BP, Martijn C, Merckelbach H, Mills NV, Michirev A, Miyake A, Mosser AE, Muise M, Muller D, Muzi M, Nalis D, Nurwanti R, Otgaar H, Philipp MC, Primoceri P, Rentzsch K, Ringos L, Schlinkert C, Schmeichel BJ, Schoch SF, Schrama M, Schtz A, Stamos A, Tinghg G, Ullrich J, vanDellen M, Wimbarti S, Wolff W, Yusainy C, Zerhouni O, Zwienenberg M (2016) A multilab preregistered replication of the ego-depletion effect. \emph{Perspect Psychol Sci} \textbf{11}:546--573. doi:\href{https://doi.org/10.1177/1745691616652873}{10.1177/1745691616652873}

\leavevmode\vadjust pre{\hypertarget{ref-hawkins}{}}%
Hawkins RXD, Smith EN, Au C, Arias JM, Hermann E, Keil M, Lampinen A, Raposo S, Salehi S, Salloum J, Tan J, Frank MC Improving the {Replicability} of {Psychological Science Through Pedagogy}. :41

\leavevmode\vadjust pre{\hypertarget{ref-hoogeveen2019}{}}%
Hoogeveen S, Sarafoglou A, Wagenmakers E-J (2019) Laypeople {Can Predict Which Social Science Studies Replicate}. preprint. {PsyArXiv}. Available from: \url{https://osf.io/egw9d} {[}Last accessed 30 September 2019{]}. doi:\href{https://doi.org/10.31234/osf.io/egw9d}{10.31234/osf.io/egw9d}

\leavevmode\vadjust pre{\hypertarget{ref-klein2014}{}}%
Klein RA, Ratliff KA, Vianello M, Adams RB, Bahnk , Bernstein MJ, Bocian K, Brandt MJ, Brooks B, Brumbaugh CC, Cemalcilar Z, Chandler J, Cheong W, Davis WE, Devos T, Eisner M, Frankowska N, Furrow D, Galliani EM, Hasselman F, Hicks JA, Hovermale JF, Hunt SJ, Huntsinger JR, IJzerman H, John M-S, Joy-Gaba JA, Barry Kappes H, Krueger LE, Kurtz J, Levitan CA, Mallett RK, Morris WL, Nelson AJ, Nier JA, Packard G, Pilati R, Rutchick AM, Schmidt K, Skorinko JL, Smith R, Steiner TG, Storbeck J, Van Swol LM, Thompson D, Veer AE van `t, Ann Vaughn L, Vranka M, Wichman AL, Woodzicka JA, Nosek BA (2014) Investigating {Variation} in {Replicability}: {A} {``{Many Labs}''} {Replication Project}. \emph{Social Psychology} \textbf{45}:142--152. doi:\href{https://doi.org/10.1027/1864-9335/a000178}{10.1027/1864-9335/a000178}

\leavevmode\vadjust pre{\hypertarget{ref-klein2018}{}}%
Klein RA, Vianello M, Hasselman F, Adams BG, Adams RB, Alper S, Aveyard M, Axt JR, Babalola MT, Bahnk , Batra R, Berkics M, Bernstein MJ, Berry DR, Bialobrzeska O, Binan ED, Bocian K, Brandt MJ, Busching R, Rdei AC, Cai H, Cambier F, Cantarero K, Carmichael CL, Ceric F, Chandler J, Chang J-H, Chatard A, Chen EE, Cheong W, Cicero DC, Coen S, Coleman JA, Collisson B, Conway MA, Corker KS, Curran PG, Cushman F, Dagona ZK, Dalgar I, Dalla Rosa A, Davis WE, Bruijn M de, De Schutter L, Devos T, Vries M de, Doulu C, Dozo N, Dukes KN, Dunham Y, Durrheim K, Ebersole CR, Edlund JE, Eller A, English AS, Finck C, Frankowska N, Freyre M-, Friedman M, Galliani EM, Gandi JC, Ghoshal T, Giessner SR, Gill T, Gnambs T, Gmez , Gonzlez R, Graham J, Grahe JE, Grahek I, Green EGT, Hai K, Haigh M, Haines EL, Hall MP, Heffernan ME, Hicks JA, Houdek P, Huntsinger JR, Huynh HP, IJzerman H, Inbar Y, Innes-Ker H, Jimnez-Leal W, John M-S, Joy-Gaba JA, Kamilolu RG, Kappes HB, Karabati S, Karick H, Keller VN, Kende A, Kervyn N, Kneevi G, Kovacs C, Krueger LE, Kurapov G, Kurtz J, Lakens D, Lazarevi LB, Levitan CA, Lewis NA, Lins S, Lipsey NP, Losee JE, Maassen E, Maitner AT, Malingumu W, Mallett RK, Marotta SA, Meedovi J, Mena-Pacheco F, Milfont TL, Morris WL, Murphy SC, Myachykov A, Neave N, Neijenhuijs K, Nelson AJ, Neto F, Lee Nichols A, Ocampo A, O'Donnell SL, Oikawa H, Oikawa M, Ong E, Orosz G, Osowiecka M, Packard G, Prez-Snchez R, Petrovi B, Pilati R, Pinter B, Podesta L, Pogge G, Pollmann MMH, Rutchick AM, Saavedra P, Saeri AK, Salomon E, Schmidt K, Schnbrodt FD, Sekerdej MB, Sirlop D, Skorinko JLM, Smith MA, Smith-Castro V, Smolders KCHJ, Sobkow A, Sowden W, Spachtholz P, Srivastava M, Steiner TG, Stouten J, Street CNH, Sundfelt OK, Szeto S, Szumowska E, Tang ACW, Tanzer N, Tear MJ, Theriault J, Thomae M, Torres D, Traczyk J, Tybur JM, Ujhelyi A, Aert RCM van, Assen MALM van, Hulst M van der, Lange PAM van, Veer AE van 't, Vsquez- Echeverra A, Ann Vaughn L, Vzquez A, Vega LD, Verniers C, Verschoor M, Voermans IPJ, Vranka MA, Welch C, Wichman AL, Williams LA, Wood M, Woodzicka JA, Wronska MK, Young L, Zelenski JM, Zhijia Z, Nosek BA (2018) Many {Labs} 2: {Investigating Variation} in {Replicability Across Samples} and {Settings}. \emph{Adv Methods Pract Psychol Sci} \textbf{1}:443--490. doi:\href{https://doi.org/10.1177/2515245918810225}{10.1177/2515245918810225}

\leavevmode\vadjust pre{\hypertarget{ref-klein2022}{}}%
Klein RA, Cook CL, Ebersole CR, Vitiello C, Nosek BA, Hilgard J, Ahn PH, Brady AJ, Chartier CR, Christopherson CD, Clay S, Collisson B, Crawford JT, Cromar R, Gardiner G, Gosnell CL, Grahe J, Hall C, Howard I, Joy-Gaba JA, Kolb M, Legg AM, Levitan CA, Mancini AD, Manfredi D, Miller J, Nave G, Redford L, Schlitz I, Schmidt K, Skorinko JLM, Storage D, Swanson T, Van Swol LM, Vaughn LA, Vidamuerte D, Wiggins B, Ratliff KA (2022) Many {Labs} 4: {Failure} to {Replicate Mortality Salience Effect With} and {Without Original Author Involvement}. \emph{Collabra: Psychology} \textbf{8}:35271. doi:\href{https://doi.org/10.1525/collabra.35271}{10.1525/collabra.35271}

\leavevmode\vadjust pre{\hypertarget{ref-mathur2020}{}}%
Mathur MB, VanderWeele TJ (2020) New statistical metrics for multisite replication projects. \emph{J R Stat Soc Ser A Stat Soc} \textbf{183}:1145--1166. doi:\href{https://doi.org/10.1111/rssa.12572}{10.1111/rssa.12572}

\leavevmode\vadjust pre{\hypertarget{ref-openscienceconsortium2015}{}}%
Open Science Consortium (2015) \href{https://www.science.org/doi/full/10.1126/science.aac4716?casa_token=IJ35TwwlcjsAAAAA\%3AqiP68QbVAHleIg9zD3WugKWuV6Oa5rswS0VQnDsCq5I14ME4WIQabNGVD_T6SBSuAt6voVHNnWc0sw}{Estimating the reproducibility of psychological science}. \emph{Science}

\leavevmode\vadjust pre{\hypertarget{ref-patil2016}{}}%
Patil P, Peng RD, Leek JT (2016) What {Should Researchers Expect When They Replicate Studies}? {A Statistical View} of {Replicability} in {Psychological Science}. \emph{Perspect Psychol Sci} \textbf{11}:539--544. doi:\href{https://doi.org/10.1177/1745691616646366}{10.1177/1745691616646366}

\leavevmode\vadjust pre{\hypertarget{ref-protzko2020}{}}%
Protzko J, Krosnick J, Nelson LD, Nosek BA, Axt J, Berent M, Buttrick N, DeBell M, Ebersole CR, Lundmark S, MacInnis B, O'Donnell M, Perfecto H, Pustejovsky JE, Roeder SS, Walleczek J, Schooler J (2020) High {Replicability} of {Newly-Discovered Social-behavioral Findings} is {Achievable}. preprint. {PsyArXiv}. Available from: \url{https://osf.io/n2a9x} {[}Last accessed 5 April 2023{]}. doi:\href{https://doi.org/10.31234/osf.io/n2a9x}{10.31234/osf.io/n2a9x}

\leavevmode\vadjust pre{\hypertarget{ref-quintana2021}{}}%
Quintana DS (2021) Replication studies for undergraduate theses to improve science and education. \emph{Nat Hum Behav} \textbf{5}:1117--1118. doi:\href{https://doi.org/10.1038/s41562-021-01192-8}{10.1038/s41562-021-01192-8}

\leavevmode\vadjust pre{\hypertarget{ref-ramscar}{}}%
Ramscar M, Shaoul C, Baayen RH Why many priming results don't (and won't) replicate: {A} quantitative analysis.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2015}{}}%
Simonsohn U (2015) Small {Telescopes}: {Detectability} and the {Evaluation} of {Replication Results}. \emph{Psychol Sci} \textbf{26}:559--569. doi:\href{https://doi.org/10.1177/0956797614567341}{10.1177/0956797614567341}

\leavevmode\vadjust pre{\hypertarget{ref-vanbavel2016}{}}%
Van Bavel JJ, Mende-Siedlecki P, Brady WJ, Reinero DA (2016) Contextual sensitivity in scientific reproducibility. \emph{Proc Natl Acad Sci} \textbf{113}:6454--6459. doi:\href{https://doi.org/10.1073/pnas.1521897113}{10.1073/pnas.1521897113}

\leavevmode\vadjust pre{\hypertarget{ref-wagge2019}{}}%
Wagge JR, Brandt MJ, Lazarevic LB, Legate N, Christopherson C, Wiggins B, Grahe JE (2019) \href{https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00247}{Publishing {Research With Undergraduate Students} via {Replication Work}: {The Collaborative Replications} and {Education Project}}. \emph{Front Psychol} \textbf{10}

\leavevmode\vadjust pre{\hypertarget{ref-wilson2020}{}}%
Wilson BM, Harris CR, Wixted JT (2020) Science is not a signal detection problem. \emph{Proc Natl Acad Sci USA} \textbf{117}:5559--5567. doi:\href{https://doi.org/10.1073/pnas.1914237117}{10.1073/pnas.1914237117}

\end{CSLReferences}


\end{document}
